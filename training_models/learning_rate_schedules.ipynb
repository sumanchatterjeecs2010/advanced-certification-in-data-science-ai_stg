{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import pandas as pd\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Fashion-MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# trimming the data since it takes lot of time\n",
    "X_train_full = X_train_full[:30000]\n",
    "y_train_full = y_train_full[:30000]\n",
    "\n",
    "X_test = X_test[:5000]\n",
    "y_test = y_test[:5000]\n",
    "\n",
    "# scaling the dataset\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# dividing the dataset into traingin and validation set\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "# scaling the dataset\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at different learning rate schedules here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr = lr0 * 0.1**(epoch / s)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining exponential decay without hardcoding\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 4s 178us/sample - loss: 0.9128 - accuracy: 0.7455 - val_loss: 0.8480 - val_accuracy: 0.7428\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 4s 152us/sample - loss: 0.6547 - accuracy: 0.7936 - val_loss: 0.7723 - val_accuracy: 0.7556\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 4s 159us/sample - loss: 0.5880 - accuracy: 0.8153 - val_loss: 0.8921 - val_accuracy: 0.7842\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 4s 162us/sample - loss: 0.5404 - accuracy: 0.8286 - val_loss: 0.6437 - val_accuracy: 0.8174\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 4s 159us/sample - loss: 0.4576 - accuracy: 0.8532 - val_loss: 0.6813 - val_accuracy: 0.7956\n"
     ]
    }
   ],
   "source": [
    "# LearningRateScheduler update the optimizer’s learning_rate\n",
    "# attribute at the beginning of each epoch\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the learning rate by epoch for exponential scheduling\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "Learning Rate 0.01\n",
      "   32/25000 [..............................] - ETA: 8:47 - loss: 2.6560 - accuracy: 0.0938Learning Rate 0.009998526\n",
      "Learning Rate 0.009997053\n",
      "Learning Rate 0.00999558\n",
      "Learning Rate 0.009994106\n",
      "Learning Rate 0.009992634\n",
      "Learning Rate 0.0099911615\n",
      "Learning Rate 0.009989689\n",
      "Learning Rate 0.009988217\n",
      "Learning Rate 0.009986745\n",
      "Learning Rate 0.009985274\n",
      "  352/25000 [..............................] - ETA: 51s - loss: 4.9726 - accuracy: 0.4915 Learning Rate 0.009983802\n",
      "Learning Rate 0.009982331\n",
      "Learning Rate 0.00998086\n",
      "Learning Rate 0.00997939\n",
      "Learning Rate 0.009977919\n",
      "Learning Rate 0.0099764485\n",
      "Learning Rate 0.009974979\n",
      "Learning Rate 0.009973509\n",
      "Learning Rate 0.00997204\n",
      "  640/25000 [..............................] - ETA: 29s - loss: 3.9639 - accuracy: 0.5609Learning Rate 0.00997057\n",
      "Learning Rate 0.0099691\n",
      "Learning Rate 0.009967632\n",
      "Learning Rate 0.009966163\n",
      "Learning Rate 0.009964694\n",
      "Learning Rate 0.009963226\n",
      "Learning Rate 0.009961758\n",
      "Learning Rate 0.00996029\n",
      "Learning Rate 0.009958822\n",
      "Learning Rate 0.0099573545\n",
      "  960/25000 [>.............................] - ETA: 20s - loss: 3.4546 - accuracy: 0.5833Learning Rate 0.009955888\n",
      "Learning Rate 0.009954421\n",
      "Learning Rate 0.009952954\n",
      "Learning Rate 0.009951487\n",
      "Learning Rate 0.00995002\n",
      "Learning Rate 0.009948554\n",
      "Learning Rate 0.009947089\n",
      "Learning Rate 0.009945623\n",
      "Learning Rate 0.009944157\n",
      " 1248/25000 [>.............................] - ETA: 16s - loss: 3.1441 - accuracy: 0.5970Learning Rate 0.009942692\n",
      "Learning Rate 0.009941227\n",
      "Learning Rate 0.009939762\n",
      "Learning Rate 0.009938297\n",
      "Learning Rate 0.009936833\n",
      "Learning Rate 0.009935369\n",
      "Learning Rate 0.009933905\n",
      "Learning Rate 0.009932441\n",
      "Learning Rate 0.009930977\n",
      " 1536/25000 [>.............................] - ETA: 14s - loss: 2.9445 - accuracy: 0.6061Learning Rate 0.009929514\n",
      "Learning Rate 0.00992805\n",
      "Learning Rate 0.009926587\n",
      "Learning Rate 0.009925124\n",
      "Learning Rate 0.009923662\n",
      "Learning Rate 0.0099222\n",
      "Learning Rate 0.009920738\n",
      "Learning Rate 0.0099192755\n",
      "Learning Rate 0.009917814\n",
      " 1824/25000 [=>............................] - ETA: 12s - loss: 2.6575 - accuracy: 0.6255Learning Rate 0.009916353\n",
      "Learning Rate 0.009914892\n",
      "Learning Rate 0.009913431\n",
      "Learning Rate 0.009911969\n",
      "Learning Rate 0.009910509\n",
      "Learning Rate 0.009909049\n",
      "Learning Rate 0.009907588\n",
      "Learning Rate 0.009906128\n",
      "Learning Rate 0.009904669\n",
      " 2112/25000 [=>............................] - ETA: 11s - loss: 2.4467 - accuracy: 0.6312Learning Rate 0.009903209\n",
      "Learning Rate 0.00990175\n",
      "Learning Rate 0.0099002905\n",
      "Learning Rate 0.009898832\n",
      "Learning Rate 0.009897374\n",
      "Learning Rate 0.009895915\n",
      "Learning Rate 0.009894457\n",
      "Learning Rate 0.009892998\n",
      "Learning Rate 0.009891541\n",
      " 2400/25000 [=>............................] - ETA: 10s - loss: 2.2644 - accuracy: 0.6396Learning Rate 0.009890083\n",
      "Learning Rate 0.009888626\n",
      "Learning Rate 0.009887168\n",
      "Learning Rate 0.009885712\n",
      "Learning Rate 0.009884255\n",
      "Learning Rate 0.009882798\n",
      "Learning Rate 0.009881342\n",
      "Learning Rate 0.009879886\n",
      "Learning Rate 0.0098784305\n",
      "Learning Rate 0.009876975\n",
      " 2720/25000 [==>...........................] - ETA: 9s - loss: 2.0710 - accuracy: 0.6511 Learning Rate 0.009875519\n",
      "Learning Rate 0.009874064\n",
      "Learning Rate 0.009872609\n",
      "Learning Rate 0.009871154\n",
      "Learning Rate 0.009869699\n",
      "Learning Rate 0.009868245\n",
      "Learning Rate 0.009866791\n",
      "Learning Rate 0.009865337\n",
      "Learning Rate 0.009863883\n",
      " 3008/25000 [==>...........................] - ETA: 8s - loss: 1.9718 - accuracy: 0.6549Learning Rate 0.009862429\n",
      "Learning Rate 0.009860977\n",
      "Learning Rate 0.009859524\n",
      "Learning Rate 0.009858071\n",
      "Learning Rate 0.009856618\n",
      "Learning Rate 0.009855165\n",
      "Learning Rate 0.009853713\n",
      "Learning Rate 0.009852261\n",
      "Learning Rate 0.009850809\n",
      " 3296/25000 [==>...........................] - ETA: 8s - loss: 1.8650 - accuracy: 0.6602Learning Rate 0.009849357\n",
      "Learning Rate 0.009847906\n",
      "Learning Rate 0.009846455\n",
      "Learning Rate 0.009845004\n",
      "Learning Rate 0.009843553\n",
      "Learning Rate 0.009842103\n",
      "Learning Rate 0.009840653\n",
      "Learning Rate 0.009839203\n",
      "Learning Rate 0.009837753\n",
      " 3584/25000 [===>..........................] - ETA: 7s - loss: 1.7815 - accuracy: 0.6652Learning Rate 0.009836303\n",
      "Learning Rate 0.009834854\n",
      "Learning Rate 0.009833405\n",
      "Learning Rate 0.009831956\n",
      "Learning Rate 0.0098305065\n",
      "Learning Rate 0.009829058\n",
      "Learning Rate 0.00982761\n",
      "Learning Rate 0.009826162\n",
      "Learning Rate 0.009824714\n",
      "Learning Rate 0.009823266\n",
      " 3904/25000 [===>..........................] - ETA: 7s - loss: 1.6894 - accuracy: 0.6739Learning Rate 0.009821819\n",
      "Learning Rate 0.009820372\n",
      "Learning Rate 0.009818925\n",
      "Learning Rate 0.009817477\n",
      "Learning Rate 0.009816031\n",
      "Learning Rate 0.009814585\n",
      "Learning Rate 0.009813138\n",
      "Learning Rate 0.009811692\n",
      "Learning Rate 0.0098102465\n",
      " 4192/25000 [====>.........................] - ETA: 6s - loss: 1.6194 - accuracy: 0.6808Learning Rate 0.009808801\n",
      "Learning Rate 0.009807356\n",
      "Learning Rate 0.00980591\n",
      "Learning Rate 0.009804466\n",
      "Learning Rate 0.009803021\n",
      "Learning Rate 0.009801577\n",
      "Learning Rate 0.009800132\n",
      "Learning Rate 0.009798688\n",
      "Learning Rate 0.009797244\n",
      " 4480/25000 [====>.........................] - ETA: 6s - loss: 1.5558 - accuracy: 0.6877Learning Rate 0.009795801\n",
      "Learning Rate 0.009794357\n",
      "Learning Rate 0.009792914\n",
      "Learning Rate 0.009791471\n",
      "Learning Rate 0.009790028\n",
      "Learning Rate 0.009788586\n",
      "Learning Rate 0.009787143\n",
      "Learning Rate 0.009785701\n",
      "Learning Rate 0.009784259\n",
      " 4768/25000 [====>.........................] - ETA: 6s - loss: 1.5041 - accuracy: 0.6917Learning Rate 0.009782817\n",
      "Learning Rate 0.0097813755\n",
      "Learning Rate 0.009779934\n",
      "Learning Rate 0.009778493\n",
      "Learning Rate 0.009777052\n",
      "Learning Rate 0.009775612\n",
      "Learning Rate 0.009774171\n",
      "Learning Rate 0.009772731\n",
      " 5024/25000 [=====>........................] - ETA: 6s - loss: 1.4579 - accuracy: 0.6967Learning Rate 0.009771291\n",
      "Learning Rate 0.009769851\n",
      "Learning Rate 0.0097684115\n",
      "Learning Rate 0.009766972\n",
      "Learning Rate 0.009765533\n",
      "Learning Rate 0.009764094\n",
      "Learning Rate 0.009762655\n",
      "Learning Rate 0.009761216\n",
      "Learning Rate 0.009759778\n",
      " 5312/25000 [=====>........................] - ETA: 6s - loss: 1.4142 - accuracy: 0.7014Learning Rate 0.00975834\n",
      "Learning Rate 0.009756902\n",
      "Learning Rate 0.009755464\n",
      "Learning Rate 0.009754026\n",
      "Learning Rate 0.009752589\n",
      "Learning Rate 0.009751152\n",
      "Learning Rate 0.009749715\n",
      "Learning Rate 0.009748278\n",
      "Learning Rate 0.009746842\n",
      " 5600/25000 [=====>........................] - ETA: 5s - loss: 1.3809 - accuracy: 0.7055Learning Rate 0.009745406\n",
      "Learning Rate 0.00974397\n",
      "Learning Rate 0.009742534\n",
      "Learning Rate 0.009741099\n",
      "Learning Rate 0.009739663\n",
      "Learning Rate 0.009738228\n",
      "Learning Rate 0.009736793\n",
      "Learning Rate 0.009735358\n",
      "Learning Rate 0.009733924\n",
      " 5888/25000 [======>.......................] - ETA: 5s - loss: 1.3522 - accuracy: 0.7082Learning Rate 0.0097324895\n",
      "Learning Rate 0.009731055\n",
      "Learning Rate 0.009729621\n",
      "Learning Rate 0.009728188\n",
      "Learning Rate 0.009726754\n",
      "Learning Rate 0.009725321\n",
      "Learning Rate 0.009723888\n",
      "Learning Rate 0.0097224545\n",
      "Learning Rate 0.009721022\n",
      " 6176/25000 [======>.......................] - ETA: 5s - loss: 1.3186 - accuracy: 0.7110Learning Rate 0.00971959\n",
      "Learning Rate 0.009718157\n",
      "Learning Rate 0.009716725\n",
      "Learning Rate 0.0097152935\n",
      "Learning Rate 0.009713862\n",
      "Learning Rate 0.009712431\n",
      "Learning Rate 0.009710999\n",
      "Learning Rate 0.009709569\n",
      "Learning Rate 0.009708138\n",
      " 6464/25000 [======>.......................] - ETA: 5s - loss: 1.2904 - accuracy: 0.7147Learning Rate 0.009706708\n",
      "Learning Rate 0.009705277\n",
      "Learning Rate 0.009703847\n",
      "Learning Rate 0.009702417\n",
      "Learning Rate 0.0097009875\n",
      "Learning Rate 0.009699558\n",
      "Learning Rate 0.009698128\n",
      "Learning Rate 0.0096967\n",
      "Learning Rate 0.009695271\n",
      " 6752/25000 [=======>......................] - ETA: 5s - loss: 1.2732 - accuracy: 0.7143Learning Rate 0.009693842\n",
      "Learning Rate 0.009692414\n",
      "Learning Rate 0.009690985\n",
      "Learning Rate 0.009689557\n",
      "Learning Rate 0.00968813\n",
      "Learning Rate 0.009686702\n",
      "Learning Rate 0.009685274\n",
      "Learning Rate 0.009683847\n",
      "Learning Rate 0.009682421\n",
      "Learning Rate 0.009680994\n",
      " 7072/25000 [=======>......................] - ETA: 4s - loss: 1.2499 - accuracy: 0.7146Learning Rate 0.009679567\n",
      "Learning Rate 0.00967814\n",
      "Learning Rate 0.009676714\n",
      "Learning Rate 0.009675289\n",
      "Learning Rate 0.009673863\n",
      "Learning Rate 0.009672437\n",
      "Learning Rate 0.009671012\n",
      "Learning Rate 0.009669587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.009668162\n",
      " 7360/25000 [=======>......................] - ETA: 4s - loss: 1.2356 - accuracy: 0.7162Learning Rate 0.009666737\n",
      "Learning Rate 0.009665313\n",
      "Learning Rate 0.009663889\n",
      "Learning Rate 0.009662465\n",
      "Learning Rate 0.009661041\n",
      "Learning Rate 0.009659617\n",
      "Learning Rate 0.009658194\n",
      "Learning Rate 0.009656771\n",
      "Learning Rate 0.009655348\n",
      "Learning Rate 0.009653925\n",
      " 7680/25000 [========>.....................] - ETA: 4s - loss: 1.2087 - accuracy: 0.7201Learning Rate 0.009652503\n",
      "Learning Rate 0.009651081\n",
      "Learning Rate 0.009649659\n",
      "Learning Rate 0.009648236\n",
      "Learning Rate 0.009646814\n",
      "Learning Rate 0.009645393\n",
      "Learning Rate 0.009643972\n",
      "Learning Rate 0.009642551\n",
      "Learning Rate 0.0096411295\n",
      " 7968/25000 [========>.....................] - ETA: 4s - loss: 1.1897 - accuracy: 0.7213Learning Rate 0.009639709\n",
      "Learning Rate 0.009638289\n",
      "Learning Rate 0.009636869\n",
      "Learning Rate 0.009635448\n",
      "Learning Rate 0.009634028\n",
      "Learning Rate 0.009632609\n",
      "Learning Rate 0.0096311895\n",
      "Learning Rate 0.00962977\n",
      "Learning Rate 0.009628351\n",
      " 8256/25000 [========>.....................] - ETA: 4s - loss: 1.1817 - accuracy: 0.7201Learning Rate 0.009626932\n",
      "Learning Rate 0.009625514\n",
      "Learning Rate 0.009624096\n",
      "Learning Rate 0.009622677\n",
      "Learning Rate 0.009621259\n",
      "Learning Rate 0.009619841\n",
      "Learning Rate 0.009618424\n",
      "Learning Rate 0.009617006\n",
      "Learning Rate 0.009615589\n",
      " 8544/25000 [=========>....................] - ETA: 4s - loss: 1.1606 - accuracy: 0.7235Learning Rate 0.009614172\n",
      "Learning Rate 0.009612756\n",
      "Learning Rate 0.009611339\n",
      "Learning Rate 0.009609923\n",
      "Learning Rate 0.009608507\n",
      "Learning Rate 0.009607092\n",
      "Learning Rate 0.009605676\n",
      "Learning Rate 0.00960426\n",
      "Learning Rate 0.009602845\n",
      " 8832/25000 [=========>....................] - ETA: 4s - loss: 1.1476 - accuracy: 0.7237Learning Rate 0.00960143\n",
      "Learning Rate 0.009600015\n",
      "Learning Rate 0.009598601\n",
      "Learning Rate 0.009597186\n",
      "Learning Rate 0.009595772\n",
      "Learning Rate 0.0095943585\n",
      "Learning Rate 0.009592945\n",
      "Learning Rate 0.009591531\n",
      "Learning Rate 0.009590117\n",
      " 9120/25000 [=========>....................] - ETA: 3s - loss: 1.1359 - accuracy: 0.7240Learning Rate 0.009588704\n",
      "Learning Rate 0.009587292\n",
      "Learning Rate 0.009585879\n",
      "Learning Rate 0.009584466\n",
      "Learning Rate 0.009583054\n",
      "Learning Rate 0.009581642\n",
      "Learning Rate 0.00958023\n",
      "Learning Rate 0.009578818\n",
      "Learning Rate 0.009577407\n",
      " 9408/25000 [==========>...................] - ETA: 3s - loss: 1.1266 - accuracy: 0.7245Learning Rate 0.009575996\n",
      "Learning Rate 0.009574585\n",
      "Learning Rate 0.009573174\n",
      "Learning Rate 0.009571763\n",
      "Learning Rate 0.009570353\n",
      "Learning Rate 0.009568943\n",
      "Learning Rate 0.009567533\n",
      "Learning Rate 0.009566123\n",
      "Learning Rate 0.009564713\n",
      "Learning Rate 0.009563304\n",
      " 9728/25000 [==========>...................] - ETA: 3s - loss: 1.1120 - accuracy: 0.7262Learning Rate 0.009561894\n",
      "Learning Rate 0.009560485\n",
      "Learning Rate 0.009559076\n",
      "Learning Rate 0.009557668\n",
      "Learning Rate 0.00955626\n",
      "Learning Rate 0.009554852\n",
      "Learning Rate 0.009553444\n",
      "Learning Rate 0.0095520355\n",
      "Learning Rate 0.009550628\n",
      "10016/25000 [===========>..................] - ETA: 3s - loss: 1.1041 - accuracy: 0.7255Learning Rate 0.009549221\n",
      "Learning Rate 0.009547814\n",
      "Learning Rate 0.009546407\n",
      "Learning Rate 0.009545\n",
      "Learning Rate 0.009543594\n",
      "Learning Rate 0.009542188\n",
      "Learning Rate 0.009540781\n",
      "Learning Rate 0.009539375\n",
      "Learning Rate 0.00953797\n",
      "Learning Rate 0.009536564\n",
      "10336/25000 [===========>..................] - ETA: 3s - loss: 1.0904 - accuracy: 0.7270Learning Rate 0.009535159\n",
      "Learning Rate 0.009533754\n",
      "Learning Rate 0.009532349\n",
      "Learning Rate 0.009530945\n",
      "Learning Rate 0.00952954\n",
      "Learning Rate 0.009528136\n",
      "Learning Rate 0.009526731\n",
      "Learning Rate 0.009525328\n",
      "Learning Rate 0.009523924\n",
      "10624/25000 [===========>..................] - ETA: 3s - loss: 1.0791 - accuracy: 0.7290Learning Rate 0.009522521\n",
      "Learning Rate 0.009521117\n",
      "Learning Rate 0.009519715\n",
      "Learning Rate 0.009518312\n",
      "Learning Rate 0.00951691\n",
      "Learning Rate 0.009515507\n",
      "Learning Rate 0.009514105\n",
      "Learning Rate 0.009512703\n",
      "Learning Rate 0.009511301\n",
      "Learning Rate 0.0095099\n",
      "10944/25000 [============>.................] - ETA: 3s - loss: 1.0699 - accuracy: 0.7294Learning Rate 0.009508498\n",
      "Learning Rate 0.009507097\n",
      "Learning Rate 0.009505697\n",
      "Learning Rate 0.009504296\n",
      "Learning Rate 0.009502895\n",
      "Learning Rate 0.0095014945\n",
      "Learning Rate 0.009500095\n",
      "Learning Rate 0.009498695\n",
      "Learning Rate 0.009497295\n",
      "11232/25000 [============>.................] - ETA: 3s - loss: 1.0599 - accuracy: 0.7307Learning Rate 0.009495895\n",
      "Learning Rate 0.0094944965\n",
      "Learning Rate 0.009493098\n",
      "Learning Rate 0.009491699\n",
      "Learning Rate 0.0094903\n",
      "Learning Rate 0.009488901\n",
      "Learning Rate 0.009487503\n",
      "Learning Rate 0.009486105\n",
      "Learning Rate 0.009484707\n",
      "11520/25000 [============>.................] - ETA: 3s - loss: 1.0505 - accuracy: 0.7315Learning Rate 0.009483309\n",
      "Learning Rate 0.0094819125\n",
      "Learning Rate 0.0094805155\n",
      "Learning Rate 0.0094791185\n",
      "Learning Rate 0.0094777215\n",
      "Learning Rate 0.009476325\n",
      "Learning Rate 0.0094749285\n",
      "Learning Rate 0.009473532\n",
      "Learning Rate 0.009472136\n",
      "Learning Rate 0.00947074\n",
      "11840/25000 [=============>................] - ETA: 3s - loss: 1.0406 - accuracy: 0.7318Learning Rate 0.009469345\n",
      "Learning Rate 0.00946795\n",
      "Learning Rate 0.009466555\n",
      "Learning Rate 0.00946516\n",
      "Learning Rate 0.009463765\n",
      "Learning Rate 0.009462371\n",
      "Learning Rate 0.009460976\n",
      "Learning Rate 0.009459582\n",
      "Learning Rate 0.009458188\n",
      "12128/25000 [=============>................] - ETA: 2s - loss: 1.0329 - accuracy: 0.7333Learning Rate 0.009456795\n",
      "Learning Rate 0.009455401\n",
      "Learning Rate 0.009454008\n",
      "Learning Rate 0.009452615\n",
      "Learning Rate 0.009451222\n",
      "Learning Rate 0.009449829\n",
      "Learning Rate 0.009448437\n",
      "Learning Rate 0.009447045\n",
      "Learning Rate 0.009445652\n",
      "12416/25000 [=============>................] - ETA: 2s - loss: 1.0260 - accuracy: 0.7344Learning Rate 0.009444261\n",
      "Learning Rate 0.00944287\n",
      "Learning Rate 0.009441478\n",
      "Learning Rate 0.009440087\n",
      "Learning Rate 0.009438695\n",
      "Learning Rate 0.009437305\n",
      "Learning Rate 0.009435914\n",
      "Learning Rate 0.009434524\n",
      "Learning Rate 0.0094331335\n",
      "12704/25000 [==============>...............] - ETA: 2s - loss: 1.0188 - accuracy: 0.7352Learning Rate 0.009431743\n",
      "Learning Rate 0.0094303535\n",
      "Learning Rate 0.009428964\n",
      "Learning Rate 0.009427574\n",
      "Learning Rate 0.009426185\n",
      "Learning Rate 0.009424796\n",
      "Learning Rate 0.009423408\n",
      "Learning Rate 0.009422019\n",
      "Learning Rate 0.0094206305\n",
      "12992/25000 [==============>...............] - ETA: 2s - loss: 1.0113 - accuracy: 0.7366Learning Rate 0.009419242\n",
      "Learning Rate 0.009417854\n",
      "Learning Rate 0.009416467\n",
      "Learning Rate 0.009415079\n",
      "Learning Rate 0.009413691\n",
      "Learning Rate 0.0094123045\n",
      "Learning Rate 0.009410918\n",
      "Learning Rate 0.009409531\n",
      "13248/25000 [==============>...............] - ETA: 2s - loss: 1.0043 - accuracy: 0.7371Learning Rate 0.009408144\n",
      "Learning Rate 0.009406758\n",
      "Learning Rate 0.009405372\n",
      "Learning Rate 0.009403986\n",
      "Learning Rate 0.0094026\n",
      "Learning Rate 0.009401214\n",
      "Learning Rate 0.009399829\n",
      "Learning Rate 0.009398445\n",
      "Learning Rate 0.00939706\n",
      "Learning Rate 0.009395675\n",
      "13568/25000 [===============>..............] - ETA: 2s - loss: 0.9988 - accuracy: 0.7381Learning Rate 0.00939429\n",
      "Learning Rate 0.009392906\n",
      "Learning Rate 0.009391522\n",
      "Learning Rate 0.009390138\n",
      "Learning Rate 0.009388754\n",
      "Learning Rate 0.009387371\n",
      "Learning Rate 0.009385988\n",
      "Learning Rate 0.009384605\n",
      "Learning Rate 0.009383222\n",
      "13856/25000 [===============>..............] - ETA: 2s - loss: 1.0005 - accuracy: 0.7374Learning Rate 0.009381839\n",
      "Learning Rate 0.009380457\n",
      "Learning Rate 0.009379075\n",
      "Learning Rate 0.009377693\n",
      "Learning Rate 0.009376311\n",
      "Learning Rate 0.009374929\n",
      "Learning Rate 0.0093735475\n",
      "Learning Rate 0.009372166\n",
      "Learning Rate 0.009370785\n",
      "14144/25000 [===============>..............] - ETA: 2s - loss: 0.9933 - accuracy: 0.7385Learning Rate 0.009369404\n",
      "Learning Rate 0.009368024\n",
      "Learning Rate 0.009366644\n",
      "Learning Rate 0.009365263\n",
      "Learning Rate 0.009363883\n",
      "Learning Rate 0.009362503\n",
      "Learning Rate 0.009361124\n",
      "Learning Rate 0.009359744\n",
      "Learning Rate 0.009358365\n",
      "14432/25000 [================>.............] - ETA: 2s - loss: 0.9896 - accuracy: 0.7386Learning Rate 0.009356986\n",
      "Learning Rate 0.009355607\n",
      "Learning Rate 0.009354229\n",
      "Learning Rate 0.009352851\n",
      "Learning Rate 0.009351472\n",
      "Learning Rate 0.009350094\n",
      "Learning Rate 0.009348717\n",
      "Learning Rate 0.009347339\n",
      "Learning Rate 0.009345962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14720/25000 [================>.............] - ETA: 2s - loss: 0.9837 - accuracy: 0.7393Learning Rate 0.009344584\n",
      "Learning Rate 0.009343207\n",
      "Learning Rate 0.00934183\n",
      "Learning Rate 0.009340454\n",
      "Learning Rate 0.009339077\n",
      "Learning Rate 0.009337701\n",
      "Learning Rate 0.009336325\n",
      "Learning Rate 0.00933495\n",
      "Learning Rate 0.009333574\n",
      "15008/25000 [=================>............] - ETA: 2s - loss: 0.9792 - accuracy: 0.7403Learning Rate 0.009332199\n",
      "Learning Rate 0.009330823\n",
      "Learning Rate 0.009329448\n",
      "Learning Rate 0.009328074\n",
      "Learning Rate 0.009326699\n",
      "Learning Rate 0.009325325\n",
      "Learning Rate 0.009323951\n",
      "Learning Rate 0.009322577\n",
      "Learning Rate 0.009321203\n",
      "15296/25000 [=================>............] - ETA: 2s - loss: 0.9716 - accuracy: 0.7420Learning Rate 0.00931983\n",
      "Learning Rate 0.009318456\n",
      "Learning Rate 0.009317083\n",
      "Learning Rate 0.0093157105\n",
      "Learning Rate 0.009314338\n",
      "Learning Rate 0.009312965\n",
      "Learning Rate 0.009311592\n",
      "Learning Rate 0.00931022\n",
      "Learning Rate 0.0093088485\n",
      "15584/25000 [=================>............] - ETA: 2s - loss: 0.9679 - accuracy: 0.7420Learning Rate 0.009307477\n",
      "Learning Rate 0.009306105\n",
      "Learning Rate 0.009304734\n",
      "Learning Rate 0.009303363\n",
      "Learning Rate 0.009301992\n",
      "Learning Rate 0.009300621\n",
      "Learning Rate 0.00929925\n",
      "Learning Rate 0.00929788\n",
      "Learning Rate 0.00929651\n",
      "15872/25000 [==================>...........] - ETA: 2s - loss: 0.9641 - accuracy: 0.7423Learning Rate 0.00929514\n",
      "Learning Rate 0.00929377\n",
      "Learning Rate 0.009292401\n",
      "Learning Rate 0.009291032\n",
      "Learning Rate 0.009289663\n",
      "Learning Rate 0.009288294\n",
      "Learning Rate 0.009286925\n",
      "Learning Rate 0.009285557\n",
      "Learning Rate 0.009284189\n",
      "16160/25000 [==================>...........] - ETA: 1s - loss: 0.9568 - accuracy: 0.7434Learning Rate 0.009282821\n",
      "Learning Rate 0.009281453\n",
      "Learning Rate 0.009280085\n",
      "Learning Rate 0.009278717\n",
      "Learning Rate 0.00927735\n",
      "Learning Rate 0.009275983\n",
      "Learning Rate 0.009274616\n",
      "Learning Rate 0.00927325\n",
      "Learning Rate 0.009271883\n",
      "16448/25000 [==================>...........] - ETA: 1s - loss: 0.9535 - accuracy: 0.7436Learning Rate 0.009270517\n",
      "Learning Rate 0.009269151\n",
      "Learning Rate 0.009267785\n",
      "Learning Rate 0.009266419\n",
      "Learning Rate 0.009265054\n",
      "Learning Rate 0.009263689\n",
      "Learning Rate 0.009262323\n",
      "Learning Rate 0.009260959\n",
      "Learning Rate 0.009259595\n",
      "16736/25000 [===================>..........] - ETA: 1s - loss: 0.9494 - accuracy: 0.7437Learning Rate 0.00925823\n",
      "Learning Rate 0.009256866\n",
      "Learning Rate 0.009255501\n",
      "Learning Rate 0.009254138\n",
      "Learning Rate 0.0092527745\n",
      "Learning Rate 0.009251411\n",
      "Learning Rate 0.009250048\n",
      "Learning Rate 0.009248684\n",
      "Learning Rate 0.009247322\n",
      "17024/25000 [===================>..........] - ETA: 1s - loss: 0.9442 - accuracy: 0.7442Learning Rate 0.009245959\n",
      "Learning Rate 0.009244597\n",
      "Learning Rate 0.009243234\n",
      "Learning Rate 0.0092418725\n",
      "Learning Rate 0.009240511\n",
      "Learning Rate 0.009239149\n",
      "Learning Rate 0.009237788\n",
      "Learning Rate 0.009236426\n",
      "Learning Rate 0.009235065\n",
      "17312/25000 [===================>..........] - ETA: 1s - loss: 0.9409 - accuracy: 0.7450Learning Rate 0.009233705\n",
      "Learning Rate 0.009232344\n",
      "Learning Rate 0.009230983\n",
      "Learning Rate 0.009229623\n",
      "Learning Rate 0.009228263\n",
      "Learning Rate 0.009226903\n",
      "Learning Rate 0.009225544\n",
      "Learning Rate 0.009224184\n",
      "Learning Rate 0.009222825\n",
      "17600/25000 [====================>.........] - ETA: 1s - loss: 0.9391 - accuracy: 0.7451Learning Rate 0.009221466\n",
      "Learning Rate 0.009220107\n",
      "Learning Rate 0.009218749\n",
      "Learning Rate 0.00921739\n",
      "Learning Rate 0.009216032\n",
      "Learning Rate 0.009214674\n",
      "Learning Rate 0.009213316\n",
      "Learning Rate 0.009211958\n",
      "Learning Rate 0.0092106005\n",
      "17888/25000 [====================>.........] - ETA: 1s - loss: 0.9369 - accuracy: 0.7452Learning Rate 0.009209244\n",
      "Learning Rate 0.009207887\n",
      "Learning Rate 0.00920653\n",
      "Learning Rate 0.009205173\n",
      "Learning Rate 0.009203817\n",
      "Learning Rate 0.009202461\n",
      "Learning Rate 0.009201105\n",
      "Learning Rate 0.009199749\n",
      "Learning Rate 0.009198393\n",
      "18176/25000 [====================>.........] - ETA: 1s - loss: 0.9337 - accuracy: 0.7455Learning Rate 0.009197038\n",
      "Learning Rate 0.009195683\n",
      "Learning Rate 0.0091943275\n",
      "Learning Rate 0.009192972\n",
      "Learning Rate 0.009191617\n",
      "Learning Rate 0.009190263\n",
      "Learning Rate 0.009188909\n",
      "Learning Rate 0.009187555\n",
      "18432/25000 [=====================>........] - ETA: 1s - loss: 0.9309 - accuracy: 0.7462Learning Rate 0.009186201\n",
      "Learning Rate 0.009184848\n",
      "Learning Rate 0.009183494\n",
      "Learning Rate 0.009182141\n",
      "Learning Rate 0.009180788\n",
      "Learning Rate 0.009179435\n",
      "Learning Rate 0.009178082\n",
      "Learning Rate 0.00917673\n",
      "Learning Rate 0.009175378\n",
      "18720/25000 [=====================>........] - ETA: 1s - loss: 0.9279 - accuracy: 0.7466Learning Rate 0.009174026\n",
      "Learning Rate 0.009172673\n",
      "Learning Rate 0.009171322\n",
      "Learning Rate 0.009169971\n",
      "Learning Rate 0.009168619\n",
      "Learning Rate 0.009167268\n",
      "Learning Rate 0.0091659175\n",
      "Learning Rate 0.009164567\n",
      "Learning Rate 0.009163217\n",
      "19008/25000 [=====================>........] - ETA: 1s - loss: 0.9239 - accuracy: 0.7468Learning Rate 0.009161866\n",
      "Learning Rate 0.009160516\n",
      "Learning Rate 0.009159166\n",
      "Learning Rate 0.009157817\n",
      "Learning Rate 0.009156467\n",
      "Learning Rate 0.009155118\n",
      "Learning Rate 0.009153768\n",
      "Learning Rate 0.00915242\n",
      "Learning Rate 0.009151071\n",
      "19296/25000 [======================>.......] - ETA: 1s - loss: 0.9205 - accuracy: 0.7472Learning Rate 0.009149723\n",
      "Learning Rate 0.009148374\n",
      "Learning Rate 0.009147027\n",
      "Learning Rate 0.009145679\n",
      "Learning Rate 0.009144331\n",
      "Learning Rate 0.009142984\n",
      "Learning Rate 0.009141636\n",
      "Learning Rate 0.009140289\n",
      "Learning Rate 0.009138943\n",
      "19584/25000 [======================>.......] - ETA: 1s - loss: 0.9187 - accuracy: 0.7473Learning Rate 0.009137596\n",
      "Learning Rate 0.009136249\n",
      "Learning Rate 0.009134903\n",
      "Learning Rate 0.009133557\n",
      "Learning Rate 0.009132211\n",
      "Learning Rate 0.009130865\n",
      "Learning Rate 0.00912952\n",
      "Learning Rate 0.009128175\n",
      "Learning Rate 0.00912683\n",
      "19872/25000 [======================>.......] - ETA: 1s - loss: 0.9170 - accuracy: 0.7478Learning Rate 0.009125485\n",
      "Learning Rate 0.00912414\n",
      "Learning Rate 0.009122795\n",
      "Learning Rate 0.0091214515\n",
      "Learning Rate 0.009120108\n",
      "Learning Rate 0.009118764\n",
      "Learning Rate 0.00911742\n",
      "Learning Rate 0.009116076\n",
      "Learning Rate 0.009114733\n",
      "20160/25000 [=======================>......] - ETA: 1s - loss: 0.9138 - accuracy: 0.7480Learning Rate 0.00911339\n",
      "Learning Rate 0.009112047\n",
      "Learning Rate 0.009110704\n",
      "Learning Rate 0.009109361\n",
      "Learning Rate 0.009108019\n",
      "Learning Rate 0.009106677\n",
      "Learning Rate 0.009105335\n",
      "Learning Rate 0.009103993\n",
      "Learning Rate 0.009102652\n",
      "20448/25000 [=======================>......] - ETA: 0s - loss: 0.9095 - accuracy: 0.7490Learning Rate 0.009101311\n",
      "Learning Rate 0.00909997\n",
      "Learning Rate 0.009098629\n",
      "Learning Rate 0.009097287\n",
      "Learning Rate 0.009095947\n",
      "Learning Rate 0.009094607\n",
      "Learning Rate 0.009093267\n",
      "Learning Rate 0.009091927\n",
      "20704/25000 [=======================>......] - ETA: 0s - loss: 0.9083 - accuracy: 0.7495Learning Rate 0.009090587\n",
      "Learning Rate 0.009089247\n",
      "Learning Rate 0.009087908\n",
      "Learning Rate 0.009086569\n",
      "Learning Rate 0.00908523\n",
      "Learning Rate 0.009083891\n",
      "Learning Rate 0.009082553\n",
      "Learning Rate 0.009081215\n",
      "Learning Rate 0.009079876\n",
      "20992/25000 [========================>.....] - ETA: 0s - loss: 0.9046 - accuracy: 0.7500Learning Rate 0.009078538\n",
      "Learning Rate 0.009077201\n",
      "Learning Rate 0.009075863\n",
      "Learning Rate 0.009074526\n",
      "Learning Rate 0.0090731885\n",
      "Learning Rate 0.009071851\n",
      "Learning Rate 0.009070515\n",
      "Learning Rate 0.009069178\n",
      "Learning Rate 0.009067842\n",
      "Learning Rate 0.009066505\n",
      "21312/25000 [========================>.....] - ETA: 0s - loss: 0.9013 - accuracy: 0.7507Learning Rate 0.009065169\n",
      "Learning Rate 0.009063833\n",
      "Learning Rate 0.009062498\n",
      "Learning Rate 0.009061162\n",
      "Learning Rate 0.009059827\n",
      "Learning Rate 0.009058492\n",
      "Learning Rate 0.009057158\n",
      "Learning Rate 0.009055823\n",
      "Learning Rate 0.0090544885\n",
      "21600/25000 [========================>.....] - ETA: 0s - loss: 0.8980 - accuracy: 0.7514Learning Rate 0.009053154\n",
      "Learning Rate 0.00905182\n",
      "Learning Rate 0.009050487\n",
      "Learning Rate 0.009049153\n",
      "Learning Rate 0.009047819\n",
      "Learning Rate 0.009046486\n",
      "Learning Rate 0.009045153\n",
      "Learning Rate 0.00904382\n",
      "Learning Rate 0.0090424875\n",
      "21888/25000 [=========================>....] - ETA: 0s - loss: 0.8943 - accuracy: 0.7517Learning Rate 0.009041155\n",
      "Learning Rate 0.009039823\n",
      "Learning Rate 0.009038491\n",
      "Learning Rate 0.009037159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.009035828\n",
      "Learning Rate 0.009034496\n",
      "Learning Rate 0.009033165\n",
      "Learning Rate 0.009031834\n",
      "Learning Rate 0.009030503\n",
      "22176/25000 [=========================>....] - ETA: 0s - loss: 0.8908 - accuracy: 0.7523Learning Rate 0.009029172\n",
      "Learning Rate 0.0090278415\n",
      "Learning Rate 0.009026512\n",
      "Learning Rate 0.009025182\n",
      "Learning Rate 0.009023852\n",
      "Learning Rate 0.009022522\n",
      "Learning Rate 0.009021192\n",
      "Learning Rate 0.009019863\n",
      "Learning Rate 0.009018534\n",
      "Learning Rate 0.009017205\n",
      "22496/25000 [=========================>....] - ETA: 0s - loss: 0.8894 - accuracy: 0.7522Learning Rate 0.009015876\n",
      "Learning Rate 0.009014548\n",
      "Learning Rate 0.00901322\n",
      "Learning Rate 0.009011892\n",
      "Learning Rate 0.009010564\n",
      "Learning Rate 0.009009236\n",
      "Learning Rate 0.009007908\n",
      "Learning Rate 0.009006581\n",
      "Learning Rate 0.009005254\n",
      "22784/25000 [==========================>...] - ETA: 0s - loss: 0.8856 - accuracy: 0.7529Learning Rate 0.009003927\n",
      "Learning Rate 0.0090026\n",
      "Learning Rate 0.009001274\n",
      "Learning Rate 0.008999947\n",
      "Learning Rate 0.008998621\n",
      "Learning Rate 0.008997295\n",
      "Learning Rate 0.008995969\n",
      "Learning Rate 0.008994644\n",
      "Learning Rate 0.008993318\n",
      "23072/25000 [==========================>...] - ETA: 0s - loss: 0.8823 - accuracy: 0.7533Learning Rate 0.008991993\n",
      "Learning Rate 0.008990668\n",
      "Learning Rate 0.0089893425\n",
      "Learning Rate 0.008988018\n",
      "Learning Rate 0.008986694\n",
      "Learning Rate 0.0089853695\n",
      "Learning Rate 0.008984045\n",
      "Learning Rate 0.008982722\n",
      "Learning Rate 0.008981398\n",
      "Learning Rate 0.008980075\n",
      "23392/25000 [===========================>..] - ETA: 0s - loss: 0.8792 - accuracy: 0.7536Learning Rate 0.0089787515\n",
      "Learning Rate 0.008977428\n",
      "Learning Rate 0.008976106\n",
      "Learning Rate 0.008974783\n",
      "Learning Rate 0.008973461\n",
      "Learning Rate 0.008972138\n",
      "Learning Rate 0.008970816\n",
      "Learning Rate 0.008969494\n",
      "Learning Rate 0.008968173\n",
      "23680/25000 [===========================>..] - ETA: 0s - loss: 0.8765 - accuracy: 0.7542Learning Rate 0.008966851\n",
      "Learning Rate 0.0089655295\n",
      "Learning Rate 0.008964208\n",
      "Learning Rate 0.008962887\n",
      "Learning Rate 0.008961567\n",
      "Learning Rate 0.008960246\n",
      "Learning Rate 0.0089589255\n",
      "Learning Rate 0.008957606\n",
      "Learning Rate 0.008956286\n",
      "23968/25000 [===========================>..] - ETA: 0s - loss: 0.8733 - accuracy: 0.7549Learning Rate 0.008954966\n",
      "Learning Rate 0.008953647\n",
      "Learning Rate 0.008952327\n",
      "Learning Rate 0.008951008\n",
      "Learning Rate 0.00894969\n",
      "Learning Rate 0.008948371\n",
      "Learning Rate 0.008947052\n",
      "Learning Rate 0.008945733\n",
      "Learning Rate 0.0089444155\n",
      "24256/25000 [============================>.] - ETA: 0s - loss: 0.8725 - accuracy: 0.7549Learning Rate 0.008943098\n",
      "Learning Rate 0.00894178\n",
      "Learning Rate 0.008940462\n",
      "Learning Rate 0.008939144\n",
      "Learning Rate 0.008937827\n",
      "Learning Rate 0.00893651\n",
      "Learning Rate 0.0089351935\n",
      "Learning Rate 0.008933877\n",
      "Learning Rate 0.00893256\n",
      "24544/25000 [============================>.] - ETA: 0s - loss: 0.8681 - accuracy: 0.7560Learning Rate 0.008931244\n",
      "Learning Rate 0.008929928\n",
      "Learning Rate 0.008928612\n",
      "Learning Rate 0.008927296\n",
      "Learning Rate 0.008925981\n",
      "Learning Rate 0.008924666\n",
      "Learning Rate 0.008923351\n",
      "Learning Rate 0.008922036\n",
      "Learning Rate 0.008920721\n",
      "24832/25000 [============================>.] - ETA: 0s - loss: 0.8648 - accuracy: 0.7563Learning Rate 0.008919407\n",
      "Learning Rate 0.008918093\n",
      "Learning Rate 0.0089167785\n",
      "Learning Rate 0.008915464\n",
      "Learning Rate 0.00891415\n",
      "Learning Rate 0.008912837\n",
      "25000/25000 [==============================] - 6s 226us/sample - loss: 0.8652 - accuracy: 0.7560 - val_loss: 1.0560 - val_accuracy: 0.6680\n",
      "Epoch 2/5\n",
      "Learning Rate 0.008911524\n",
      "   32/25000 [..............................] - ETA: 5s - loss: 0.8151 - accuracy: 0.7500Learning Rate 0.008910211\n",
      "Learning Rate 0.008908898\n",
      "Learning Rate 0.0089075845\n",
      "Learning Rate 0.008906272\n",
      "Learning Rate 0.00890496\n",
      "Learning Rate 0.008903648\n",
      "Learning Rate 0.008902336\n",
      "Learning Rate 0.008901023\n",
      "Learning Rate 0.008899712\n",
      "Learning Rate 0.008898401\n",
      "  352/25000 [..............................] - ETA: 4s - loss: 0.7687 - accuracy: 0.7642Learning Rate 0.008897089\n",
      "Learning Rate 0.008895778\n",
      "Learning Rate 0.008894468\n",
      "Learning Rate 0.008893157\n",
      "Learning Rate 0.008891847\n",
      "Learning Rate 0.008890537\n",
      "Learning Rate 0.008889226\n",
      "Learning Rate 0.008887917\n",
      "Learning Rate 0.008886607\n",
      "  640/25000 [..............................] - ETA: 4s - loss: 0.7088 - accuracy: 0.7750Learning Rate 0.008885298\n",
      "Learning Rate 0.0088839885\n",
      "Learning Rate 0.008882679\n",
      "Learning Rate 0.008881371\n",
      "Learning Rate 0.008880062\n",
      "Learning Rate 0.0088787535\n",
      "Learning Rate 0.008877445\n",
      "Learning Rate 0.0088761365\n",
      "Learning Rate 0.008874829\n",
      "  928/25000 [>.............................] - ETA: 4s - loss: 0.6631 - accuracy: 0.7780Learning Rate 0.008873521\n",
      "Learning Rate 0.008872214\n",
      "Learning Rate 0.008870906\n",
      "Learning Rate 0.008869599\n",
      "Learning Rate 0.008868292\n",
      "Learning Rate 0.008866985\n",
      "Learning Rate 0.008865679\n",
      "Learning Rate 0.008864372\n",
      "Learning Rate 0.008863065\n",
      " 1216/25000 [>.............................] - ETA: 4s - loss: 0.6511 - accuracy: 0.7771Learning Rate 0.00886176\n",
      "Learning Rate 0.008860454\n",
      "Learning Rate 0.008859148\n",
      "Learning Rate 0.008857843\n",
      "Learning Rate 0.008856538\n",
      "Learning Rate 0.008855233\n",
      "Learning Rate 0.008853928\n",
      "Learning Rate 0.008852623\n",
      "Learning Rate 0.008851319\n",
      " 1504/25000 [>.............................] - ETA: 4s - loss: 0.6254 - accuracy: 0.7879Learning Rate 0.008850015\n",
      "Learning Rate 0.008848711\n",
      "Learning Rate 0.008847407\n",
      "Learning Rate 0.008846103\n",
      "Learning Rate 0.008844799\n",
      "Learning Rate 0.008843496\n",
      "Learning Rate 0.0088421935\n",
      "Learning Rate 0.008840891\n",
      "Learning Rate 0.008839588\n",
      " 1792/25000 [=>............................] - ETA: 4s - loss: 0.6424 - accuracy: 0.7863Learning Rate 0.008838285\n",
      "Learning Rate 0.008836983\n",
      "Learning Rate 0.008835681\n",
      "Learning Rate 0.008834379\n",
      "Learning Rate 0.008833077\n",
      "Learning Rate 0.008831775\n",
      "Learning Rate 0.008830474\n",
      "Learning Rate 0.008829173\n",
      "Learning Rate 0.008827872\n",
      "Learning Rate 0.008826571\n",
      " 2112/25000 [=>............................] - ETA: 4s - loss: 0.6503 - accuracy: 0.7803Learning Rate 0.0088252695\n",
      "Learning Rate 0.008823969\n",
      "Learning Rate 0.008822669\n",
      "Learning Rate 0.008821369\n",
      "Learning Rate 0.008820069\n",
      "Learning Rate 0.008818769\n",
      "Learning Rate 0.00881747\n",
      "Learning Rate 0.0088161705\n",
      "Learning Rate 0.008814871\n",
      " 2400/25000 [=>............................] - ETA: 4s - loss: 0.6337 - accuracy: 0.7850Learning Rate 0.008813572\n",
      "Learning Rate 0.008812274\n",
      "Learning Rate 0.008810976\n",
      "Learning Rate 0.008809677\n",
      "Learning Rate 0.008808379\n",
      "Learning Rate 0.008807081\n",
      "Learning Rate 0.008805783\n",
      "Learning Rate 0.008804486\n",
      "Learning Rate 0.008803189\n",
      "Learning Rate 0.0088018915\n",
      " 2720/25000 [==>...........................] - ETA: 3s - loss: 0.6260 - accuracy: 0.7857Learning Rate 0.008800594\n",
      "Learning Rate 0.008799298\n",
      "Learning Rate 0.008798001\n",
      "Learning Rate 0.008796705\n",
      "Learning Rate 0.0087954085\n",
      "Learning Rate 0.008794112\n",
      "Learning Rate 0.008792817\n",
      "Learning Rate 0.008791521\n",
      "Learning Rate 0.008790226\n",
      " 3008/25000 [==>...........................] - ETA: 3s - loss: 0.6361 - accuracy: 0.7856Learning Rate 0.00878893\n",
      "Learning Rate 0.008787635\n",
      "Learning Rate 0.00878634\n",
      "Learning Rate 0.008785046\n",
      "Learning Rate 0.008783751\n",
      "Learning Rate 0.008782457\n",
      "Learning Rate 0.008781162\n",
      "Learning Rate 0.0087798685\n",
      "Learning Rate 0.008778575\n",
      " 3296/25000 [==>...........................] - ETA: 3s - loss: 0.6374 - accuracy: 0.7882Learning Rate 0.008777281\n",
      "Learning Rate 0.008775988\n",
      "Learning Rate 0.008774694\n",
      "Learning Rate 0.008773401\n",
      "Learning Rate 0.008772109\n",
      "Learning Rate 0.008770816\n",
      "Learning Rate 0.008769523\n",
      "Learning Rate 0.008768231\n",
      "Learning Rate 0.008766939\n",
      " 3584/25000 [===>..........................] - ETA: 3s - loss: 0.6484 - accuracy: 0.7852Learning Rate 0.008765647\n",
      "Learning Rate 0.008764355\n",
      "Learning Rate 0.008763064\n",
      "Learning Rate 0.008761773\n",
      "Learning Rate 0.008760482\n",
      "Learning Rate 0.008759191\n",
      "Learning Rate 0.0087579\n",
      "Learning Rate 0.00875661\n",
      "Learning Rate 0.00875532\n",
      " 3872/25000 [===>..........................] - ETA: 3s - loss: 0.6498 - accuracy: 0.7843Learning Rate 0.00875403\n",
      "Learning Rate 0.00875274\n",
      "Learning Rate 0.00875145\n",
      "Learning Rate 0.00875016\n",
      "Learning Rate 0.008748871\n",
      "Learning Rate 0.008747582\n",
      "Learning Rate 0.008746293\n",
      "Learning Rate 0.008745004\n",
      "Learning Rate 0.0087437155\n",
      " 4160/25000 [===>..........................] - ETA: 3s - loss: 0.6448 - accuracy: 0.7841Learning Rate 0.008742427\n",
      "Learning Rate 0.008741139\n",
      "Learning Rate 0.008739851\n",
      "Learning Rate 0.008738563\n",
      "Learning Rate 0.008737275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.008735988\n",
      "Learning Rate 0.008734701\n",
      "Learning Rate 0.008733414\n",
      "Learning Rate 0.008732127\n",
      " 4448/25000 [====>.........................] - ETA: 3s - loss: 0.6400 - accuracy: 0.7846Learning Rate 0.00873084\n",
      "Learning Rate 0.008729554\n",
      "Learning Rate 0.008728268\n",
      "Learning Rate 0.0087269815\n",
      "Learning Rate 0.008725695\n",
      "Learning Rate 0.008724409\n",
      "Learning Rate 0.008723124\n",
      "Learning Rate 0.008721839\n",
      "Learning Rate 0.0087205535\n",
      "Learning Rate 0.008719268\n",
      " 4768/25000 [====>.........................] - ETA: 3s - loss: 0.6303 - accuracy: 0.7873Learning Rate 0.008717983\n",
      "Learning Rate 0.008716699\n",
      "Learning Rate 0.008715414\n",
      "Learning Rate 0.00871413\n",
      "Learning Rate 0.008712846\n",
      "Learning Rate 0.008711562\n",
      "Learning Rate 0.008710278\n",
      "Learning Rate 0.008708995\n",
      "Learning Rate 0.0087077115\n",
      " 5056/25000 [=====>........................] - ETA: 3s - loss: 0.6322 - accuracy: 0.7884Learning Rate 0.008706428\n",
      "Learning Rate 0.008705145\n",
      "Learning Rate 0.008703862\n",
      "Learning Rate 0.00870258\n",
      "Learning Rate 0.008701297\n",
      "Learning Rate 0.008700015\n",
      "Learning Rate 0.008698733\n",
      "Learning Rate 0.008697451\n",
      "Learning Rate 0.00869617\n",
      "Learning Rate 0.008694888\n",
      " 5376/25000 [=====>........................] - ETA: 3s - loss: 0.6268 - accuracy: 0.7892Learning Rate 0.008693607\n",
      "Learning Rate 0.008692325\n",
      "Learning Rate 0.0086910445\n",
      "Learning Rate 0.008689764\n",
      "Learning Rate 0.008688483\n",
      "Learning Rate 0.008687203\n",
      "Learning Rate 0.008685923\n",
      "Learning Rate 0.008684644\n",
      "Learning Rate 0.008683364\n",
      " 5664/25000 [=====>........................] - ETA: 3s - loss: 0.6248 - accuracy: 0.7888Learning Rate 0.008682084\n",
      "Learning Rate 0.008680805\n",
      "Learning Rate 0.008679526\n",
      "Learning Rate 0.008678247\n",
      "Learning Rate 0.0086769685\n",
      "Learning Rate 0.00867569\n",
      "Learning Rate 0.008674411\n",
      "Learning Rate 0.008673133\n",
      "Learning Rate 0.008671856\n",
      " 5952/25000 [======>.......................] - ETA: 3s - loss: 0.6215 - accuracy: 0.7895Learning Rate 0.008670578\n",
      "Learning Rate 0.0086693\n",
      "Learning Rate 0.008668022\n",
      "Learning Rate 0.008666745\n",
      "Learning Rate 0.008665469\n",
      "Learning Rate 0.008664192\n",
      "Learning Rate 0.008662915\n",
      "Learning Rate 0.008661638\n",
      "Learning Rate 0.008660362\n",
      "Learning Rate 0.008659086\n",
      " 6272/25000 [======>.......................] - ETA: 3s - loss: 0.6197 - accuracy: 0.7902Learning Rate 0.00865781\n",
      "Learning Rate 0.008656534\n",
      "Learning Rate 0.008655258\n",
      "Learning Rate 0.0086539835\n",
      "Learning Rate 0.0086527085\n",
      "Learning Rate 0.0086514335\n",
      "Learning Rate 0.0086501585\n",
      "Learning Rate 0.008648884\n",
      "Learning Rate 0.0086476095\n",
      " 6560/25000 [======>.......................] - ETA: 3s - loss: 0.6218 - accuracy: 0.7909Learning Rate 0.008646335\n",
      "Learning Rate 0.008645061\n",
      "Learning Rate 0.008643787\n",
      "Learning Rate 0.008642513\n",
      "Learning Rate 0.00864124\n",
      "Learning Rate 0.008639967\n",
      "Learning Rate 0.008638694\n",
      "Learning Rate 0.008637421\n",
      "Learning Rate 0.008636148\n",
      " 6848/25000 [=======>......................] - ETA: 3s - loss: 0.6181 - accuracy: 0.7929Learning Rate 0.0086348755\n",
      "Learning Rate 0.008633603\n",
      "Learning Rate 0.008632331\n",
      "Learning Rate 0.008631059\n",
      "Learning Rate 0.008629787\n",
      "Learning Rate 0.0086285155\n",
      "Learning Rate 0.008627244\n",
      "Learning Rate 0.008625973\n",
      "Learning Rate 0.008624702\n",
      " 7136/25000 [=======>......................] - ETA: 3s - loss: 0.6210 - accuracy: 0.7941Learning Rate 0.0086234305\n",
      "Learning Rate 0.00862216\n",
      "Learning Rate 0.00862089\n",
      "Learning Rate 0.0086196195\n",
      "Learning Rate 0.008618349\n",
      "Learning Rate 0.008617079\n",
      "Learning Rate 0.0086158095\n",
      "Learning Rate 0.00861454\n",
      "Learning Rate 0.008613271\n",
      " 7424/25000 [=======>......................] - ETA: 3s - loss: 0.6188 - accuracy: 0.7945Learning Rate 0.008612001\n",
      "Learning Rate 0.008610732\n",
      "Learning Rate 0.008609463\n",
      "Learning Rate 0.008608195\n",
      "Learning Rate 0.008606927\n",
      "Learning Rate 0.008605658\n",
      "Learning Rate 0.00860439\n",
      "Learning Rate 0.008603122\n",
      "Learning Rate 0.008601855\n",
      " 7712/25000 [========>.....................] - ETA: 3s - loss: 0.6217 - accuracy: 0.7928Learning Rate 0.008600587\n",
      "Learning Rate 0.0085993195\n",
      "Learning Rate 0.008598052\n",
      "Learning Rate 0.008596785\n",
      "Learning Rate 0.008595519\n",
      "Learning Rate 0.008594252\n",
      "Learning Rate 0.008592986\n",
      "Learning Rate 0.008591719\n",
      "Learning Rate 0.008590453\n",
      " 8000/25000 [========>.....................] - ETA: 3s - loss: 0.6245 - accuracy: 0.7926Learning Rate 0.008589188\n",
      "Learning Rate 0.008587922\n",
      "Learning Rate 0.008586656\n",
      "Learning Rate 0.008585391\n",
      "Learning Rate 0.008584126\n",
      "Learning Rate 0.008582861\n",
      "Learning Rate 0.008581596\n",
      "Learning Rate 0.008580332\n",
      "Learning Rate 0.008579067\n",
      " 8288/25000 [========>.....................] - ETA: 3s - loss: 0.6263 - accuracy: 0.7913Learning Rate 0.008577803\n",
      "Learning Rate 0.008576539\n",
      "Learning Rate 0.008575276\n",
      "Learning Rate 0.008574012\n",
      "Learning Rate 0.008572748\n",
      "Learning Rate 0.008571485\n",
      "Learning Rate 0.008570222\n",
      "Learning Rate 0.008568959\n",
      " 8544/25000 [=========>....................] - ETA: 2s - loss: 0.6283 - accuracy: 0.7906Learning Rate 0.008567696\n",
      "Learning Rate 0.008566434\n",
      "Learning Rate 0.008565172\n",
      "Learning Rate 0.00856391\n",
      "Learning Rate 0.008562648\n",
      "Learning Rate 0.008561386\n",
      "Learning Rate 0.008560124\n",
      "Learning Rate 0.008558863\n",
      "Learning Rate 0.008557602\n",
      " 8832/25000 [=========>....................] - ETA: 2s - loss: 0.6288 - accuracy: 0.7908Learning Rate 0.008556341\n",
      "Learning Rate 0.00855508\n",
      "Learning Rate 0.008553819\n",
      "Learning Rate 0.008552559\n",
      "Learning Rate 0.008551299\n",
      "Learning Rate 0.008550039\n",
      "Learning Rate 0.0085487785\n",
      "Learning Rate 0.008547518\n",
      "Learning Rate 0.008546259\n",
      " 9120/25000 [=========>....................] - ETA: 2s - loss: 0.6375 - accuracy: 0.7894Learning Rate 0.008545\n",
      "Learning Rate 0.008543741\n",
      "Learning Rate 0.008542482\n",
      "Learning Rate 0.008541223\n",
      "Learning Rate 0.008539964\n",
      "Learning Rate 0.008538706\n",
      "Learning Rate 0.008537448\n",
      "Learning Rate 0.00853619\n",
      " 9376/25000 [==========>...................] - ETA: 2s - loss: 0.6380 - accuracy: 0.7888Learning Rate 0.008534932\n",
      "Learning Rate 0.008533674\n",
      "Learning Rate 0.008532417\n",
      "Learning Rate 0.00853116\n",
      "Learning Rate 0.008529902\n",
      "Learning Rate 0.008528645\n",
      "Learning Rate 0.008527389\n",
      "Learning Rate 0.008526132\n",
      "Learning Rate 0.008524876\n",
      " 9664/25000 [==========>...................] - ETA: 2s - loss: 0.6367 - accuracy: 0.7892Learning Rate 0.00852362\n",
      "Learning Rate 0.008522363\n",
      "Learning Rate 0.008521108\n",
      "Learning Rate 0.0085198525\n",
      "Learning Rate 0.008518597\n",
      "Learning Rate 0.008517342\n",
      "Learning Rate 0.008516086\n",
      "Learning Rate 0.008514832\n",
      "Learning Rate 0.008513577\n",
      " 9952/25000 [==========>...................] - ETA: 2s - loss: 0.6351 - accuracy: 0.7901Learning Rate 0.008512323\n",
      "Learning Rate 0.008511068\n",
      "Learning Rate 0.008509814\n",
      "Learning Rate 0.00850856\n",
      "Learning Rate 0.008507307\n",
      "Learning Rate 0.008506053\n",
      "Learning Rate 0.0085048\n",
      "Learning Rate 0.008503546\n",
      "Learning Rate 0.008502293\n",
      "10240/25000 [===========>..................] - ETA: 2s - loss: 0.6305 - accuracy: 0.7909Learning Rate 0.008501041\n",
      "Learning Rate 0.008499788\n",
      "Learning Rate 0.0084985355\n",
      "Learning Rate 0.008497283\n",
      "Learning Rate 0.008496031\n",
      "Learning Rate 0.0084947795\n",
      "Learning Rate 0.008493528\n",
      "Learning Rate 0.008492276\n",
      "Learning Rate 0.008491024\n",
      "10528/25000 [===========>..................] - ETA: 2s - loss: 0.6316 - accuracy: 0.7912Learning Rate 0.008489774\n",
      "Learning Rate 0.008488523\n",
      "Learning Rate 0.008487272\n",
      "Learning Rate 0.008486021\n",
      "Learning Rate 0.008484771\n",
      "Learning Rate 0.008483521\n",
      "Learning Rate 0.008482271\n",
      "Learning Rate 0.008481021\n",
      "Learning Rate 0.008479771\n",
      "10816/25000 [===========>..................] - ETA: 2s - loss: 0.6324 - accuracy: 0.7910Learning Rate 0.008478521\n",
      "Learning Rate 0.0084772725\n",
      "Learning Rate 0.008476024\n",
      "Learning Rate 0.008474775\n",
      "Learning Rate 0.008473526\n",
      "Learning Rate 0.008472277\n",
      "Learning Rate 0.008471029\n",
      "Learning Rate 0.008469781\n",
      "Learning Rate 0.008468533\n",
      "11104/25000 [============>.................] - ETA: 2s - loss: 0.6308 - accuracy: 0.7913Learning Rate 0.008467285\n",
      "Learning Rate 0.008466037\n",
      "Learning Rate 0.008464789\n",
      "Learning Rate 0.008463542\n",
      "Learning Rate 0.008462295\n",
      "Learning Rate 0.008461048\n",
      "Learning Rate 0.008459801\n",
      "Learning Rate 0.008458554\n",
      "Learning Rate 0.008457308\n",
      "11392/25000 [============>.................] - ETA: 2s - loss: 0.6285 - accuracy: 0.7915Learning Rate 0.008456062\n",
      "Learning Rate 0.0084548155\n",
      "Learning Rate 0.008453569\n",
      "Learning Rate 0.008452323\n",
      "Learning Rate 0.008451078\n",
      "Learning Rate 0.008449833\n",
      "Learning Rate 0.008448588\n",
      "Learning Rate 0.008447343\n",
      "Learning Rate 0.008446097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11680/25000 [=============>................] - ETA: 2s - loss: 0.6297 - accuracy: 0.7910Learning Rate 0.008444853\n",
      "Learning Rate 0.008443609\n",
      "Learning Rate 0.008442365\n",
      "Learning Rate 0.00844112\n",
      "Learning Rate 0.008439876\n",
      "Learning Rate 0.008438633\n",
      "Learning Rate 0.0084373895\n",
      "Learning Rate 0.008436146\n",
      "Learning Rate 0.008434903\n",
      "11968/25000 [=============>................] - ETA: 2s - loss: 0.6349 - accuracy: 0.7903Learning Rate 0.00843366\n",
      "Learning Rate 0.008432417\n",
      "Learning Rate 0.008431175\n",
      "Learning Rate 0.008429932\n",
      "Learning Rate 0.00842869\n",
      "Learning Rate 0.008427448\n",
      "Learning Rate 0.008426206\n",
      "Learning Rate 0.008424965\n",
      "Learning Rate 0.008423723\n",
      "12256/25000 [=============>................] - ETA: 2s - loss: 0.6349 - accuracy: 0.7904Learning Rate 0.008422482\n",
      "Learning Rate 0.00842124\n",
      "Learning Rate 0.00842\n",
      "Learning Rate 0.008418759\n",
      "Learning Rate 0.008417519\n",
      "Learning Rate 0.008416278\n",
      "Learning Rate 0.008415038\n",
      "Learning Rate 0.008413798\n",
      "Learning Rate 0.008412559\n",
      "12544/25000 [==============>...............] - ETA: 2s - loss: 0.6388 - accuracy: 0.7895Learning Rate 0.008411319\n",
      "Learning Rate 0.008410079\n",
      "Learning Rate 0.00840884\n",
      "Learning Rate 0.008407601\n",
      "Learning Rate 0.0084063625\n",
      "Learning Rate 0.008405124\n",
      "Learning Rate 0.008403885\n",
      "Learning Rate 0.0084026465\n",
      "Learning Rate 0.008401409\n",
      "12832/25000 [==============>...............] - ETA: 2s - loss: 0.6382 - accuracy: 0.7894Learning Rate 0.008400171\n",
      "Learning Rate 0.008398933\n",
      "Learning Rate 0.008397696\n",
      "Learning Rate 0.008396458\n",
      "Learning Rate 0.008395221\n",
      "Learning Rate 0.008393984\n",
      "Learning Rate 0.0083927475\n",
      "Learning Rate 0.008391511\n",
      "Learning Rate 0.008390274\n",
      "13120/25000 [==============>...............] - ETA: 2s - loss: 0.6364 - accuracy: 0.7898Learning Rate 0.008389037\n",
      "Learning Rate 0.008387801\n",
      "Learning Rate 0.008386565\n",
      "Learning Rate 0.0083853295\n",
      "Learning Rate 0.008384094\n",
      "Learning Rate 0.008382858\n",
      "Learning Rate 0.008381623\n",
      "Learning Rate 0.008380388\n",
      "Learning Rate 0.008379153\n",
      "13408/25000 [===============>..............] - ETA: 2s - loss: 0.6399 - accuracy: 0.7893Learning Rate 0.008377918\n",
      "Learning Rate 0.008376683\n",
      "Learning Rate 0.008375449\n",
      "Learning Rate 0.008374215\n",
      "Learning Rate 0.008372981\n",
      "Learning Rate 0.008371747\n",
      "Learning Rate 0.008370513\n",
      "Learning Rate 0.00836928\n",
      "Learning Rate 0.008368047\n",
      "13696/25000 [===============>..............] - ETA: 2s - loss: 0.6402 - accuracy: 0.7891Learning Rate 0.008366814\n",
      "Learning Rate 0.008365581\n",
      "Learning Rate 0.008364348\n",
      "Learning Rate 0.008363116\n",
      "Learning Rate 0.008361883\n",
      "Learning Rate 0.008360651\n",
      "Learning Rate 0.008359419\n",
      "Learning Rate 0.008358187\n",
      "13952/25000 [===============>..............] - ETA: 2s - loss: 0.6393 - accuracy: 0.7892Learning Rate 0.008356956\n",
      "Learning Rate 0.008355725\n",
      "Learning Rate 0.008354493\n",
      "Learning Rate 0.008353262\n",
      "Learning Rate 0.008352031\n",
      "Learning Rate 0.008350801\n",
      "Learning Rate 0.00834957\n",
      "Learning Rate 0.00834834\n",
      "Learning Rate 0.00834711\n",
      "14240/25000 [================>.............] - ETA: 1s - loss: 0.6396 - accuracy: 0.7893Learning Rate 0.00834588\n",
      "Learning Rate 0.00834465\n",
      "Learning Rate 0.008343421\n",
      "Learning Rate 0.008342192\n",
      "Learning Rate 0.008340962\n",
      "Learning Rate 0.008339733\n",
      "Learning Rate 0.008338504\n",
      "Learning Rate 0.008337275\n",
      "Learning Rate 0.008336047\n",
      "14528/25000 [================>.............] - ETA: 1s - loss: 0.6394 - accuracy: 0.7897Learning Rate 0.008334818\n",
      "Learning Rate 0.00833359\n",
      "Learning Rate 0.0083323615\n",
      "Learning Rate 0.008331134\n",
      "Learning Rate 0.0083299065\n",
      "Learning Rate 0.008328679\n",
      "Learning Rate 0.008327452\n",
      "Learning Rate 0.008326224\n",
      "Learning Rate 0.0083249975\n",
      "Learning Rate 0.008323771\n",
      "14848/25000 [================>.............] - ETA: 1s - loss: 0.6433 - accuracy: 0.7889Learning Rate 0.008322544\n",
      "Learning Rate 0.008321318\n",
      "Learning Rate 0.008320091\n",
      "Learning Rate 0.008318866\n",
      "Learning Rate 0.00831764\n",
      "Learning Rate 0.008316414\n",
      "Learning Rate 0.008315189\n",
      "Learning Rate 0.008313963\n",
      "Learning Rate 0.0083127385\n",
      "15136/25000 [=================>............] - ETA: 1s - loss: 0.6422 - accuracy: 0.7892Learning Rate 0.008311514\n",
      "Learning Rate 0.008310289\n",
      "Learning Rate 0.008309064\n",
      "Learning Rate 0.00830784\n",
      "Learning Rate 0.008306616\n",
      "Learning Rate 0.008305392\n",
      "Learning Rate 0.0083041685\n",
      "Learning Rate 0.008302945\n",
      "Learning Rate 0.008301721\n",
      "Learning Rate 0.008300497\n",
      "15456/25000 [=================>............] - ETA: 1s - loss: 0.6405 - accuracy: 0.7902Learning Rate 0.008299274\n",
      "Learning Rate 0.008298052\n",
      "Learning Rate 0.008296829\n",
      "Learning Rate 0.008295606\n",
      "Learning Rate 0.008294383\n",
      "Learning Rate 0.008293161\n",
      "Learning Rate 0.008291939\n",
      "Learning Rate 0.008290717\n",
      "Learning Rate 0.0082894955\n",
      "15744/25000 [=================>............] - ETA: 1s - loss: 0.6394 - accuracy: 0.7908Learning Rate 0.008288274\n",
      "Learning Rate 0.008287053\n",
      "Learning Rate 0.008285832\n",
      "Learning Rate 0.008284611\n",
      "Learning Rate 0.00828339\n",
      "Learning Rate 0.008282169\n",
      "Learning Rate 0.008280949\n",
      "Learning Rate 0.008279729\n",
      "Learning Rate 0.008278509\n",
      "16032/25000 [==================>...........] - ETA: 1s - loss: 0.6402 - accuracy: 0.7908Learning Rate 0.008277289\n",
      "Learning Rate 0.008276069\n",
      "Learning Rate 0.0082748495\n",
      "Learning Rate 0.00827363\n",
      "Learning Rate 0.008272411\n",
      "Learning Rate 0.008271192\n",
      "Learning Rate 0.008269973\n",
      "Learning Rate 0.008268755\n",
      "Learning Rate 0.008267537\n",
      "16320/25000 [==================>...........] - ETA: 1s - loss: 0.6400 - accuracy: 0.7910Learning Rate 0.008266319\n",
      "Learning Rate 0.0082651\n",
      "Learning Rate 0.008263882\n",
      "Learning Rate 0.008262664\n",
      "Learning Rate 0.008261447\n",
      "Learning Rate 0.00826023\n",
      "Learning Rate 0.008259012\n",
      "Learning Rate 0.008257795\n",
      "Learning Rate 0.008256578\n",
      "16608/25000 [==================>...........] - ETA: 1s - loss: 0.6426 - accuracy: 0.7905Learning Rate 0.008255362\n",
      "Learning Rate 0.008254145\n",
      "Learning Rate 0.008252929\n",
      "Learning Rate 0.008251713\n",
      "Learning Rate 0.008250496\n",
      "Learning Rate 0.008249281\n",
      "Learning Rate 0.008248066\n",
      "Learning Rate 0.00824685\n",
      "Learning Rate 0.008245635\n",
      "16896/25000 [===================>..........] - ETA: 1s - loss: 0.6418 - accuracy: 0.7907Learning Rate 0.0082444195\n",
      "Learning Rate 0.008243205\n",
      "Learning Rate 0.008241991\n",
      "Learning Rate 0.008240776\n",
      "Learning Rate 0.008239562\n",
      "Learning Rate 0.008238347\n",
      "Learning Rate 0.008237134\n",
      "Learning Rate 0.00823592\n",
      "Learning Rate 0.008234707\n",
      "17184/25000 [===================>..........] - ETA: 1s - loss: 0.6532 - accuracy: 0.7896Learning Rate 0.008233493\n",
      "Learning Rate 0.00823228\n",
      "Learning Rate 0.008231066\n",
      "Learning Rate 0.008229854\n",
      "Learning Rate 0.008228641\n",
      "Learning Rate 0.008227428\n",
      "Learning Rate 0.008226216\n",
      "Learning Rate 0.008225003\n",
      "Learning Rate 0.008223792\n",
      "17472/25000 [===================>..........] - ETA: 1s - loss: 0.6570 - accuracy: 0.7892Learning Rate 0.00822258\n",
      "Learning Rate 0.008221368\n",
      "Learning Rate 0.008220157\n",
      "Learning Rate 0.008218945\n",
      "Learning Rate 0.008217734\n",
      "Learning Rate 0.008216524\n",
      "Learning Rate 0.008215313\n",
      "Learning Rate 0.008214102\n",
      "Learning Rate 0.008212891\n",
      "17760/25000 [====================>.........] - ETA: 1s - loss: 0.6585 - accuracy: 0.7891Learning Rate 0.008211682\n",
      "Learning Rate 0.008210472\n",
      "Learning Rate 0.008209262\n",
      "Learning Rate 0.008208052\n",
      "Learning Rate 0.0082068425\n",
      "Learning Rate 0.008205634\n",
      "Learning Rate 0.008204425\n",
      "Learning Rate 0.008203216\n",
      "Learning Rate 0.008202007\n",
      "18048/25000 [====================>.........] - ETA: 1s - loss: 0.6608 - accuracy: 0.7881Learning Rate 0.008200798\n",
      "Learning Rate 0.008199589\n",
      "Learning Rate 0.008198381\n",
      "Learning Rate 0.0081971735\n",
      "Learning Rate 0.008195966\n",
      "Learning Rate 0.008194758\n",
      "Learning Rate 0.00819355\n",
      "Learning Rate 0.008192343\n",
      "Learning Rate 0.008191136\n",
      "18336/25000 [=====================>........] - ETA: 1s - loss: 0.6643 - accuracy: 0.7881Learning Rate 0.008189929\n",
      "Learning Rate 0.008188722\n",
      "Learning Rate 0.008187515\n",
      "Learning Rate 0.008186309\n",
      "Learning Rate 0.008185103\n",
      "Learning Rate 0.008183897\n",
      "Learning Rate 0.0081826905\n",
      "Learning Rate 0.008181484\n",
      "18592/25000 [=====================>........] - ETA: 1s - loss: 0.6653 - accuracy: 0.7879Learning Rate 0.008180279\n",
      "Learning Rate 0.008179074\n",
      "Learning Rate 0.008177869\n",
      "Learning Rate 0.008176664\n",
      "Learning Rate 0.008175459\n",
      "Learning Rate 0.008174254\n",
      "Learning Rate 0.008173049\n",
      "Learning Rate 0.008171845\n",
      "18848/25000 [=====================>........] - ETA: 1s - loss: 0.6655 - accuracy: 0.7877Learning Rate 0.008170641\n",
      "Learning Rate 0.008169437\n",
      "Learning Rate 0.008168233\n",
      "Learning Rate 0.008167029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.008165826\n",
      "Learning Rate 0.008164623\n",
      "Learning Rate 0.00816342\n",
      "Learning Rate 0.008162216\n",
      "19104/25000 [=====================>........] - ETA: 1s - loss: 0.6650 - accuracy: 0.7881Learning Rate 0.008161014\n",
      "Learning Rate 0.008159812\n",
      "Learning Rate 0.008158609\n",
      "Learning Rate 0.008157407\n",
      "Learning Rate 0.008156205\n",
      "Learning Rate 0.008155003\n",
      "Learning Rate 0.008153802\n",
      "Learning Rate 0.0081526\n",
      "19360/25000 [======================>.......] - ETA: 1s - loss: 0.6659 - accuracy: 0.7875Learning Rate 0.008151399\n",
      "Learning Rate 0.008150198\n",
      "Learning Rate 0.008148996\n",
      "Learning Rate 0.008147796\n",
      "Learning Rate 0.008146595\n",
      "Learning Rate 0.008145395\n",
      "Learning Rate 0.008144194\n",
      "Learning Rate 0.008142994\n",
      "Learning Rate 0.008141794\n",
      "19648/25000 [======================>.......] - ETA: 0s - loss: 0.6681 - accuracy: 0.7865Learning Rate 0.008140595\n",
      "Learning Rate 0.008139395\n",
      "Learning Rate 0.008138196\n",
      "Learning Rate 0.008136996\n",
      "Learning Rate 0.008135797\n",
      "Learning Rate 0.008134599\n",
      "Learning Rate 0.0081334\n",
      "Learning Rate 0.008132202\n",
      "19904/25000 [======================>.......] - ETA: 0s - loss: 0.6677 - accuracy: 0.7864Learning Rate 0.008131003\n",
      "Learning Rate 0.008129805\n",
      "Learning Rate 0.008128608\n",
      "Learning Rate 0.00812741\n",
      "Learning Rate 0.008126212\n",
      "Learning Rate 0.008125015\n",
      "Learning Rate 0.008123817\n",
      "Learning Rate 0.00812262\n",
      "Learning Rate 0.008121423\n",
      "20192/25000 [=======================>......] - ETA: 0s - loss: 0.6681 - accuracy: 0.7860Learning Rate 0.008120227\n",
      "Learning Rate 0.00811903\n",
      "Learning Rate 0.008117833\n",
      "Learning Rate 0.008116637\n",
      "Learning Rate 0.008115442\n",
      "Learning Rate 0.008114246\n",
      "Learning Rate 0.00811305\n",
      "Learning Rate 0.008111854\n",
      "20448/25000 [=======================>......] - ETA: 0s - loss: 0.6708 - accuracy: 0.7860Learning Rate 0.008110659\n",
      "Learning Rate 0.008109464\n",
      "Learning Rate 0.008108269\n",
      "Learning Rate 0.008107075\n",
      "Learning Rate 0.00810588\n",
      "Learning Rate 0.008104685\n",
      "Learning Rate 0.008103491\n",
      "Learning Rate 0.008102297\n",
      "Learning Rate 0.008101103\n",
      "20736/25000 [=======================>......] - ETA: 0s - loss: 0.6710 - accuracy: 0.7857Learning Rate 0.008099909\n",
      "Learning Rate 0.008098715\n",
      "Learning Rate 0.008097522\n",
      "Learning Rate 0.008096329\n",
      "Learning Rate 0.008095136\n",
      "Learning Rate 0.008093943\n",
      "Learning Rate 0.00809275\n",
      "Learning Rate 0.008091558\n",
      "Learning Rate 0.008090366\n",
      "21024/25000 [========================>.....] - ETA: 0s - loss: 0.6714 - accuracy: 0.7858Learning Rate 0.008089174\n",
      "Learning Rate 0.0080879815\n",
      "Learning Rate 0.008086789\n",
      "Learning Rate 0.008085598\n",
      "Learning Rate 0.008084407\n",
      "Learning Rate 0.008083216\n",
      "Learning Rate 0.008082025\n",
      "Learning Rate 0.008080834\n",
      "Learning Rate 0.008079642\n",
      "21312/25000 [========================>.....] - ETA: 0s - loss: 0.6715 - accuracy: 0.7862Learning Rate 0.008078452\n",
      "Learning Rate 0.008077262\n",
      "Learning Rate 0.008076072\n",
      "Learning Rate 0.0080748815\n",
      "Learning Rate 0.008073691\n",
      "Learning Rate 0.008072502\n",
      "Learning Rate 0.008071313\n",
      "Learning Rate 0.008070123\n",
      "Learning Rate 0.008068934\n",
      "21600/25000 [========================>.....] - ETA: 0s - loss: 0.6742 - accuracy: 0.7855Learning Rate 0.008067745\n",
      "Learning Rate 0.008066556\n",
      "Learning Rate 0.008065368\n",
      "Learning Rate 0.00806418\n",
      "Learning Rate 0.008062991\n",
      "Learning Rate 0.008061803\n",
      "Learning Rate 0.008060615\n",
      "Learning Rate 0.008059427\n",
      "21856/25000 [=========================>....] - ETA: 0s - loss: 0.6764 - accuracy: 0.7855Learning Rate 0.00805824\n",
      "Learning Rate 0.008057052\n",
      "Learning Rate 0.008055865\n",
      "Learning Rate 0.008054677\n",
      "Learning Rate 0.008053491\n",
      "Learning Rate 0.008052304\n",
      "Learning Rate 0.008051118\n",
      "Learning Rate 0.008049931\n",
      "Learning Rate 0.008048745\n",
      "22144/25000 [=========================>....] - ETA: 0s - loss: 0.6769 - accuracy: 0.7853Learning Rate 0.008047559\n",
      "Learning Rate 0.008046374\n",
      "Learning Rate 0.008045188\n",
      "Learning Rate 0.008044003\n",
      "Learning Rate 0.008042817\n",
      "Learning Rate 0.008041631\n",
      "Learning Rate 0.008040447\n",
      "Learning Rate 0.008039262\n",
      "Learning Rate 0.0080380775\n",
      "22432/25000 [=========================>....] - ETA: 0s - loss: 0.6754 - accuracy: 0.7856Learning Rate 0.008036893\n",
      "Learning Rate 0.008035708\n",
      "Learning Rate 0.0080345245\n",
      "Learning Rate 0.008033341\n",
      "Learning Rate 0.008032157\n",
      "Learning Rate 0.008030973\n",
      "Learning Rate 0.00802979\n",
      "Learning Rate 0.008028607\n",
      "Learning Rate 0.008027424\n",
      "22720/25000 [==========================>...] - ETA: 0s - loss: 0.6747 - accuracy: 0.7857Learning Rate 0.008026241\n",
      "Learning Rate 0.008025059\n",
      "Learning Rate 0.008023876\n",
      "Learning Rate 0.008022693\n",
      "Learning Rate 0.008021511\n",
      "Learning Rate 0.008020329\n",
      "Learning Rate 0.008019147\n",
      "Learning Rate 0.008017966\n",
      "Learning Rate 0.008016784\n",
      "Learning Rate 0.008015603\n",
      "23040/25000 [==========================>...] - ETA: 0s - loss: 0.6759 - accuracy: 0.7853Learning Rate 0.008014422\n",
      "Learning Rate 0.008013241\n",
      "Learning Rate 0.00801206\n",
      "Learning Rate 0.008010879\n",
      "Learning Rate 0.008009699\n",
      "Learning Rate 0.008008519\n",
      "Learning Rate 0.008007339\n",
      "Learning Rate 0.008006159\n",
      "Learning Rate 0.008004979\n",
      "23328/25000 [==========================>...] - ETA: 0s - loss: 0.6743 - accuracy: 0.7854Learning Rate 0.008003799\n",
      "Learning Rate 0.00800262\n",
      "Learning Rate 0.008001441\n",
      "Learning Rate 0.008000262\n",
      "Learning Rate 0.007999083\n",
      "Learning Rate 0.007997904\n",
      "Learning Rate 0.007996726\n",
      "Learning Rate 0.007995548\n",
      "Learning Rate 0.00799437\n",
      "23616/25000 [===========================>..] - ETA: 0s - loss: 0.6732 - accuracy: 0.7854Learning Rate 0.0079931915\n",
      "Learning Rate 0.007992013\n",
      "Learning Rate 0.007990835\n",
      "Learning Rate 0.007989658\n",
      "Learning Rate 0.007988481\n",
      "Learning Rate 0.007987304\n",
      "Learning Rate 0.0079861265\n",
      "Learning Rate 0.007984949\n",
      "Learning Rate 0.007983773\n",
      "23904/25000 [===========================>..] - ETA: 0s - loss: 0.6711 - accuracy: 0.7860Learning Rate 0.007982597\n",
      "Learning Rate 0.0079814205\n",
      "Learning Rate 0.007980244\n",
      "Learning Rate 0.007979068\n",
      "Learning Rate 0.007977893\n",
      "Learning Rate 0.007976717\n",
      "Learning Rate 0.007975542\n",
      "Learning Rate 0.007974367\n",
      "Learning Rate 0.007973191\n",
      "Learning Rate 0.007972016\n",
      "24224/25000 [============================>.] - ETA: 0s - loss: 0.6682 - accuracy: 0.7867Learning Rate 0.007970842\n",
      "Learning Rate 0.007969667\n",
      "Learning Rate 0.007968493\n",
      "Learning Rate 0.007967318\n",
      "Learning Rate 0.007966144\n",
      "Learning Rate 0.007964971\n",
      "Learning Rate 0.007963797\n",
      "Learning Rate 0.007962624\n",
      "Learning Rate 0.00796145\n",
      "24512/25000 [============================>.] - ETA: 0s - loss: 0.6676 - accuracy: 0.7869Learning Rate 0.007960277\n",
      "Learning Rate 0.007959104\n",
      "Learning Rate 0.007957932\n",
      "Learning Rate 0.007956759\n",
      "Learning Rate 0.007955587\n",
      "Learning Rate 0.007954414\n",
      "Learning Rate 0.0079532415\n",
      "Learning Rate 0.00795207\n",
      "Learning Rate 0.007950898\n",
      "24800/25000 [============================>.] - ETA: 0s - loss: 0.6678 - accuracy: 0.7867Learning Rate 0.007949727\n",
      "Learning Rate 0.007948555\n",
      "Learning Rate 0.007947383\n",
      "Learning Rate 0.007946213\n",
      "Learning Rate 0.007945042\n",
      "Learning Rate 0.007943871\n",
      "Learning Rate 0.007942701\n",
      "25000/25000 [==============================] - 5s 199us/sample - loss: 0.6678 - accuracy: 0.7868 - val_loss: 1.1803 - val_accuracy: 0.6976\n",
      "Epoch 3/5\n",
      "Learning Rate 0.00794153\n",
      "   32/25000 [..............................] - ETA: 4s - loss: 1.2669 - accuracy: 0.7812Learning Rate 0.007940359\n",
      "Learning Rate 0.00793919\n",
      "Learning Rate 0.00793802\n",
      "Learning Rate 0.00793685\n",
      "Learning Rate 0.00793568\n",
      "Learning Rate 0.007934511\n",
      "Learning Rate 0.007933342\n",
      "Learning Rate 0.007932173\n",
      "Learning Rate 0.007931004\n",
      "  320/25000 [..............................] - ETA: 4s - loss: 0.6591 - accuracy: 0.7969Learning Rate 0.0079298355\n",
      "Learning Rate 0.007928667\n",
      "Learning Rate 0.007927499\n",
      "Learning Rate 0.007926331\n",
      "Learning Rate 0.007925163\n",
      "Learning Rate 0.007923995\n",
      "Learning Rate 0.007922827\n",
      "Learning Rate 0.007921659\n",
      "Learning Rate 0.007920492\n",
      "  608/25000 [..............................] - ETA: 4s - loss: 0.6374 - accuracy: 0.8026Learning Rate 0.0079193255\n",
      "Learning Rate 0.007918159\n",
      "Learning Rate 0.007916992\n",
      "Learning Rate 0.007915825\n",
      "Learning Rate 0.007914659\n",
      "Learning Rate 0.007913493\n",
      "Learning Rate 0.007912327\n",
      "Learning Rate 0.007911161\n",
      "Learning Rate 0.007909995\n",
      "  896/25000 [>.............................] - ETA: 4s - loss: 0.6743 - accuracy: 0.7958Learning Rate 0.007908829\n",
      "Learning Rate 0.0079076635\n",
      "Learning Rate 0.007906498\n",
      "Learning Rate 0.007905333\n",
      "Learning Rate 0.007904168\n",
      "Learning Rate 0.007903003\n",
      "Learning Rate 0.007901839\n",
      "Learning Rate 0.007900675\n",
      "Learning Rate 0.007899511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1184/25000 [>.............................] - ETA: 4s - loss: 0.6449 - accuracy: 0.7981Learning Rate 0.0078983465\n",
      "Learning Rate 0.007897182\n",
      "Learning Rate 0.007896019\n",
      "Learning Rate 0.007894856\n",
      "Learning Rate 0.007893693\n",
      "Learning Rate 0.0078925295\n",
      "Learning Rate 0.007891366\n",
      "Learning Rate 0.007890203\n",
      "Learning Rate 0.007889041\n",
      " 1472/25000 [>.............................] - ETA: 4s - loss: 0.6320 - accuracy: 0.8043Learning Rate 0.007887878\n",
      "Learning Rate 0.007886716\n",
      "Learning Rate 0.007885554\n",
      "Learning Rate 0.007884392\n",
      "Learning Rate 0.00788323\n",
      "Learning Rate 0.007882069\n",
      "Learning Rate 0.0078809075\n",
      "Learning Rate 0.007879746\n",
      "Learning Rate 0.007878585\n",
      "Learning Rate 0.007877423\n",
      " 1792/25000 [=>............................] - ETA: 4s - loss: 0.6032 - accuracy: 0.8103Learning Rate 0.007876263\n",
      "Learning Rate 0.007875103\n",
      "Learning Rate 0.007873942\n",
      "Learning Rate 0.007872782\n",
      "Learning Rate 0.007871621\n",
      "Learning Rate 0.007870462\n",
      "Learning Rate 0.007869302\n",
      "Learning Rate 0.007868143\n",
      "Learning Rate 0.007866983\n",
      " 2080/25000 [=>............................] - ETA: 4s - loss: 0.5933 - accuracy: 0.8087Learning Rate 0.007865824\n",
      "Learning Rate 0.007864664\n",
      "Learning Rate 0.007863506\n",
      "Learning Rate 0.007862347\n",
      "Learning Rate 0.007861189\n",
      "Learning Rate 0.00786003\n",
      "Learning Rate 0.0078588715\n",
      "Learning Rate 0.007857714\n",
      "Learning Rate 0.007856556\n",
      " 2368/25000 [=>............................] - ETA: 4s - loss: 0.5882 - accuracy: 0.8087Learning Rate 0.007855399\n",
      "Learning Rate 0.007854241\n",
      "Learning Rate 0.007853083\n",
      "Learning Rate 0.007851926\n",
      "Learning Rate 0.007850769\n",
      "Learning Rate 0.007849612\n",
      "Learning Rate 0.007848456\n",
      "Learning Rate 0.007847299\n",
      "Learning Rate 0.007846142\n",
      " 2656/25000 [==>...........................] - ETA: 4s - loss: 0.5783 - accuracy: 0.8095Learning Rate 0.007844986\n",
      "Learning Rate 0.007843831\n",
      "Learning Rate 0.007842675\n",
      "Learning Rate 0.007841519\n",
      "Learning Rate 0.007840363\n",
      "Learning Rate 0.007839208\n",
      "Learning Rate 0.007838053\n",
      "Learning Rate 0.007836898\n",
      "Learning Rate 0.007835743\n",
      " 2944/25000 [==>...........................] - ETA: 3s - loss: 0.5806 - accuracy: 0.8071Learning Rate 0.007834588\n",
      "Learning Rate 0.007833433\n",
      "Learning Rate 0.007832279\n",
      "Learning Rate 0.0078311255\n",
      "Learning Rate 0.007829972\n",
      "Learning Rate 0.007828818\n",
      "Learning Rate 0.007827664\n",
      "Learning Rate 0.007826511\n",
      " 3200/25000 [==>...........................] - ETA: 3s - loss: 0.5903 - accuracy: 0.8056Learning Rate 0.007825358\n",
      "Learning Rate 0.007824205\n",
      "Learning Rate 0.007823052\n",
      "Learning Rate 0.007821899\n",
      "Learning Rate 0.007820746\n",
      "Learning Rate 0.007819594\n",
      "Learning Rate 0.007818442\n",
      "Learning Rate 0.00781729\n",
      "Learning Rate 0.007816138\n",
      " 3488/25000 [===>..........................] - ETA: 3s - loss: 0.5997 - accuracy: 0.8036Learning Rate 0.007814986\n",
      "Learning Rate 0.007813835\n",
      "Learning Rate 0.0078126835\n",
      "Learning Rate 0.0078115324\n",
      "Learning Rate 0.0078103812\n",
      "Learning Rate 0.00780923\n",
      "Learning Rate 0.0078080795\n",
      "Learning Rate 0.007806929\n",
      "Learning Rate 0.0078057786\n",
      " 3776/25000 [===>..........................] - ETA: 3s - loss: 0.5891 - accuracy: 0.8067Learning Rate 0.0078046285\n",
      "Learning Rate 0.0078034783\n",
      "Learning Rate 0.0078023286\n",
      "Learning Rate 0.007801179\n",
      "Learning Rate 0.007800029\n",
      "Learning Rate 0.00779888\n",
      "Learning Rate 0.0077977306\n",
      "Learning Rate 0.0077965814\n",
      "Learning Rate 0.0077954326\n",
      " 4064/25000 [===>..........................] - ETA: 3s - loss: 0.5997 - accuracy: 0.8044Learning Rate 0.007794284\n",
      "Learning Rate 0.0077931355\n",
      "Learning Rate 0.007791987\n",
      "Learning Rate 0.007790839\n",
      "Learning Rate 0.007789691\n",
      "Learning Rate 0.007788543\n",
      "Learning Rate 0.0077873953\n",
      "Learning Rate 0.007786248\n",
      " 4320/25000 [====>.........................] - ETA: 3s - loss: 0.6030 - accuracy: 0.8035Learning Rate 0.0077851005\n",
      "Learning Rate 0.007783953\n",
      "Learning Rate 0.007782806\n",
      "Learning Rate 0.0077816593\n",
      "Learning Rate 0.007780513\n",
      "Learning Rate 0.0077793663\n",
      "Learning Rate 0.00777822\n",
      "Learning Rate 0.007777074\n",
      "Learning Rate 0.007775928\n",
      " 4608/25000 [====>.........................] - ETA: 3s - loss: 0.6065 - accuracy: 0.8030Learning Rate 0.007774782\n",
      "Learning Rate 0.0077736364\n",
      "Learning Rate 0.007772491\n",
      "Learning Rate 0.0077713453\n",
      "Learning Rate 0.0077702003\n",
      "Learning Rate 0.007769055\n",
      "Learning Rate 0.0077679106\n",
      "Learning Rate 0.007766766\n",
      "Learning Rate 0.0077656214\n",
      "Learning Rate 0.0077644773\n",
      " 4928/25000 [====>.........................] - ETA: 3s - loss: 0.6078 - accuracy: 0.8026Learning Rate 0.007763333\n",
      "Learning Rate 0.007762189\n",
      "Learning Rate 0.0077610454\n",
      "Learning Rate 0.0077599017\n",
      "Learning Rate 0.007758758\n",
      "Learning Rate 0.007757615\n",
      "Learning Rate 0.0077564716\n",
      "Learning Rate 0.007755329\n",
      "Learning Rate 0.007754186\n",
      " 5216/25000 [=====>........................] - ETA: 3s - loss: 0.6034 - accuracy: 0.8029Learning Rate 0.0077530434\n",
      "Learning Rate 0.007751901\n",
      "Learning Rate 0.007750759\n",
      "Learning Rate 0.0077496166\n",
      "Learning Rate 0.007748475\n",
      "Learning Rate 0.007747333\n",
      "Learning Rate 0.007746191\n",
      "Learning Rate 0.00774505\n",
      "Learning Rate 0.0077439086\n",
      "Learning Rate 0.0077427677\n",
      " 5536/25000 [=====>........................] - ETA: 3s - loss: 0.6064 - accuracy: 0.8040Learning Rate 0.007741627\n",
      "Learning Rate 0.007740486\n",
      "Learning Rate 0.0077393455\n",
      "Learning Rate 0.007738205\n",
      "Learning Rate 0.0077370647\n",
      "Learning Rate 0.007735925\n",
      "Learning Rate 0.007734785\n",
      "Learning Rate 0.007733645\n",
      "Learning Rate 0.0077325054\n",
      " 5824/25000 [=====>........................] - ETA: 3s - loss: 0.6034 - accuracy: 0.8051Learning Rate 0.007731366\n",
      "Learning Rate 0.0077302265\n",
      "Learning Rate 0.0077290875\n",
      "Learning Rate 0.0077279485\n",
      "Learning Rate 0.00772681\n",
      "Learning Rate 0.0077256714\n",
      "Learning Rate 0.007724533\n",
      "Learning Rate 0.007723395\n",
      "Learning Rate 0.0077222567\n",
      "Learning Rate 0.0077211186\n",
      " 6144/25000 [======>.......................] - ETA: 3s - loss: 0.5971 - accuracy: 0.8078Learning Rate 0.007719981\n",
      "Learning Rate 0.0077188434\n",
      "Learning Rate 0.007717706\n",
      "Learning Rate 0.0077165687\n",
      "Learning Rate 0.0077154315\n",
      "Learning Rate 0.007714295\n",
      "Learning Rate 0.007713158\n",
      "Learning Rate 0.0077120215\n",
      "Learning Rate 0.0077108853\n",
      " 6432/25000 [======>.......................] - ETA: 3s - loss: 0.5958 - accuracy: 0.8069Learning Rate 0.007709749\n",
      "Learning Rate 0.007708613\n",
      "Learning Rate 0.007707477\n",
      "Learning Rate 0.0077063413\n",
      "Learning Rate 0.0077052056\n",
      "Learning Rate 0.0077040703\n",
      "Learning Rate 0.007702935\n",
      "Learning Rate 0.0077017997\n",
      "Learning Rate 0.007700665\n",
      " 6720/25000 [=======>......................] - ETA: 3s - loss: 0.5922 - accuracy: 0.8073Learning Rate 0.00769953\n",
      "Learning Rate 0.0076983958\n",
      "Learning Rate 0.0076972614\n",
      "Learning Rate 0.007696127\n",
      "Learning Rate 0.007694993\n",
      "Learning Rate 0.0076938593\n",
      "Learning Rate 0.0076927254\n",
      "Learning Rate 0.007691592\n",
      "Learning Rate 0.0076904586\n",
      "Learning Rate 0.007689325\n",
      " 7040/25000 [=======>......................] - ETA: 3s - loss: 0.5982 - accuracy: 0.8058Learning Rate 0.007688192\n",
      "Learning Rate 0.007687059\n",
      "Learning Rate 0.0076859263\n",
      "Learning Rate 0.007684794\n",
      "Learning Rate 0.0076836613\n",
      "Learning Rate 0.0076825293\n",
      "Learning Rate 0.0076813973\n",
      "Learning Rate 0.0076802652\n",
      "Learning Rate 0.0076791337\n",
      "Learning Rate 0.007678002\n",
      " 7360/25000 [=======>......................] - ETA: 3s - loss: 0.5940 - accuracy: 0.8069Learning Rate 0.0076768706\n",
      "Learning Rate 0.0076757395\n",
      "Learning Rate 0.0076746084\n",
      "Learning Rate 0.0076734773\n",
      "Learning Rate 0.0076723467\n",
      "Learning Rate 0.007671216\n",
      "Learning Rate 0.007670086\n",
      "Learning Rate 0.0076689557\n",
      "Learning Rate 0.0076678256\n",
      " 7648/25000 [========>.....................] - ETA: 3s - loss: 0.5893 - accuracy: 0.8075Learning Rate 0.007666696\n",
      "Learning Rate 0.007665566\n",
      "Learning Rate 0.0076644365\n",
      "Learning Rate 0.0076633072\n",
      "Learning Rate 0.007662178\n",
      "Learning Rate 0.007661049\n",
      "Learning Rate 0.00765992\n",
      "Learning Rate 0.0076587913\n",
      "Learning Rate 0.0076576625\n",
      " 7936/25000 [========>.....................] - ETA: 3s - loss: 0.5843 - accuracy: 0.8095Learning Rate 0.007656534\n",
      "Learning Rate 0.007655406\n",
      "Learning Rate 0.007654278\n",
      "Learning Rate 0.0076531502\n",
      "Learning Rate 0.0076520224\n",
      "Learning Rate 0.007650895\n",
      "Learning Rate 0.0076497677\n",
      "Learning Rate 0.0076486403\n",
      "Learning Rate 0.0076475134\n",
      " 8224/25000 [========>.....................] - ETA: 3s - loss: 0.5867 - accuracy: 0.8084Learning Rate 0.0076463865\n",
      "Learning Rate 0.0076452596\n",
      "Learning Rate 0.007644133\n",
      "Learning Rate 0.0076430067\n",
      "Learning Rate 0.0076418803\n",
      "Learning Rate 0.0076407543\n",
      "Learning Rate 0.0076396284\n",
      "Learning Rate 0.007638503\n",
      "Learning Rate 0.0076373774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.007636252\n",
      " 8544/25000 [=========>....................] - ETA: 2s - loss: 0.5857 - accuracy: 0.8088Learning Rate 0.007635127\n",
      "Learning Rate 0.007634002\n",
      "Learning Rate 0.0076328767\n",
      "Learning Rate 0.007631752\n",
      "Learning Rate 0.0076306276\n",
      "Learning Rate 0.007629503\n",
      "Learning Rate 0.007628379\n",
      "Learning Rate 0.007627255\n",
      "Learning Rate 0.0076261307\n",
      " 8832/25000 [=========>....................] - ETA: 2s - loss: 0.5870 - accuracy: 0.8077Learning Rate 0.007625007\n",
      "Learning Rate 0.0076238834\n",
      "Learning Rate 0.00762276\n",
      "Learning Rate 0.0076216366\n",
      "Learning Rate 0.0076205134\n",
      "Learning Rate 0.0076193907\n",
      "Learning Rate 0.007618268\n",
      "Learning Rate 0.0076171453\n",
      "Learning Rate 0.007616023\n",
      " 9120/25000 [=========>....................] - ETA: 2s - loss: 0.5838 - accuracy: 0.8086Learning Rate 0.007614901\n",
      "Learning Rate 0.0076137786\n",
      "Learning Rate 0.007612657\n",
      "Learning Rate 0.007611535\n",
      "Learning Rate 0.0076104132\n",
      "Learning Rate 0.007609292\n",
      "Learning Rate 0.0076081706\n",
      "Learning Rate 0.0076070493\n",
      "Learning Rate 0.0076059285\n",
      " 9408/25000 [==========>...................] - ETA: 2s - loss: 0.5868 - accuracy: 0.8082Learning Rate 0.0076048076\n",
      "Learning Rate 0.0076036872\n",
      "Learning Rate 0.007602567\n",
      "Learning Rate 0.0076014465\n",
      "Learning Rate 0.0076003266\n",
      "Learning Rate 0.0075992066\n",
      "Learning Rate 0.0075980867\n",
      "Learning Rate 0.0075969673\n",
      "Learning Rate 0.007595848\n",
      " 9696/25000 [==========>...................] - ETA: 2s - loss: 0.5837 - accuracy: 0.8091Learning Rate 0.0075947284\n",
      "Learning Rate 0.0075936094\n",
      "Learning Rate 0.0075924904\n",
      "Learning Rate 0.0075913714\n",
      "Learning Rate 0.007590253\n",
      "Learning Rate 0.0075891344\n",
      "Learning Rate 0.007588016\n",
      "Learning Rate 0.007586898\n",
      "Learning Rate 0.0075857798\n",
      " 9984/25000 [==========>...................] - ETA: 2s - loss: 0.5858 - accuracy: 0.8085Learning Rate 0.007584662\n",
      "Learning Rate 0.0075835446\n",
      "Learning Rate 0.007582427\n",
      "Learning Rate 0.00758131\n",
      "Learning Rate 0.0075801928\n",
      "Learning Rate 0.0075790756\n",
      "Learning Rate 0.007577959\n",
      "Learning Rate 0.0075768423\n",
      "Learning Rate 0.0075757257\n",
      "10272/25000 [===========>..................] - ETA: 2s - loss: 0.5936 - accuracy: 0.8073Learning Rate 0.0075746095\n",
      "Learning Rate 0.0075734933\n",
      "Learning Rate 0.007572377\n",
      "Learning Rate 0.0075712614\n",
      "Learning Rate 0.0075701457\n",
      "Learning Rate 0.00756903\n",
      "Learning Rate 0.0075679147\n",
      "Learning Rate 0.0075667994\n",
      "Learning Rate 0.0075656846\n",
      "Learning Rate 0.00756457\n",
      "10592/25000 [===========>..................] - ETA: 2s - loss: 0.5941 - accuracy: 0.8072Learning Rate 0.007563455\n",
      "Learning Rate 0.0075623407\n",
      "Learning Rate 0.0075612264\n",
      "Learning Rate 0.007560112\n",
      "Learning Rate 0.007558998\n",
      "Learning Rate 0.0075578843\n",
      "Learning Rate 0.0075567705\n",
      "Learning Rate 0.007555657\n",
      "Learning Rate 0.0075545437\n",
      "10880/25000 [============>.................] - ETA: 2s - loss: 0.5949 - accuracy: 0.8064Learning Rate 0.0075534303\n",
      "Learning Rate 0.0075523173\n",
      "Learning Rate 0.0075512044\n",
      "Learning Rate 0.0075500915\n",
      "Learning Rate 0.007548979\n",
      "Learning Rate 0.0075478666\n",
      "Learning Rate 0.0075467546\n",
      "Learning Rate 0.0075456426\n",
      "Learning Rate 0.0075445306\n",
      "11168/25000 [============>.................] - ETA: 2s - loss: 0.5973 - accuracy: 0.8057Learning Rate 0.007543419\n",
      "Learning Rate 0.0075423075\n",
      "Learning Rate 0.007541196\n",
      "Learning Rate 0.007540085\n",
      "Learning Rate 0.007538974\n",
      "Learning Rate 0.0075378628\n",
      "Learning Rate 0.007536752\n",
      "Learning Rate 0.0075356415\n",
      "Learning Rate 0.007534531\n",
      "11456/25000 [============>.................] - ETA: 2s - loss: 0.6065 - accuracy: 0.8053Learning Rate 0.007533421\n",
      "Learning Rate 0.0075323107\n",
      "Learning Rate 0.0075312005\n",
      "Learning Rate 0.007530091\n",
      "Learning Rate 0.007528981\n",
      "Learning Rate 0.007527872\n",
      "Learning Rate 0.007526763\n",
      "Learning Rate 0.0075256536\n",
      "Learning Rate 0.007524545\n",
      "11744/25000 [=============>................] - ETA: 2s - loss: 0.6070 - accuracy: 0.8059Learning Rate 0.007523436\n",
      "Learning Rate 0.0075223274\n",
      "Learning Rate 0.007521219\n",
      "Learning Rate 0.007520111\n",
      "Learning Rate 0.0075190025\n",
      "Learning Rate 0.0075178947\n",
      "Learning Rate 0.007516787\n",
      "Learning Rate 0.007515679\n",
      "Learning Rate 0.007514572\n",
      "12032/25000 [=============>................] - ETA: 2s - loss: 0.6080 - accuracy: 0.8058Learning Rate 0.0075134644\n",
      "Learning Rate 0.007512357\n",
      "Learning Rate 0.00751125\n",
      "Learning Rate 0.0075101433\n",
      "Learning Rate 0.0075090365\n",
      "Learning Rate 0.00750793\n",
      "Learning Rate 0.0075068236\n",
      "Learning Rate 0.0075057177\n",
      "12288/25000 [=============>................] - ETA: 2s - loss: 0.6093 - accuracy: 0.8063Learning Rate 0.0075046117\n",
      "Learning Rate 0.007503506\n",
      "Learning Rate 0.0075024003\n",
      "Learning Rate 0.007501295\n",
      "Learning Rate 0.0075001894\n",
      "Learning Rate 0.0074990843\n",
      "Learning Rate 0.0074979793\n",
      "Learning Rate 0.0074968743\n",
      "Learning Rate 0.0074957698\n",
      "12576/25000 [==============>...............] - ETA: 2s - loss: 0.6066 - accuracy: 0.8068Learning Rate 0.007494665\n",
      "Learning Rate 0.0074935607\n",
      "Learning Rate 0.0074924566\n",
      "Learning Rate 0.0074913525\n",
      "Learning Rate 0.0074902484\n",
      "Learning Rate 0.007489145\n",
      "Learning Rate 0.007488041\n",
      "Learning Rate 0.0074869376\n",
      "Learning Rate 0.0074858344\n",
      "Learning Rate 0.0074847313\n",
      "12896/25000 [==============>...............] - ETA: 2s - loss: 0.6065 - accuracy: 0.8068Learning Rate 0.0074836286\n",
      "Learning Rate 0.007482526\n",
      "Learning Rate 0.007481423\n",
      "Learning Rate 0.007480321\n",
      "Learning Rate 0.0074792188\n",
      "Learning Rate 0.0074781165\n",
      "Learning Rate 0.007477015\n",
      "Learning Rate 0.007475913\n",
      "Learning Rate 0.0074748113\n",
      "Learning Rate 0.00747371\n",
      "13216/25000 [==============>...............] - ETA: 2s - loss: 0.6096 - accuracy: 0.8062Learning Rate 0.0074726087\n",
      "Learning Rate 0.0074715074\n",
      "Learning Rate 0.0074704066\n",
      "Learning Rate 0.007469306\n",
      "Learning Rate 0.007468205\n",
      "Learning Rate 0.0074671046\n",
      "Learning Rate 0.007466004\n",
      "Learning Rate 0.007464904\n",
      "Learning Rate 0.007463804\n",
      "13504/25000 [===============>..............] - ETA: 2s - loss: 0.6108 - accuracy: 0.8055Learning Rate 0.007462704\n",
      "Learning Rate 0.007461604\n",
      "Learning Rate 0.007460505\n",
      "Learning Rate 0.0074594053\n",
      "Learning Rate 0.0074583064\n",
      "Learning Rate 0.0074572074\n",
      "Learning Rate 0.0074561085\n",
      "Learning Rate 0.00745501\n",
      "Learning Rate 0.0074539115\n",
      "13792/25000 [===============>..............] - ETA: 2s - loss: 0.6114 - accuracy: 0.8054Learning Rate 0.007452813\n",
      "Learning Rate 0.007451715\n",
      "Learning Rate 0.007450617\n",
      "Learning Rate 0.007449519\n",
      "Learning Rate 0.0074484213\n",
      "Learning Rate 0.0074473238\n",
      "Learning Rate 0.007446226\n",
      "Learning Rate 0.007445129\n",
      "Learning Rate 0.007444032\n",
      "14080/25000 [===============>..............] - ETA: 1s - loss: 0.6102 - accuracy: 0.8055Learning Rate 0.007442935\n",
      "Learning Rate 0.0074418383\n",
      "Learning Rate 0.0074407416\n",
      "Learning Rate 0.007439645\n",
      "Learning Rate 0.007438549\n",
      "Learning Rate 0.0074374527\n",
      "Learning Rate 0.0074363565\n",
      "Learning Rate 0.007435261\n",
      "Learning Rate 0.007434165\n",
      "Learning Rate 0.00743307\n",
      "14400/25000 [================>.............] - ETA: 1s - loss: 0.6056 - accuracy: 0.8065Learning Rate 0.0074319746\n",
      "Learning Rate 0.0074308794\n",
      "Learning Rate 0.0074297846\n",
      "Learning Rate 0.00742869\n",
      "Learning Rate 0.007427595\n",
      "Learning Rate 0.007426501\n",
      "Learning Rate 0.0074254065\n",
      "Learning Rate 0.007424312\n",
      "Learning Rate 0.0074232183\n",
      "14688/25000 [================>.............] - ETA: 1s - loss: 0.6057 - accuracy: 0.8067Learning Rate 0.0074221245\n",
      "Learning Rate 0.0074210307\n",
      "Learning Rate 0.0074199373\n",
      "Learning Rate 0.007418844\n",
      "Learning Rate 0.0074177505\n",
      "Learning Rate 0.0074166576\n",
      "Learning Rate 0.0074155647\n",
      "Learning Rate 0.007414472\n",
      "Learning Rate 0.0074133794\n",
      "14976/25000 [================>.............] - ETA: 1s - loss: 0.6078 - accuracy: 0.8055Learning Rate 0.007412287\n",
      "Learning Rate 0.0074111945\n",
      "Learning Rate 0.0074101025\n",
      "Learning Rate 0.0074090105\n",
      "Learning Rate 0.0074079186\n",
      "Learning Rate 0.007406827\n",
      "Learning Rate 0.0074057356\n",
      "Learning Rate 0.0074046445\n",
      "Learning Rate 0.0074035535\n",
      "15264/25000 [=================>............] - ETA: 1s - loss: 0.6115 - accuracy: 0.8045Learning Rate 0.0074024624\n",
      "Learning Rate 0.007401372\n",
      "Learning Rate 0.0074002813\n",
      "Learning Rate 0.0073991907\n",
      "Learning Rate 0.0073981006\n",
      "Learning Rate 0.0073970105\n",
      "Learning Rate 0.0073959203\n",
      "Learning Rate 0.0073948307\n",
      "Learning Rate 0.007393741\n",
      "15552/25000 [=================>............] - ETA: 1s - loss: 0.6114 - accuracy: 0.8037Learning Rate 0.0073926514\n",
      "Learning Rate 0.007391562\n",
      "Learning Rate 0.007390473\n",
      "Learning Rate 0.007389384\n",
      "Learning Rate 0.007388295\n",
      "Learning Rate 0.0073872064\n",
      "Learning Rate 0.0073861177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0073850295\n",
      "Learning Rate 0.007383941\n",
      "15840/25000 [==================>...........] - ETA: 1s - loss: 0.6100 - accuracy: 0.8037Learning Rate 0.007382853\n",
      "Learning Rate 0.007381765\n",
      "Learning Rate 0.0073806774\n",
      "Learning Rate 0.0073795896\n",
      "Learning Rate 0.0073785023\n",
      "Learning Rate 0.007377415\n",
      "Learning Rate 0.0073763276\n",
      "Learning Rate 0.007375241\n",
      "Learning Rate 0.007374154\n",
      "Learning Rate 0.0073730676\n",
      "16160/25000 [==================>...........] - ETA: 1s - loss: 0.6096 - accuracy: 0.8037Learning Rate 0.007371981\n",
      "Learning Rate 0.007370895\n",
      "Learning Rate 0.007369809\n",
      "Learning Rate 0.007368723\n",
      "Learning Rate 0.007367637\n",
      "Learning Rate 0.0073665516\n",
      "Learning Rate 0.007365466\n",
      "Learning Rate 0.0073643806\n",
      "Learning Rate 0.0073632956\n",
      "16448/25000 [==================>...........] - ETA: 1s - loss: 0.6076 - accuracy: 0.8045Learning Rate 0.0073622107\n",
      "Learning Rate 0.0073611257\n",
      "Learning Rate 0.007360041\n",
      "Learning Rate 0.0073589566\n",
      "Learning Rate 0.007357872\n",
      "Learning Rate 0.007356788\n",
      "Learning Rate 0.007355704\n",
      "Learning Rate 0.00735462\n",
      "Learning Rate 0.0073535363\n",
      "16736/25000 [===================>..........] - ETA: 1s - loss: 0.6059 - accuracy: 0.8046Learning Rate 0.0073524527\n",
      "Learning Rate 0.007351369\n",
      "Learning Rate 0.007350286\n",
      "Learning Rate 0.007349203\n",
      "Learning Rate 0.0073481197\n",
      "Learning Rate 0.007347037\n",
      "Learning Rate 0.0073459544\n",
      "Learning Rate 0.0073448718\n",
      "Learning Rate 0.0073437896\n",
      "Learning Rate 0.0073427074\n",
      "17056/25000 [===================>..........] - ETA: 1s - loss: 0.6045 - accuracy: 0.8048Learning Rate 0.007341625\n",
      "Learning Rate 0.0073405434\n",
      "Learning Rate 0.0073394617\n",
      "Learning Rate 0.00733838\n",
      "Learning Rate 0.0073372987\n",
      "Learning Rate 0.0073362174\n",
      "Learning Rate 0.0073351366\n",
      "Learning Rate 0.007334056\n",
      "Learning Rate 0.007332975\n",
      "17344/25000 [===================>..........] - ETA: 1s - loss: 0.6041 - accuracy: 0.8051Learning Rate 0.0073318947\n",
      "Learning Rate 0.0073308144\n",
      "Learning Rate 0.007329734\n",
      "Learning Rate 0.007328654\n",
      "Learning Rate 0.0073275743\n",
      "Learning Rate 0.0073264944\n",
      "Learning Rate 0.007325415\n",
      "Learning Rate 0.0073243356\n",
      "Learning Rate 0.007323256\n",
      "17632/25000 [====================>.........] - ETA: 1s - loss: 0.6030 - accuracy: 0.8056Learning Rate 0.0073221773\n",
      "Learning Rate 0.0073210984\n",
      "Learning Rate 0.0073200194\n",
      "Learning Rate 0.007318941\n",
      "Learning Rate 0.0073178625\n",
      "Learning Rate 0.007316784\n",
      "Learning Rate 0.007315706\n",
      "Learning Rate 0.007314628\n",
      "Learning Rate 0.00731355\n",
      "17920/25000 [====================>.........] - ETA: 1s - loss: 0.6007 - accuracy: 0.8062Learning Rate 0.0073124724\n",
      "Learning Rate 0.007311395\n",
      "Learning Rate 0.0073103174\n",
      "Learning Rate 0.0073092403\n",
      "Learning Rate 0.007308163\n",
      "Learning Rate 0.007307086\n",
      "Learning Rate 0.0073060095\n",
      "Learning Rate 0.007304933\n",
      "Learning Rate 0.0073038563\n",
      "18208/25000 [====================>.........] - ETA: 1s - loss: 0.6036 - accuracy: 0.8056Learning Rate 0.00730278\n",
      "Learning Rate 0.007301704\n",
      "Learning Rate 0.007300628\n",
      "Learning Rate 0.007299552\n",
      "Learning Rate 0.0072984765\n",
      "Learning Rate 0.007297401\n",
      "Learning Rate 0.0072963256\n",
      "Learning Rate 0.0072952504\n",
      "Learning Rate 0.007294175\n",
      "18496/25000 [=====================>........] - ETA: 1s - loss: 0.6005 - accuracy: 0.8063Learning Rate 0.0072931005\n",
      "Learning Rate 0.0072920257\n",
      "Learning Rate 0.007290951\n",
      "Learning Rate 0.0072898767\n",
      "Learning Rate 0.0072888024\n",
      "Learning Rate 0.0072877286\n",
      "Learning Rate 0.007286655\n",
      "Learning Rate 0.007285581\n",
      "Learning Rate 0.0072845076\n",
      "18784/25000 [=====================>........] - ETA: 1s - loss: 0.5988 - accuracy: 0.8068Learning Rate 0.0072834343\n",
      "Learning Rate 0.007282361\n",
      "Learning Rate 0.007281288\n",
      "Learning Rate 0.007280215\n",
      "Learning Rate 0.0072791423\n",
      "Learning Rate 0.00727807\n",
      "Learning Rate 0.0072769974\n",
      "Learning Rate 0.007275925\n",
      "Learning Rate 0.007274853\n",
      "19072/25000 [=====================>........] - ETA: 1s - loss: 0.5966 - accuracy: 0.8073Learning Rate 0.007273781\n",
      "Learning Rate 0.007272709\n",
      "Learning Rate 0.0072716377\n",
      "Learning Rate 0.007270566\n",
      "Learning Rate 0.0072694947\n",
      "Learning Rate 0.0072684237\n",
      "Learning Rate 0.0072673527\n",
      "Learning Rate 0.0072662816\n",
      "Learning Rate 0.007265211\n",
      "19360/25000 [======================>.......] - ETA: 1s - loss: 0.5959 - accuracy: 0.8073Learning Rate 0.0072641405\n",
      "Learning Rate 0.00726307\n",
      "Learning Rate 0.007262\n",
      "Learning Rate 0.00726093\n",
      "Learning Rate 0.0072598597\n",
      "Learning Rate 0.00725879\n",
      "Learning Rate 0.0072577205\n",
      "Learning Rate 0.007256651\n",
      "Learning Rate 0.0072555817\n",
      "19648/25000 [======================>.......] - ETA: 0s - loss: 0.5947 - accuracy: 0.8080Learning Rate 0.0072545125\n",
      "Learning Rate 0.0072534434\n",
      "Learning Rate 0.0072523747\n",
      "Learning Rate 0.007251306\n",
      "Learning Rate 0.0072502373\n",
      "Learning Rate 0.007249169\n",
      "Learning Rate 0.007248101\n",
      "Learning Rate 0.0072470326\n",
      "Learning Rate 0.007245965\n",
      "19936/25000 [======================>.......] - ETA: 0s - loss: 0.5931 - accuracy: 0.8084Learning Rate 0.007244897\n",
      "Learning Rate 0.0072438293\n",
      "Learning Rate 0.007242762\n",
      "Learning Rate 0.0072416947\n",
      "Learning Rate 0.0072406274\n",
      "Learning Rate 0.0072395606\n",
      "Learning Rate 0.0072384938\n",
      "Learning Rate 0.007237427\n",
      "Learning Rate 0.0072363606\n",
      "20224/25000 [=======================>......] - ETA: 0s - loss: 0.5905 - accuracy: 0.8092Learning Rate 0.007235294\n",
      "Learning Rate 0.007234228\n",
      "Learning Rate 0.007233162\n",
      "Learning Rate 0.007232096\n",
      "Learning Rate 0.00723103\n",
      "Learning Rate 0.0072299647\n",
      "Learning Rate 0.0072288993\n",
      "Learning Rate 0.007227834\n",
      "Learning Rate 0.007226769\n",
      "20512/25000 [=======================>......] - ETA: 0s - loss: 0.5922 - accuracy: 0.8091Learning Rate 0.007225704\n",
      "Learning Rate 0.007224639\n",
      "Learning Rate 0.0072235744\n",
      "Learning Rate 0.00722251\n",
      "Learning Rate 0.0072214454\n",
      "Learning Rate 0.0072203814\n",
      "Learning Rate 0.0072193174\n",
      "Learning Rate 0.007218254\n",
      "Learning Rate 0.00721719\n",
      "Learning Rate 0.0072161267\n",
      "20832/25000 [=======================>......] - ETA: 0s - loss: 0.5906 - accuracy: 0.8095Learning Rate 0.0072150636\n",
      "Learning Rate 0.0072140004\n",
      "Learning Rate 0.0072129373\n",
      "Learning Rate 0.0072118747\n",
      "Learning Rate 0.007210812\n",
      "Learning Rate 0.0072097494\n",
      "Learning Rate 0.0072086873\n",
      "Learning Rate 0.007207625\n",
      "Learning Rate 0.007206563\n",
      "21120/25000 [========================>.....] - ETA: 0s - loss: 0.5896 - accuracy: 0.8099Learning Rate 0.007205501\n",
      "Learning Rate 0.0072044395\n",
      "Learning Rate 0.007203378\n",
      "Learning Rate 0.0072023165\n",
      "Learning Rate 0.0072012553\n",
      "Learning Rate 0.007200194\n",
      "Learning Rate 0.0071991333\n",
      "Learning Rate 0.0071980725\n",
      "Learning Rate 0.0071970117\n",
      "Learning Rate 0.0071959514\n",
      "21440/25000 [========================>.....] - ETA: 0s - loss: 0.5873 - accuracy: 0.8102Learning Rate 0.007194891\n",
      "Learning Rate 0.007193831\n",
      "Learning Rate 0.007192771\n",
      "Learning Rate 0.007191711\n",
      "Learning Rate 0.0071906513\n",
      "Learning Rate 0.007189592\n",
      "Learning Rate 0.0071885325\n",
      "Learning Rate 0.007187473\n",
      "Learning Rate 0.007186414\n",
      "Learning Rate 0.0071853553\n",
      "21760/25000 [=========================>....] - ETA: 0s - loss: 0.5865 - accuracy: 0.8106Learning Rate 0.0071842964\n",
      "Learning Rate 0.007183238\n",
      "Learning Rate 0.0071821795\n",
      "Learning Rate 0.007181121\n",
      "Learning Rate 0.007180063\n",
      "Learning Rate 0.007179005\n",
      "Learning Rate 0.007177947\n",
      "Learning Rate 0.0071768896\n",
      "Learning Rate 0.007175832\n",
      "22048/25000 [=========================>....] - ETA: 0s - loss: 0.5870 - accuracy: 0.8103Learning Rate 0.0071747745\n",
      "Learning Rate 0.0071737175\n",
      "Learning Rate 0.0071726604\n",
      "Learning Rate 0.0071716034\n",
      "Learning Rate 0.007170547\n",
      "Learning Rate 0.00716949\n",
      "Learning Rate 0.0071684336\n",
      "Learning Rate 0.0071673775\n",
      "Learning Rate 0.0071663214\n",
      "Learning Rate 0.0071652653\n",
      "22368/25000 [=========================>....] - ETA: 0s - loss: 0.5873 - accuracy: 0.8103Learning Rate 0.0071642096\n",
      "Learning Rate 0.007163154\n",
      "Learning Rate 0.0071620983\n",
      "Learning Rate 0.007161043\n",
      "Learning Rate 0.007159988\n",
      "Learning Rate 0.0071589327\n",
      "Learning Rate 0.007157878\n",
      "Learning Rate 0.0071568233\n",
      "Learning Rate 0.0071557686\n",
      "22656/25000 [==========================>...] - ETA: 0s - loss: 0.5860 - accuracy: 0.8102Learning Rate 0.0071547143\n",
      "Learning Rate 0.00715366\n",
      "Learning Rate 0.007152606\n",
      "Learning Rate 0.007151552\n",
      "Learning Rate 0.007150498\n",
      "Learning Rate 0.0071494444\n",
      "Learning Rate 0.007148391\n",
      "Learning Rate 0.007147338\n",
      "Learning Rate 0.0071462844\n",
      "Learning Rate 0.0071452316\n",
      "22976/25000 [==========================>...] - ETA: 0s - loss: 0.5830 - accuracy: 0.8110Learning Rate 0.0071441787\n",
      "Learning Rate 0.007143126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0071420735\n",
      "Learning Rate 0.007141021\n",
      "Learning Rate 0.0071399687\n",
      "Learning Rate 0.0071389168\n",
      "Learning Rate 0.007137865\n",
      "Learning Rate 0.007136813\n",
      "Learning Rate 0.0071357614\n",
      "23264/25000 [==========================>...] - ETA: 0s - loss: 0.5839 - accuracy: 0.8109Learning Rate 0.00713471\n",
      "Learning Rate 0.0071336585\n",
      "Learning Rate 0.0071326075\n",
      "Learning Rate 0.0071315565\n",
      "Learning Rate 0.0071305055\n",
      "Learning Rate 0.007129455\n",
      "Learning Rate 0.0071284045\n",
      "Learning Rate 0.007127354\n",
      "Learning Rate 0.007126304\n",
      "Learning Rate 0.007125254\n",
      "23584/25000 [===========================>..] - ETA: 0s - loss: 0.5835 - accuracy: 0.8109Learning Rate 0.0071242037\n",
      "Learning Rate 0.007123154\n",
      "Learning Rate 0.0071221045\n",
      "Learning Rate 0.007121055\n",
      "Learning Rate 0.007120006\n",
      "Learning Rate 0.0071189567\n",
      "Learning Rate 0.0071179075\n",
      "Learning Rate 0.007116859\n",
      "Learning Rate 0.00711581\n",
      "Learning Rate 0.0071147615\n",
      "23904/25000 [===========================>..] - ETA: 0s - loss: 0.5848 - accuracy: 0.8109Learning Rate 0.0071137133\n",
      "Learning Rate 0.007112665\n",
      "Learning Rate 0.007111617\n",
      "Learning Rate 0.007110569\n",
      "Learning Rate 0.0071095214\n",
      "Learning Rate 0.0071084737\n",
      "Learning Rate 0.0071074264\n",
      "Learning Rate 0.007106379\n",
      "Learning Rate 0.007105332\n",
      "24192/25000 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.8108Learning Rate 0.007104285\n",
      "Learning Rate 0.0071032383\n",
      "Learning Rate 0.0071021914\n",
      "Learning Rate 0.007101145\n",
      "Learning Rate 0.0071000988\n",
      "Learning Rate 0.0070990524\n",
      "Learning Rate 0.0070980066\n",
      "Learning Rate 0.0070969607\n",
      "Learning Rate 0.007095915\n",
      "Learning Rate 0.0070948694\n",
      "24512/25000 [============================>.] - ETA: 0s - loss: 0.5835 - accuracy: 0.8110Learning Rate 0.007093824\n",
      "Learning Rate 0.0070927786\n",
      "Learning Rate 0.0070917336\n",
      "Learning Rate 0.0070906887\n",
      "Learning Rate 0.0070896437\n",
      "Learning Rate 0.0070885993\n",
      "Learning Rate 0.007087555\n",
      "Learning Rate 0.0070865103\n",
      "Learning Rate 0.0070854663\n",
      "24800/25000 [============================>.] - ETA: 0s - loss: 0.5823 - accuracy: 0.8112Learning Rate 0.0070844223\n",
      "Learning Rate 0.0070833783\n",
      "Learning Rate 0.0070823347\n",
      "Learning Rate 0.007081291\n",
      "Learning Rate 0.0070802476\n",
      "Learning Rate 0.0070792045\n",
      "Learning Rate 0.0070781615\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.5811 - accuracy: 0.8117 - val_loss: 0.7031 - val_accuracy: 0.8266\n",
      "Epoch 4/5\n",
      "Learning Rate 0.0070771184\n",
      "   32/25000 [..............................] - ETA: 5s - loss: 0.0861 - accuracy: 0.9688Learning Rate 0.0070760758\n",
      "Learning Rate 0.007075033\n",
      "Learning Rate 0.0070739905\n",
      "Learning Rate 0.007072948\n",
      "Learning Rate 0.007071906\n",
      "Learning Rate 0.0070708636\n",
      "Learning Rate 0.0070698215\n",
      "Learning Rate 0.00706878\n",
      "Learning Rate 0.007067738\n",
      "Learning Rate 0.0070666964\n",
      "  352/25000 [..............................] - ETA: 4s - loss: 0.4343 - accuracy: 0.8693Learning Rate 0.007065655\n",
      "Learning Rate 0.007064614\n",
      "Learning Rate 0.0070635728\n",
      "Learning Rate 0.007062532\n",
      "Learning Rate 0.0070614913\n",
      "Learning Rate 0.0070604505\n",
      "Learning Rate 0.00705941\n",
      "Learning Rate 0.00705837\n",
      "Learning Rate 0.0070573296\n",
      "  640/25000 [..............................] - ETA: 4s - loss: 0.3888 - accuracy: 0.8797Learning Rate 0.00705629\n",
      "Learning Rate 0.00705525\n",
      "Learning Rate 0.00705421\n",
      "Learning Rate 0.007053171\n",
      "Learning Rate 0.0070521315\n",
      "Learning Rate 0.007051092\n",
      "Learning Rate 0.007050053\n",
      "Learning Rate 0.0070490143\n",
      "Learning Rate 0.0070479754\n",
      "Learning Rate 0.007046937\n",
      "  960/25000 [>.............................] - ETA: 4s - loss: 0.4575 - accuracy: 0.8521Learning Rate 0.0070458986\n",
      "Learning Rate 0.00704486\n",
      "Learning Rate 0.007043822\n",
      "Learning Rate 0.0070427842\n",
      "Learning Rate 0.0070417463\n",
      "Learning Rate 0.007040709\n",
      "Learning Rate 0.0070396713\n",
      "Learning Rate 0.007038634\n",
      "Learning Rate 0.007037597\n",
      " 1248/25000 [>.............................] - ETA: 4s - loss: 0.5260 - accuracy: 0.8349Learning Rate 0.0070365597\n",
      "Learning Rate 0.0070355227\n",
      "Learning Rate 0.007034486\n",
      "Learning Rate 0.0070334496\n",
      "Learning Rate 0.007032413\n",
      "Learning Rate 0.007031377\n",
      "Learning Rate 0.007030341\n",
      "Learning Rate 0.0070293047\n",
      "Learning Rate 0.007028269\n",
      " 1536/25000 [>.............................] - ETA: 4s - loss: 0.4936 - accuracy: 0.8405Learning Rate 0.0070272335\n",
      "Learning Rate 0.007026198\n",
      "Learning Rate 0.0070251627\n",
      "Learning Rate 0.0070241275\n",
      "Learning Rate 0.0070230924\n",
      "Learning Rate 0.0070220577\n",
      "Learning Rate 0.007021023\n",
      "Learning Rate 0.0070199883\n",
      "Learning Rate 0.007018954\n",
      " 1824/25000 [=>............................] - ETA: 4s - loss: 0.4832 - accuracy: 0.8388Learning Rate 0.00701792\n",
      "Learning Rate 0.0070168856\n",
      "Learning Rate 0.007015852\n",
      "Learning Rate 0.007014818\n",
      "Learning Rate 0.0070137843\n",
      "Learning Rate 0.007012751\n",
      "Learning Rate 0.0070117177\n",
      "Learning Rate 0.0070106843\n",
      "Learning Rate 0.0070096515\n",
      "Learning Rate 0.0070086187\n",
      " 2144/25000 [=>............................] - ETA: 4s - loss: 0.4739 - accuracy: 0.8414Learning Rate 0.007007586\n",
      "Learning Rate 0.0070065535\n",
      "Learning Rate 0.007005521\n",
      "Learning Rate 0.0070044887\n",
      "Learning Rate 0.0070034564\n",
      "Learning Rate 0.0070024244\n",
      "Learning Rate 0.0070013925\n",
      "Learning Rate 0.0070003606\n",
      "Learning Rate 0.006999329\n",
      " 2432/25000 [=>............................] - ETA: 3s - loss: 0.4811 - accuracy: 0.8392Learning Rate 0.0069982978\n",
      "Learning Rate 0.0069972663\n",
      "Learning Rate 0.0069962353\n",
      "Learning Rate 0.0069952044\n",
      "Learning Rate 0.0069941734\n",
      "Learning Rate 0.006993143\n",
      "Learning Rate 0.0069921124\n",
      "Learning Rate 0.006991082\n",
      "Learning Rate 0.006990052\n",
      "Learning Rate 0.006989022\n",
      " 2752/25000 [==>...........................] - ETA: 3s - loss: 0.4746 - accuracy: 0.8387Learning Rate 0.0069879917\n",
      "Learning Rate 0.006986962\n",
      "Learning Rate 0.0069859326\n",
      "Learning Rate 0.006984903\n",
      "Learning Rate 0.006983874\n",
      "Learning Rate 0.006982845\n",
      "Learning Rate 0.0069818157\n",
      "Learning Rate 0.006980787\n",
      "Learning Rate 0.0069797584\n",
      " 3040/25000 [==>...........................] - ETA: 3s - loss: 0.4739 - accuracy: 0.8375Learning Rate 0.0069787297\n",
      "Learning Rate 0.0069777016\n",
      "Learning Rate 0.0069766734\n",
      "Learning Rate 0.006975645\n",
      "Learning Rate 0.0069746175\n",
      "Learning Rate 0.00697359\n",
      "Learning Rate 0.006972562\n",
      "Learning Rate 0.006971535\n",
      "Learning Rate 0.0069705076\n",
      "Learning Rate 0.0069694803\n",
      " 3360/25000 [===>..........................] - ETA: 3s - loss: 0.4828 - accuracy: 0.8354Learning Rate 0.0069684535\n",
      "Learning Rate 0.0069674267\n",
      "Learning Rate 0.0069664\n",
      "Learning Rate 0.0069653736\n",
      "Learning Rate 0.0069643473\n",
      "Learning Rate 0.006963321\n",
      "Learning Rate 0.006962295\n",
      "Learning Rate 0.0069612693\n",
      "Learning Rate 0.0069602435\n",
      " 3648/25000 [===>..........................] - ETA: 3s - loss: 0.4906 - accuracy: 0.8339Learning Rate 0.0069592176\n",
      "Learning Rate 0.006958192\n",
      "Learning Rate 0.006957167\n",
      "Learning Rate 0.0069561414\n",
      "Learning Rate 0.0069551165\n",
      "Learning Rate 0.0069540916\n",
      "Learning Rate 0.0069530667\n",
      "Learning Rate 0.006952042\n",
      " 3904/25000 [===>..........................] - ETA: 3s - loss: 0.4909 - accuracy: 0.8330Learning Rate 0.006951018\n",
      "Learning Rate 0.0069499933\n",
      "Learning Rate 0.0069489693\n",
      "Learning Rate 0.0069479453\n",
      "Learning Rate 0.0069469213\n",
      "Learning Rate 0.006945898\n",
      "Learning Rate 0.0069448743\n",
      "Learning Rate 0.006943851\n",
      " 4160/25000 [===>..........................] - ETA: 3s - loss: 0.4929 - accuracy: 0.8329Learning Rate 0.0069428277\n",
      "Learning Rate 0.0069418047\n",
      "Learning Rate 0.0069407816\n",
      "Learning Rate 0.006939759\n",
      "Learning Rate 0.0069387364\n",
      "Learning Rate 0.006937714\n",
      "Learning Rate 0.0069366917\n",
      "Learning Rate 0.0069356696\n",
      "Learning Rate 0.0069346474\n",
      " 4448/25000 [====>.........................] - ETA: 3s - loss: 0.5074 - accuracy: 0.8316Learning Rate 0.006933626\n",
      "Learning Rate 0.006932604\n",
      "Learning Rate 0.0069315825\n",
      "Learning Rate 0.0069305613\n",
      "Learning Rate 0.00692954\n",
      "Learning Rate 0.006928519\n",
      "Learning Rate 0.006927498\n",
      "Learning Rate 0.0069264774\n",
      "Learning Rate 0.0069254567\n",
      " 4736/25000 [====>.........................] - ETA: 3s - loss: 0.5046 - accuracy: 0.8319Learning Rate 0.006924436\n",
      "Learning Rate 0.0069234157\n",
      "Learning Rate 0.0069223954\n",
      "Learning Rate 0.006921375\n",
      "Learning Rate 0.0069203554\n",
      "Learning Rate 0.0069193356\n",
      "Learning Rate 0.006918316\n",
      "Learning Rate 0.0069172964\n",
      "Learning Rate 0.006916277\n",
      "Learning Rate 0.006915258\n",
      " 5056/25000 [=====>........................] - ETA: 3s - loss: 0.4970 - accuracy: 0.8341Learning Rate 0.006914239\n",
      "Learning Rate 0.00691322\n",
      "Learning Rate 0.006912201\n",
      "Learning Rate 0.006911183\n",
      "Learning Rate 0.0069101644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.006909146\n",
      "Learning Rate 0.006908128\n",
      "Learning Rate 0.00690711\n",
      "Learning Rate 0.006906092\n",
      " 5344/25000 [=====>........................] - ETA: 3s - loss: 0.4957 - accuracy: 0.8340Learning Rate 0.0069050747\n",
      "Learning Rate 0.006904057\n",
      "Learning Rate 0.0069030398\n",
      "Learning Rate 0.0069020228\n",
      "Learning Rate 0.0069010057\n",
      "Learning Rate 0.0068999887\n",
      "Learning Rate 0.006898972\n",
      "Learning Rate 0.0068979557\n",
      "Learning Rate 0.006896939\n",
      "Learning Rate 0.006895923\n",
      " 5664/25000 [=====>........................] - ETA: 3s - loss: 0.4915 - accuracy: 0.8349Learning Rate 0.006894907\n",
      "Learning Rate 0.006893891\n",
      "Learning Rate 0.006892875\n",
      "Learning Rate 0.006891859\n",
      "Learning Rate 0.0068908436\n",
      "Learning Rate 0.006889828\n",
      "Learning Rate 0.006888813\n",
      "Learning Rate 0.0068877977\n",
      " 5920/25000 [======>.......................] - ETA: 3s - loss: 0.4890 - accuracy: 0.8368Learning Rate 0.0068867826\n",
      "Learning Rate 0.006885768\n",
      "Learning Rate 0.0068847532\n",
      "Learning Rate 0.0068837386\n",
      "Learning Rate 0.0068827244\n",
      "Learning Rate 0.00688171\n",
      "Learning Rate 0.006880696\n",
      "Learning Rate 0.006879682\n",
      " 6176/25000 [======>.......................] - ETA: 3s - loss: 0.4878 - accuracy: 0.8374Learning Rate 0.0068786684\n",
      "Learning Rate 0.0068776547\n",
      "Learning Rate 0.0068766414\n",
      "Learning Rate 0.006875628\n",
      "Learning Rate 0.006874615\n",
      "Learning Rate 0.006873602\n",
      "Learning Rate 0.0068725892\n",
      "Learning Rate 0.0068715764\n",
      "Learning Rate 0.006870564\n",
      " 6464/25000 [======>.......................] - ETA: 3s - loss: 0.4843 - accuracy: 0.8385Learning Rate 0.0068695517\n",
      "Learning Rate 0.0068685394\n",
      "Learning Rate 0.0068675275\n",
      "Learning Rate 0.0068665156\n",
      "Learning Rate 0.0068655037\n",
      "Learning Rate 0.006864492\n",
      "Learning Rate 0.0068634804\n",
      "Learning Rate 0.006862469\n",
      "Learning Rate 0.0068614576\n",
      " 6752/25000 [=======>......................] - ETA: 3s - loss: 0.4835 - accuracy: 0.8392Learning Rate 0.0068604467\n",
      "Learning Rate 0.0068594357\n",
      "Learning Rate 0.0068584247\n",
      "Learning Rate 0.0068574143\n",
      "Learning Rate 0.006856404\n",
      "Learning Rate 0.0068553933\n",
      "Learning Rate 0.0068543833\n",
      "Learning Rate 0.0068533733\n",
      "Learning Rate 0.0068523632\n",
      "Learning Rate 0.0068513537\n",
      " 7072/25000 [=======>......................] - ETA: 3s - loss: 0.4846 - accuracy: 0.8377Learning Rate 0.006850344\n",
      "Learning Rate 0.0068493346\n",
      "Learning Rate 0.0068483255\n",
      "Learning Rate 0.0068473164\n",
      "Learning Rate 0.0068463073\n",
      "Learning Rate 0.0068452987\n",
      "Learning Rate 0.00684429\n",
      "Learning Rate 0.0068432814\n",
      "Learning Rate 0.0068422733\n",
      " 7360/25000 [=======>......................] - ETA: 3s - loss: 0.4832 - accuracy: 0.8374Learning Rate 0.006841265\n",
      "Learning Rate 0.006840257\n",
      "Learning Rate 0.006839249\n",
      "Learning Rate 0.006838241\n",
      "Learning Rate 0.0068372334\n",
      "Learning Rate 0.0068362257\n",
      "Learning Rate 0.0068352185\n",
      "Learning Rate 0.0068342113\n",
      "Learning Rate 0.006833204\n",
      " 7648/25000 [========>.....................] - ETA: 3s - loss: 0.4791 - accuracy: 0.8385Learning Rate 0.0068321973\n",
      "Learning Rate 0.0068311905\n",
      "Learning Rate 0.006830184\n",
      "Learning Rate 0.0068291775\n",
      "Learning Rate 0.006828171\n",
      "Learning Rate 0.006827165\n",
      "Learning Rate 0.006826159\n",
      "Learning Rate 0.0068251533\n",
      "Learning Rate 0.0068241474\n",
      " 7936/25000 [========>.....................] - ETA: 3s - loss: 0.4839 - accuracy: 0.8373Learning Rate 0.006823142\n",
      "Learning Rate 0.0068221367\n",
      "Learning Rate 0.0068211313\n",
      "Learning Rate 0.0068201264\n",
      "Learning Rate 0.0068191215\n",
      "Learning Rate 0.0068181166\n",
      "Learning Rate 0.0068171117\n",
      "Learning Rate 0.0068161073\n",
      "Learning Rate 0.006815103\n",
      " 8224/25000 [========>.....................] - ETA: 3s - loss: 0.4894 - accuracy: 0.8368Learning Rate 0.0068140985\n",
      "Learning Rate 0.0068130945\n",
      "Learning Rate 0.0068120905\n",
      "Learning Rate 0.0068110866\n",
      "Learning Rate 0.006810083\n",
      "Learning Rate 0.0068090796\n",
      "Learning Rate 0.006808076\n",
      "Learning Rate 0.006807073\n",
      "Learning Rate 0.00680607\n",
      " 8512/25000 [=========>....................] - ETA: 2s - loss: 0.4890 - accuracy: 0.8361Learning Rate 0.006805067\n",
      "Learning Rate 0.0068040644\n",
      "Learning Rate 0.006803062\n",
      "Learning Rate 0.0068020592\n",
      "Learning Rate 0.006801057\n",
      "Learning Rate 0.006800055\n",
      "Learning Rate 0.006799053\n",
      "Learning Rate 0.006798051\n",
      "Learning Rate 0.006797049\n",
      " 8800/25000 [=========>....................] - ETA: 2s - loss: 0.4905 - accuracy: 0.8353Learning Rate 0.0067960476\n",
      "Learning Rate 0.006795046\n",
      "Learning Rate 0.0067940447\n",
      "Learning Rate 0.0067930436\n",
      "Learning Rate 0.0067920424\n",
      "Learning Rate 0.0067910417\n",
      "Learning Rate 0.006790041\n",
      "Learning Rate 0.0067890403\n",
      "Learning Rate 0.00678804\n",
      " 9088/25000 [=========>....................] - ETA: 2s - loss: 0.4891 - accuracy: 0.8356Learning Rate 0.00678704\n",
      "Learning Rate 0.0067860396\n",
      "Learning Rate 0.00678504\n",
      "Learning Rate 0.00678404\n",
      "Learning Rate 0.0067830402\n",
      "Learning Rate 0.006782041\n",
      "Learning Rate 0.0067810416\n",
      "Learning Rate 0.0067800423\n",
      " 9344/25000 [==========>...................] - ETA: 2s - loss: 0.4872 - accuracy: 0.8363Learning Rate 0.0067790435\n",
      "Learning Rate 0.0067780446\n",
      "Learning Rate 0.006777046\n",
      "Learning Rate 0.006776047\n",
      "Learning Rate 0.0067750486\n",
      "Learning Rate 0.00677405\n",
      "Learning Rate 0.006773052\n",
      "Learning Rate 0.006772054\n",
      " 9600/25000 [==========>...................] - ETA: 2s - loss: 0.4842 - accuracy: 0.8372Learning Rate 0.006771056\n",
      "Learning Rate 0.006770058\n",
      "Learning Rate 0.0067690606\n",
      "Learning Rate 0.006768063\n",
      "Learning Rate 0.0067670657\n",
      "Learning Rate 0.0067660687\n",
      "Learning Rate 0.006765072\n",
      "Learning Rate 0.006764075\n",
      "Learning Rate 0.0067630783\n",
      " 9888/25000 [==========>...................] - ETA: 2s - loss: 0.4874 - accuracy: 0.8369Learning Rate 0.0067620818\n",
      "Learning Rate 0.0067610852\n",
      "Learning Rate 0.006760089\n",
      "Learning Rate 0.006759093\n",
      "Learning Rate 0.006758097\n",
      "Learning Rate 0.006757101\n",
      "Learning Rate 0.0067561055\n",
      "Learning Rate 0.00675511\n",
      "Learning Rate 0.0067541143\n",
      "Learning Rate 0.006753119\n",
      "10208/25000 [===========>..................] - ETA: 2s - loss: 0.4927 - accuracy: 0.8353Learning Rate 0.006752124\n",
      "Learning Rate 0.006751129\n",
      "Learning Rate 0.0067501343\n",
      "Learning Rate 0.0067491396\n",
      "Learning Rate 0.006748145\n",
      "Learning Rate 0.006747151\n",
      "Learning Rate 0.0067461566\n",
      "Learning Rate 0.0067451624\n",
      "Learning Rate 0.0067441687\n",
      "10496/25000 [===========>..................] - ETA: 2s - loss: 0.4900 - accuracy: 0.8366Learning Rate 0.006743175\n",
      "Learning Rate 0.0067421813\n",
      "Learning Rate 0.0067411875\n",
      "Learning Rate 0.0067401943\n",
      "Learning Rate 0.006739201\n",
      "Learning Rate 0.006738208\n",
      "Learning Rate 0.006737215\n",
      "Learning Rate 0.006736222\n",
      "Learning Rate 0.0067352294\n",
      "Learning Rate 0.006734237\n",
      "10816/25000 [===========>..................] - ETA: 2s - loss: 0.4921 - accuracy: 0.8364Learning Rate 0.0067332448\n",
      "Learning Rate 0.0067322524\n",
      "Learning Rate 0.0067312606\n",
      "Learning Rate 0.0067302687\n",
      "Learning Rate 0.006729277\n",
      "Learning Rate 0.0067282855\n",
      "Learning Rate 0.006727294\n",
      "Learning Rate 0.0067263027\n",
      "Learning Rate 0.0067253117\n",
      "11104/25000 [============>.................] - ETA: 2s - loss: 0.4934 - accuracy: 0.8350Learning Rate 0.006724321\n",
      "Learning Rate 0.00672333\n",
      "Learning Rate 0.006722339\n",
      "Learning Rate 0.0067213485\n",
      "Learning Rate 0.006720358\n",
      "Learning Rate 0.0067193676\n",
      "Learning Rate 0.0067183776\n",
      "Learning Rate 0.0067173876\n",
      "Learning Rate 0.0067163976\n",
      "11392/25000 [============>.................] - ETA: 2s - loss: 0.4937 - accuracy: 0.8351Learning Rate 0.006715408\n",
      "Learning Rate 0.0067144185\n",
      "Learning Rate 0.006713429\n",
      "Learning Rate 0.00671244\n",
      "Learning Rate 0.006711451\n",
      "Learning Rate 0.006710462\n",
      "Learning Rate 0.006709473\n",
      "Learning Rate 0.0067084846\n",
      "Learning Rate 0.006707496\n",
      "11680/25000 [=============>................] - ETA: 2s - loss: 0.4916 - accuracy: 0.8360Learning Rate 0.0067065074\n",
      "Learning Rate 0.0067055193\n",
      "Learning Rate 0.006704531\n",
      "Learning Rate 0.006703543\n",
      "Learning Rate 0.0067025553\n",
      "Learning Rate 0.0067015677\n",
      "Learning Rate 0.00670058\n",
      "Learning Rate 0.006699593\n",
      "Learning Rate 0.0066986056\n",
      "11968/25000 [=============>................] - ETA: 2s - loss: 0.4923 - accuracy: 0.8359Learning Rate 0.0066976184\n",
      "Learning Rate 0.0066966317\n",
      "Learning Rate 0.006695645\n",
      "Learning Rate 0.006694658\n",
      "Learning Rate 0.006693672\n",
      "Learning Rate 0.0066926857\n",
      "Learning Rate 0.0066916994\n",
      "Learning Rate 0.006690713\n",
      "12224/25000 [=============>................] - ETA: 2s - loss: 0.4894 - accuracy: 0.8370Learning Rate 0.0066897273\n",
      "Learning Rate 0.0066887415\n",
      "Learning Rate 0.0066877557\n",
      "Learning Rate 0.0066867704\n",
      "Learning Rate 0.006685785\n",
      "Learning Rate 0.0066847997\n",
      "Learning Rate 0.006683815\n",
      "Learning Rate 0.00668283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.006681845\n",
      "12512/25000 [==============>...............] - ETA: 2s - loss: 0.4939 - accuracy: 0.8362Learning Rate 0.0066808606\n",
      "Learning Rate 0.0066798762\n",
      "Learning Rate 0.006678892\n",
      "Learning Rate 0.006677908\n",
      "Learning Rate 0.006676924\n",
      "Learning Rate 0.00667594\n",
      "Learning Rate 0.006674956\n",
      "Learning Rate 0.0066739726\n",
      "12768/25000 [==============>...............] - ETA: 2s - loss: 0.4951 - accuracy: 0.8357Learning Rate 0.006672989\n",
      "Learning Rate 0.0066720056\n",
      "Learning Rate 0.0066710226\n",
      "Learning Rate 0.0066700396\n",
      "Learning Rate 0.0066690566\n",
      "Learning Rate 0.006668074\n",
      "Learning Rate 0.0066670915\n",
      "Learning Rate 0.006666109\n",
      "Learning Rate 0.006665127\n",
      "13056/25000 [==============>...............] - ETA: 2s - loss: 0.4980 - accuracy: 0.8352Learning Rate 0.006664145\n",
      "Learning Rate 0.0066631627\n",
      "Learning Rate 0.0066621806\n",
      "Learning Rate 0.006661199\n",
      "Learning Rate 0.0066602174\n",
      "Learning Rate 0.006659236\n",
      "Learning Rate 0.0066582547\n",
      "Learning Rate 0.0066572735\n",
      "Learning Rate 0.0066562924\n",
      "Learning Rate 0.0066553117\n",
      "13376/25000 [===============>..............] - ETA: 2s - loss: 0.4977 - accuracy: 0.8347Learning Rate 0.006654331\n",
      "Learning Rate 0.0066533503\n",
      "Learning Rate 0.00665237\n",
      "Learning Rate 0.00665139\n",
      "Learning Rate 0.0066504097\n",
      "Learning Rate 0.00664943\n",
      "Learning Rate 0.00664845\n",
      "Learning Rate 0.0066474704\n",
      "Learning Rate 0.0066464907\n",
      "13664/25000 [===============>..............] - ETA: 2s - loss: 0.4997 - accuracy: 0.8351Learning Rate 0.0066455114\n",
      "Learning Rate 0.006644532\n",
      "Learning Rate 0.006643553\n",
      "Learning Rate 0.006642574\n",
      "Learning Rate 0.006641595\n",
      "Learning Rate 0.0066406163\n",
      "Learning Rate 0.006639638\n",
      "Learning Rate 0.0066386596\n",
      "Learning Rate 0.0066376813\n",
      "13952/25000 [===============>..............] - ETA: 2s - loss: 0.5010 - accuracy: 0.8344Learning Rate 0.0066367034\n",
      "Learning Rate 0.0066357255\n",
      "Learning Rate 0.0066347476\n",
      "Learning Rate 0.0066337697\n",
      "Learning Rate 0.0066327923\n",
      "Learning Rate 0.006631815\n",
      "Learning Rate 0.0066308375\n",
      "Learning Rate 0.0066298605\n",
      "Learning Rate 0.0066288835\n",
      "14240/25000 [================>.............] - ETA: 1s - loss: 0.5016 - accuracy: 0.8343Learning Rate 0.0066279066\n",
      "Learning Rate 0.00662693\n",
      "Learning Rate 0.0066259536\n",
      "Learning Rate 0.006624977\n",
      "Learning Rate 0.006624001\n",
      "Learning Rate 0.006623025\n",
      "Learning Rate 0.006622049\n",
      "Learning Rate 0.0066210735\n",
      "Learning Rate 0.006620098\n",
      "14528/25000 [================>.............] - ETA: 1s - loss: 0.5012 - accuracy: 0.8340Learning Rate 0.0066191223\n",
      "Learning Rate 0.006618147\n",
      "Learning Rate 0.0066171717\n",
      "Learning Rate 0.0066161966\n",
      "Learning Rate 0.0066152215\n",
      "Learning Rate 0.006614247\n",
      "Learning Rate 0.0066132722\n",
      "Learning Rate 0.0066122976\n",
      "14784/25000 [================>.............] - ETA: 1s - loss: 0.5014 - accuracy: 0.8337Learning Rate 0.0066113234\n",
      "Learning Rate 0.0066103493\n",
      "Learning Rate 0.006609375\n",
      "Learning Rate 0.0066084014\n",
      "Learning Rate 0.0066074277\n",
      "Learning Rate 0.006606454\n",
      "Learning Rate 0.0066054803\n",
      "Learning Rate 0.006604507\n",
      "Learning Rate 0.006603534\n",
      "15072/25000 [=================>............] - ETA: 1s - loss: 0.5005 - accuracy: 0.8340Learning Rate 0.0066025606\n",
      "Learning Rate 0.006601588\n",
      "Learning Rate 0.006600615\n",
      "Learning Rate 0.0065996423\n",
      "Learning Rate 0.00659867\n",
      "Learning Rate 0.0065976977\n",
      "Learning Rate 0.0065967254\n",
      "Learning Rate 0.0065957536\n",
      "Learning Rate 0.0065947818\n",
      "15360/25000 [=================>............] - ETA: 1s - loss: 0.4985 - accuracy: 0.8344Learning Rate 0.00659381\n",
      "Learning Rate 0.006592838\n",
      "Learning Rate 0.0065918667\n",
      "Learning Rate 0.0065908954\n",
      "Learning Rate 0.006589924\n",
      "Learning Rate 0.006588953\n",
      "Learning Rate 0.006587982\n",
      "Learning Rate 0.0065870113\n",
      "Learning Rate 0.006586041\n",
      "15648/25000 [=================>............] - ETA: 1s - loss: 0.4978 - accuracy: 0.8344Learning Rate 0.0065850704\n",
      "Learning Rate 0.0065841\n",
      "Learning Rate 0.00658313\n",
      "Learning Rate 0.00658216\n",
      "Learning Rate 0.00658119\n",
      "Learning Rate 0.00658022\n",
      "Learning Rate 0.0065792506\n",
      "Learning Rate 0.006578281\n",
      "Learning Rate 0.0065773115\n",
      "15936/25000 [==================>...........] - ETA: 1s - loss: 0.4968 - accuracy: 0.8347Learning Rate 0.0065763425\n",
      "Learning Rate 0.0065753735\n",
      "Learning Rate 0.0065744044\n",
      "Learning Rate 0.006573436\n",
      "Learning Rate 0.0065724673\n",
      "Learning Rate 0.0065714987\n",
      "Learning Rate 0.0065705306\n",
      "Learning Rate 0.0065695625\n",
      "16192/25000 [==================>...........] - ETA: 1s - loss: 0.4963 - accuracy: 0.8345Learning Rate 0.0065685944\n",
      "Learning Rate 0.0065676263\n",
      "Learning Rate 0.0065666586\n",
      "Learning Rate 0.006565691\n",
      "Learning Rate 0.0065647233\n",
      "Learning Rate 0.006563756\n",
      "Learning Rate 0.006562789\n",
      "Learning Rate 0.006561822\n",
      "Learning Rate 0.006560855\n",
      "16480/25000 [==================>...........] - ETA: 1s - loss: 0.4964 - accuracy: 0.8348Learning Rate 0.0065598884\n",
      "Learning Rate 0.0065589217\n",
      "Learning Rate 0.006557955\n",
      "Learning Rate 0.0065569887\n",
      "Learning Rate 0.0065560224\n",
      "Learning Rate 0.006555056\n",
      "Learning Rate 0.0065540904\n",
      "Learning Rate 0.0065531246\n",
      "Learning Rate 0.006552159\n",
      "16768/25000 [===================>..........] - ETA: 1s - loss: 0.4972 - accuracy: 0.8346Learning Rate 0.0065511935\n",
      "Learning Rate 0.006550228\n",
      "Learning Rate 0.006549263\n",
      "Learning Rate 0.006548298\n",
      "Learning Rate 0.006547333\n",
      "Learning Rate 0.0065463684\n",
      "Learning Rate 0.0065454035\n",
      "Learning Rate 0.006544439\n",
      "Learning Rate 0.0065434747\n",
      "17056/25000 [===================>..........] - ETA: 1s - loss: 0.4959 - accuracy: 0.8353Learning Rate 0.0065425104\n",
      "Learning Rate 0.0065415464\n",
      "Learning Rate 0.0065405825\n",
      "Learning Rate 0.0065396186\n",
      "Learning Rate 0.006538655\n",
      "Learning Rate 0.0065376917\n",
      "Learning Rate 0.0065367282\n",
      "Learning Rate 0.0065357652\n",
      "Learning Rate 0.0065348023\n",
      "17344/25000 [===================>..........] - ETA: 1s - loss: 0.4955 - accuracy: 0.8351Learning Rate 0.0065338393\n",
      "Learning Rate 0.0065328763\n",
      "Learning Rate 0.0065319138\n",
      "Learning Rate 0.0065309512\n",
      "Learning Rate 0.0065299887\n",
      "Learning Rate 0.0065290267\n",
      "Learning Rate 0.0065280646\n",
      "Learning Rate 0.0065271026\n",
      "17600/25000 [====================>.........] - ETA: 1s - loss: 0.4964 - accuracy: 0.8346Learning Rate 0.006526141\n",
      "Learning Rate 0.0065251794\n",
      "Learning Rate 0.006524218\n",
      "Learning Rate 0.006523256\n",
      "Learning Rate 0.006522295\n",
      "Learning Rate 0.006521334\n",
      "Learning Rate 0.006520373\n",
      "Learning Rate 0.006519412\n",
      "17856/25000 [====================>.........] - ETA: 1s - loss: 0.4961 - accuracy: 0.8346Learning Rate 0.0065184515\n",
      "Learning Rate 0.006517491\n",
      "Learning Rate 0.0065165306\n",
      "Learning Rate 0.0065155705\n",
      "Learning Rate 0.0065146103\n",
      "Learning Rate 0.0065136505\n",
      "Learning Rate 0.006512691\n",
      "Learning Rate 0.006511731\n",
      "18112/25000 [====================>.........] - ETA: 1s - loss: 0.4950 - accuracy: 0.8348Learning Rate 0.0065107713\n",
      "Learning Rate 0.006509812\n",
      "Learning Rate 0.006508853\n",
      "Learning Rate 0.0065078936\n",
      "Learning Rate 0.0065069348\n",
      "Learning Rate 0.006505976\n",
      "Learning Rate 0.006505017\n",
      "Learning Rate 0.006504059\n",
      "Learning Rate 0.0065031005\n",
      "18400/25000 [=====================>........] - ETA: 1s - loss: 0.4943 - accuracy: 0.8347Learning Rate 0.006502142\n",
      "Learning Rate 0.006501184\n",
      "Learning Rate 0.006500226\n",
      "Learning Rate 0.006499268\n",
      "Learning Rate 0.0064983102\n",
      "Learning Rate 0.006497353\n",
      "Learning Rate 0.0064963955\n",
      "Learning Rate 0.006495438\n",
      "Learning Rate 0.006494481\n",
      "18688/25000 [=====================>........] - ETA: 1s - loss: 0.4943 - accuracy: 0.8343Learning Rate 0.006493524\n",
      "Learning Rate 0.0064925672\n",
      "Learning Rate 0.0064916103\n",
      "Learning Rate 0.006490654\n",
      "Learning Rate 0.0064896974\n",
      "Learning Rate 0.006488741\n",
      "Learning Rate 0.006487785\n",
      "Learning Rate 0.006486829\n",
      "18944/25000 [=====================>........] - ETA: 1s - loss: 0.4938 - accuracy: 0.8345Learning Rate 0.006485873\n",
      "Learning Rate 0.0064849174\n",
      "Learning Rate 0.006483962\n",
      "Learning Rate 0.0064830063\n",
      "Learning Rate 0.006482051\n",
      "Learning Rate 0.006481096\n",
      "Learning Rate 0.006480141\n",
      "Learning Rate 0.006479186\n",
      "Learning Rate 0.0064782314\n",
      "19232/25000 [======================>.......] - ETA: 1s - loss: 0.4930 - accuracy: 0.8350Learning Rate 0.006477277\n",
      "Learning Rate 0.006476322\n",
      "Learning Rate 0.006475368\n",
      "Learning Rate 0.006474414\n",
      "Learning Rate 0.00647346\n",
      "Learning Rate 0.006472506\n",
      "Learning Rate 0.0064715524\n",
      "Learning Rate 0.0064705987\n",
      "19488/25000 [======================>.......] - ETA: 1s - loss: 0.4919 - accuracy: 0.8353Learning Rate 0.006469645\n",
      "Learning Rate 0.006468692\n",
      "Learning Rate 0.0064677387\n",
      "Learning Rate 0.0064667854\n",
      "Learning Rate 0.0064658327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.00646488\n",
      "Learning Rate 0.006463927\n",
      "Learning Rate 0.006462975\n",
      "Learning Rate 0.0064620227\n",
      "19776/25000 [======================>.......] - ETA: 0s - loss: 0.4911 - accuracy: 0.8357Learning Rate 0.0064610704\n",
      "Learning Rate 0.006460118\n",
      "Learning Rate 0.0064591663\n",
      "Learning Rate 0.0064582145\n",
      "Learning Rate 0.0064572627\n",
      "Learning Rate 0.0064563113\n",
      "Learning Rate 0.00645536\n",
      "Learning Rate 0.0064544086\n",
      "Learning Rate 0.0064534578\n",
      "20064/25000 [=======================>......] - ETA: 0s - loss: 0.4910 - accuracy: 0.8362Learning Rate 0.006452507\n",
      "Learning Rate 0.006451556\n",
      "Learning Rate 0.006450605\n",
      "Learning Rate 0.0064496547\n",
      "Learning Rate 0.0064487043\n",
      "Learning Rate 0.006447754\n",
      "Learning Rate 0.006446804\n",
      "Learning Rate 0.006445854\n",
      "Learning Rate 0.006444904\n",
      "20352/25000 [=======================>......] - ETA: 0s - loss: 0.4903 - accuracy: 0.8365Learning Rate 0.0064439545\n",
      "Learning Rate 0.006443005\n",
      "Learning Rate 0.0064420556\n",
      "Learning Rate 0.006441106\n",
      "Learning Rate 0.006440157\n",
      "Learning Rate 0.006439208\n",
      "Learning Rate 0.006438259\n",
      "Learning Rate 0.0064373105\n",
      "Learning Rate 0.006436362\n",
      "20640/25000 [=======================>......] - ETA: 0s - loss: 0.4901 - accuracy: 0.8365Learning Rate 0.0064354134\n",
      "Learning Rate 0.0064344653\n",
      "Learning Rate 0.006433517\n",
      "Learning Rate 0.006432569\n",
      "Learning Rate 0.006431621\n",
      "Learning Rate 0.0064306734\n",
      "Learning Rate 0.006429726\n",
      "Learning Rate 0.006428778\n",
      "Learning Rate 0.006427831\n",
      "20928/25000 [========================>.....] - ETA: 0s - loss: 0.4904 - accuracy: 0.8366Learning Rate 0.006426884\n",
      "Learning Rate 0.0064259367\n",
      "Learning Rate 0.00642499\n",
      "Learning Rate 0.0064240433\n",
      "Learning Rate 0.0064230966\n",
      "Learning Rate 0.00642215\n",
      "Learning Rate 0.0064212037\n",
      "Learning Rate 0.0064202575\n",
      "Learning Rate 0.0064193113\n",
      "21216/25000 [========================>.....] - ETA: 0s - loss: 0.4924 - accuracy: 0.8360Learning Rate 0.0064183655\n",
      "Learning Rate 0.0064174198\n",
      "Learning Rate 0.006416474\n",
      "Learning Rate 0.0064155287\n",
      "Learning Rate 0.0064145834\n",
      "Learning Rate 0.006413638\n",
      "Learning Rate 0.006412693\n",
      "Learning Rate 0.006411748\n",
      "Learning Rate 0.006410803\n",
      "21504/25000 [========================>.....] - ETA: 0s - loss: 0.4931 - accuracy: 0.8359Learning Rate 0.0064098584\n",
      "Learning Rate 0.006408914\n",
      "Learning Rate 0.0064079696\n",
      "Learning Rate 0.0064070253\n",
      "Learning Rate 0.0064060814\n",
      "Learning Rate 0.0064051375\n",
      "Learning Rate 0.0064041936\n",
      "Learning Rate 0.0064032497\n",
      "Learning Rate 0.0064023063\n",
      "21792/25000 [=========================>....] - ETA: 0s - loss: 0.4922 - accuracy: 0.8360Learning Rate 0.006401363\n",
      "Learning Rate 0.0064004194\n",
      "Learning Rate 0.0063994764\n",
      "Learning Rate 0.0063985335\n",
      "Learning Rate 0.0063975905\n",
      "Learning Rate 0.006396648\n",
      "Learning Rate 0.0063957055\n",
      "Learning Rate 0.006394763\n",
      "Learning Rate 0.0063938205\n",
      "Learning Rate 0.0063928785\n",
      "22112/25000 [=========================>....] - ETA: 0s - loss: 0.4929 - accuracy: 0.8355Learning Rate 0.0063919364\n",
      "Learning Rate 0.0063909944\n",
      "Learning Rate 0.006390053\n",
      "Learning Rate 0.0063891113\n",
      "Learning Rate 0.0063881697\n",
      "Learning Rate 0.0063872286\n",
      "Learning Rate 0.0063862875\n",
      "Learning Rate 0.0063853464\n",
      "Learning Rate 0.0063844053\n",
      "22400/25000 [=========================>....] - ETA: 0s - loss: 0.4920 - accuracy: 0.8356Learning Rate 0.0063834647\n",
      "Learning Rate 0.006382524\n",
      "Learning Rate 0.0063815834\n",
      "Learning Rate 0.006380643\n",
      "Learning Rate 0.006379703\n",
      "Learning Rate 0.006378763\n",
      "Learning Rate 0.0063778227\n",
      "Learning Rate 0.006376883\n",
      "Learning Rate 0.0063759433\n",
      "22688/25000 [==========================>...] - ETA: 0s - loss: 0.4914 - accuracy: 0.8359Learning Rate 0.0063750036\n",
      "Learning Rate 0.0063740644\n",
      "Learning Rate 0.006373125\n",
      "Learning Rate 0.006372186\n",
      "Learning Rate 0.006371247\n",
      "Learning Rate 0.0063703083\n",
      "Learning Rate 0.0063693696\n",
      "Learning Rate 0.006368431\n",
      "Learning Rate 0.0063674925\n",
      "22976/25000 [==========================>...] - ETA: 0s - loss: 0.4896 - accuracy: 0.8365Learning Rate 0.006366554\n",
      "Learning Rate 0.006365616\n",
      "Learning Rate 0.006364678\n",
      "Learning Rate 0.00636374\n",
      "Learning Rate 0.0063628023\n",
      "Learning Rate 0.006361865\n",
      "Learning Rate 0.0063609276\n",
      "Learning Rate 0.00635999\n",
      "23232/25000 [==========================>...] - ETA: 0s - loss: 0.4874 - accuracy: 0.8374Learning Rate 0.006359053\n",
      "Learning Rate 0.006358116\n",
      "Learning Rate 0.006357179\n",
      "Learning Rate 0.006356242\n",
      "Learning Rate 0.0063553057\n",
      "Learning Rate 0.006354369\n",
      "Learning Rate 0.006353433\n",
      "Learning Rate 0.006352497\n",
      "Learning Rate 0.006351561\n",
      "23520/25000 [===========================>..] - ETA: 0s - loss: 0.4877 - accuracy: 0.8373Learning Rate 0.006350625\n",
      "Learning Rate 0.006349689\n",
      "Learning Rate 0.0063487533\n",
      "Learning Rate 0.006347818\n",
      "Learning Rate 0.0063468823\n",
      "Learning Rate 0.0063459473\n",
      "Learning Rate 0.006345012\n",
      "Learning Rate 0.006344077\n",
      "Learning Rate 0.006343142\n",
      "23808/25000 [===========================>..] - ETA: 0s - loss: 0.4868 - accuracy: 0.8375Learning Rate 0.0063422075\n",
      "Learning Rate 0.006341273\n",
      "Learning Rate 0.0063403384\n",
      "Learning Rate 0.0063394043\n",
      "Learning Rate 0.00633847\n",
      "Learning Rate 0.006337536\n",
      "Learning Rate 0.0063366024\n",
      "Learning Rate 0.0063356687\n",
      "Learning Rate 0.006334735\n",
      "24096/25000 [===========================>..] - ETA: 0s - loss: 0.4879 - accuracy: 0.8375Learning Rate 0.0063338014\n",
      "Learning Rate 0.0063328682\n",
      "Learning Rate 0.006331935\n",
      "Learning Rate 0.006331002\n",
      "Learning Rate 0.006330069\n",
      "Learning Rate 0.0063291364\n",
      "Learning Rate 0.0063282037\n",
      "Learning Rate 0.006327271\n",
      "Learning Rate 0.0063263387\n",
      "24384/25000 [============================>.] - ETA: 0s - loss: 0.4883 - accuracy: 0.8373Learning Rate 0.0063254065\n",
      "Learning Rate 0.0063244742\n",
      "Learning Rate 0.0063235424\n",
      "Learning Rate 0.0063226107\n",
      "Learning Rate 0.006321679\n",
      "Learning Rate 0.0063207475\n",
      "Learning Rate 0.006319816\n",
      "Learning Rate 0.006318885\n",
      "Learning Rate 0.0063179536\n",
      "24672/25000 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.8370Learning Rate 0.0063170227\n",
      "Learning Rate 0.006316092\n",
      "Learning Rate 0.006315161\n",
      "Learning Rate 0.0063142306\n",
      "Learning Rate 0.0063133\n",
      "Learning Rate 0.00631237\n",
      "Learning Rate 0.00631144\n",
      "Learning Rate 0.00631051\n",
      "Learning Rate 0.00630958\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.4931 - accuracy: 0.8366Learning Rate 0.00630865\n",
      "Learning Rate 0.0063077207\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.4925 - accuracy: 0.8368 - val_loss: 0.6461 - val_accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "Learning Rate 0.006306791\n",
      "   32/25000 [..............................] - ETA: 5s - loss: 0.9340 - accuracy: 0.7500Learning Rate 0.0063058618\n",
      "Learning Rate 0.0063049328\n",
      "Learning Rate 0.0063040038\n",
      "Learning Rate 0.006303075\n",
      "Learning Rate 0.006302146\n",
      "Learning Rate 0.0063012172\n",
      "Learning Rate 0.0063002887\n",
      "Learning Rate 0.00629936\n",
      "Learning Rate 0.006298432\n",
      "Learning Rate 0.006297504\n",
      "  352/25000 [..............................] - ETA: 4s - loss: 0.4974 - accuracy: 0.8381Learning Rate 0.006296576\n",
      "Learning Rate 0.0062956484\n",
      "Learning Rate 0.006294721\n",
      "Learning Rate 0.006293793\n",
      "Learning Rate 0.0062928656\n",
      "Learning Rate 0.0062919385\n",
      "Learning Rate 0.0062910113\n",
      "Learning Rate 0.006290084\n",
      "Learning Rate 0.0062891576\n",
      "Learning Rate 0.006288231\n",
      "  672/25000 [..............................] - ETA: 4s - loss: 0.4056 - accuracy: 0.8586Learning Rate 0.006287304\n",
      "Learning Rate 0.0062863776\n",
      "Learning Rate 0.0062854514\n",
      "Learning Rate 0.006284525\n",
      "Learning Rate 0.006283599\n",
      "Learning Rate 0.006282673\n",
      "Learning Rate 0.0062817475\n",
      "Learning Rate 0.0062808217\n",
      "Learning Rate 0.006279896\n",
      "  960/25000 [>.............................] - ETA: 4s - loss: 0.4055 - accuracy: 0.8615Learning Rate 0.0062789707\n",
      "Learning Rate 0.0062780455\n",
      "Learning Rate 0.00627712\n",
      "Learning Rate 0.0062761954\n",
      "Learning Rate 0.0062752706\n",
      "Learning Rate 0.006274346\n",
      "Learning Rate 0.0062734215\n",
      "Learning Rate 0.006272497\n",
      "Learning Rate 0.006271573\n",
      " 1248/25000 [>.............................] - ETA: 4s - loss: 0.3935 - accuracy: 0.8686Learning Rate 0.0062706484\n",
      "Learning Rate 0.0062697246\n",
      "Learning Rate 0.0062688007\n",
      "Learning Rate 0.006267877\n",
      "Learning Rate 0.0062669534\n",
      "Learning Rate 0.00626603\n",
      "Learning Rate 0.0062651066\n",
      "Learning Rate 0.006264183\n",
      "Learning Rate 0.0062632603\n",
      "Learning Rate 0.0062623373\n",
      " 1568/25000 [>.............................] - ETA: 4s - loss: 0.3916 - accuracy: 0.8661Learning Rate 0.0062614144\n",
      "Learning Rate 0.006260492\n",
      "Learning Rate 0.0062595694\n",
      "Learning Rate 0.006258647\n",
      "Learning Rate 0.006257725\n",
      "Learning Rate 0.006256803\n",
      "Learning Rate 0.006255881\n",
      "Learning Rate 0.006254959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0062540374\n",
      " 1856/25000 [=>............................] - ETA: 4s - loss: 0.4006 - accuracy: 0.8669Learning Rate 0.006253116\n",
      "Learning Rate 0.0062521943\n",
      "Learning Rate 0.006251273\n",
      "Learning Rate 0.006250352\n",
      "Learning Rate 0.006249431\n",
      "Learning Rate 0.00624851\n",
      "Learning Rate 0.0062475894\n",
      "Learning Rate 0.0062466688\n",
      "Learning Rate 0.006245748\n",
      " 2144/25000 [=>............................] - ETA: 3s - loss: 0.4002 - accuracy: 0.8657Learning Rate 0.006244828\n",
      "Learning Rate 0.006243908\n",
      "Learning Rate 0.0062429877\n",
      "Learning Rate 0.0062420676\n",
      "Learning Rate 0.006241148\n",
      "Learning Rate 0.006240228\n",
      "Learning Rate 0.0062393085\n",
      "Learning Rate 0.0062383893\n",
      "Learning Rate 0.00623747\n",
      " 2432/25000 [=>............................] - ETA: 3s - loss: 0.3955 - accuracy: 0.8697Learning Rate 0.006236551\n",
      "Learning Rate 0.0062356316\n",
      "Learning Rate 0.006234713\n",
      "Learning Rate 0.006233794\n",
      "Learning Rate 0.0062328754\n",
      "Learning Rate 0.006231957\n",
      "Learning Rate 0.006231039\n",
      "Learning Rate 0.0062301205\n",
      "Learning Rate 0.0062292027\n",
      "Learning Rate 0.006228285\n",
      " 2752/25000 [==>...........................] - ETA: 3s - loss: 0.3977 - accuracy: 0.8699Learning Rate 0.006227367\n",
      "Learning Rate 0.0062264493\n",
      "Learning Rate 0.006225532\n",
      "Learning Rate 0.0062246146\n",
      "Learning Rate 0.006223697\n",
      "Learning Rate 0.0062227803\n",
      "Learning Rate 0.0062218634\n",
      "Learning Rate 0.0062209466\n",
      "Learning Rate 0.0062200297\n",
      " 3040/25000 [==>...........................] - ETA: 3s - loss: 0.4122 - accuracy: 0.8668Learning Rate 0.0062191132\n",
      "Learning Rate 0.006218197\n",
      "Learning Rate 0.0062172804\n",
      "Learning Rate 0.0062163644\n",
      "Learning Rate 0.0062154485\n",
      "Learning Rate 0.0062145325\n",
      "Learning Rate 0.0062136166\n",
      "Learning Rate 0.006212701\n",
      "Learning Rate 0.0062117856\n",
      "Learning Rate 0.00621087\n",
      " 3360/25000 [===>..........................] - ETA: 3s - loss: 0.4198 - accuracy: 0.8646Learning Rate 0.006209955\n",
      "Learning Rate 0.00620904\n",
      "Learning Rate 0.006208125\n",
      "Learning Rate 0.00620721\n",
      "Learning Rate 0.0062062955\n",
      "Learning Rate 0.006205381\n",
      "Learning Rate 0.0062044663\n",
      "Learning Rate 0.0062035522\n",
      "Learning Rate 0.006202638\n",
      " 3648/25000 [===>..........................] - ETA: 3s - loss: 0.4207 - accuracy: 0.8649Learning Rate 0.006201724\n",
      "Learning Rate 0.0062008104\n",
      "Learning Rate 0.006199897\n",
      "Learning Rate 0.006198983\n",
      "Learning Rate 0.0061980695\n",
      "Learning Rate 0.0061971564\n",
      "Learning Rate 0.006196243\n",
      "Learning Rate 0.00619533\n",
      "Learning Rate 0.0061944174\n",
      " 3936/25000 [===>..........................] - ETA: 3s - loss: 0.4200 - accuracy: 0.8669Learning Rate 0.0061935047\n",
      "Learning Rate 0.006192592\n",
      "Learning Rate 0.0061916793\n",
      "Learning Rate 0.006190767\n",
      "Learning Rate 0.006189855\n",
      "Learning Rate 0.0061889426\n",
      "Learning Rate 0.006188031\n",
      "Learning Rate 0.006187119\n",
      "Learning Rate 0.0061862073\n",
      " 4224/25000 [====>.........................] - ETA: 3s - loss: 0.4242 - accuracy: 0.8651Learning Rate 0.0061852955\n",
      "Learning Rate 0.006184384\n",
      "Learning Rate 0.006183473\n",
      "Learning Rate 0.0061825616\n",
      "Learning Rate 0.006181651\n",
      "Learning Rate 0.00618074\n",
      "Learning Rate 0.006179829\n",
      "Learning Rate 0.0061789183\n",
      "Learning Rate 0.006178008\n",
      " 4512/25000 [====>.........................] - ETA: 3s - loss: 0.4259 - accuracy: 0.8659Learning Rate 0.0061770976\n",
      "Learning Rate 0.006176187\n",
      "Learning Rate 0.0061752773\n",
      "Learning Rate 0.0061743674\n",
      "Learning Rate 0.0061734575\n",
      "Learning Rate 0.0061725476\n",
      "Learning Rate 0.006171638\n",
      "Learning Rate 0.0061707287\n",
      "Learning Rate 0.0061698193\n",
      " 4800/25000 [====>.........................] - ETA: 3s - loss: 0.4258 - accuracy: 0.8660Learning Rate 0.0061689103\n",
      "Learning Rate 0.0061680013\n",
      "Learning Rate 0.0061670924\n",
      "Learning Rate 0.0061661834\n",
      "Learning Rate 0.006165275\n",
      "Learning Rate 0.0061643664\n",
      "Learning Rate 0.006163458\n",
      "Learning Rate 0.00616255\n",
      "Learning Rate 0.006161642\n",
      "Learning Rate 0.0061607338\n",
      "Learning Rate 0.0061598257\n",
      " 5152/25000 [=====>........................] - ETA: 3s - loss: 0.4228 - accuracy: 0.8670Learning Rate 0.006158918\n",
      "Learning Rate 0.0061580106\n",
      "Learning Rate 0.006157103\n",
      "Learning Rate 0.006156196\n",
      "Learning Rate 0.006155289\n",
      "Learning Rate 0.0061543817\n",
      "Learning Rate 0.0061534746\n",
      "Learning Rate 0.006152568\n",
      "Learning Rate 0.0061516613\n",
      "Learning Rate 0.0061507546\n",
      " 5472/25000 [=====>........................] - ETA: 3s - loss: 0.4228 - accuracy: 0.8668Learning Rate 0.0061498485\n",
      "Learning Rate 0.0061489423\n",
      "Learning Rate 0.006148036\n",
      "Learning Rate 0.0061471304\n",
      "Learning Rate 0.0061462247\n",
      "Learning Rate 0.006145319\n",
      "Learning Rate 0.0061444133\n",
      "Learning Rate 0.006143508\n",
      "Learning Rate 0.006142603\n",
      "Learning Rate 0.0061416975\n",
      " 5792/25000 [=====>........................] - ETA: 3s - loss: 0.4221 - accuracy: 0.8665Learning Rate 0.0061407927\n",
      "Learning Rate 0.006139888\n",
      "Learning Rate 0.006138983\n",
      "Learning Rate 0.0061380784\n",
      "Learning Rate 0.006137174\n",
      "Learning Rate 0.00613627\n",
      "Learning Rate 0.0061353655\n",
      "Learning Rate 0.0061344616\n",
      "Learning Rate 0.0061335578\n",
      " 6080/25000 [======>.......................] - ETA: 3s - loss: 0.4263 - accuracy: 0.8666Learning Rate 0.006132654\n",
      "Learning Rate 0.00613175\n",
      "Learning Rate 0.0061308467\n",
      "Learning Rate 0.0061299433\n",
      "Learning Rate 0.00612904\n",
      "Learning Rate 0.006128137\n",
      "Learning Rate 0.006127234\n",
      "Learning Rate 0.006126331\n",
      "Learning Rate 0.0061254282\n",
      "Learning Rate 0.006124526\n",
      " 6400/25000 [======>.......................] - ETA: 3s - loss: 0.4281 - accuracy: 0.8641Learning Rate 0.0061236233\n",
      "Learning Rate 0.006122721\n",
      "Learning Rate 0.006121819\n",
      "Learning Rate 0.006120917\n",
      "Learning Rate 0.006120015\n",
      "Learning Rate 0.006119113\n",
      "Learning Rate 0.0061182114\n",
      "Learning Rate 0.00611731\n",
      "Learning Rate 0.0061164084\n",
      " 6688/25000 [=======>......................] - ETA: 3s - loss: 0.4258 - accuracy: 0.8645Learning Rate 0.0061155073\n",
      "Learning Rate 0.0061146063\n",
      "Learning Rate 0.006113705\n",
      "Learning Rate 0.006112804\n",
      "Learning Rate 0.0061119036\n",
      "Learning Rate 0.006111003\n",
      "Learning Rate 0.0061101024\n",
      "Learning Rate 0.0061092023\n",
      "Learning Rate 0.006108302\n",
      "Learning Rate 0.006107402\n",
      " 7008/25000 [=======>......................] - ETA: 3s - loss: 0.4247 - accuracy: 0.8639Learning Rate 0.006106502\n",
      "Learning Rate 0.0061056023\n",
      "Learning Rate 0.0061047026\n",
      "Learning Rate 0.006103803\n",
      "Learning Rate 0.0061029037\n",
      "Learning Rate 0.0061020046\n",
      "Learning Rate 0.0061011054\n",
      "Learning Rate 0.006100206\n",
      "Learning Rate 0.0060993074\n",
      " 7296/25000 [=======>......................] - ETA: 3s - loss: 0.4254 - accuracy: 0.8629Learning Rate 0.0060984087\n",
      "Learning Rate 0.00609751\n",
      "Learning Rate 0.0060966113\n",
      "Learning Rate 0.006095713\n",
      "Learning Rate 0.0060948147\n",
      "Learning Rate 0.0060939165\n",
      "Learning Rate 0.0060930187\n",
      "Learning Rate 0.006092121\n",
      "Learning Rate 0.006091223\n",
      " 7584/25000 [========>.....................] - ETA: 3s - loss: 0.4271 - accuracy: 0.8629Learning Rate 0.0060903253\n",
      "Learning Rate 0.006089428\n",
      "Learning Rate 0.0060885306\n",
      "Learning Rate 0.0060876333\n",
      "Learning Rate 0.0060867365\n",
      "Learning Rate 0.0060858396\n",
      "Learning Rate 0.0060849427\n",
      "Learning Rate 0.006084046\n",
      "Learning Rate 0.0060831495\n",
      " 7872/25000 [========>.....................] - ETA: 2s - loss: 0.4285 - accuracy: 0.8615Learning Rate 0.006082253\n",
      "Learning Rate 0.0060813567\n",
      "Learning Rate 0.0060804607\n",
      "Learning Rate 0.006079565\n",
      "Learning Rate 0.006078669\n",
      "Learning Rate 0.006077773\n",
      "Learning Rate 0.0060768775\n",
      "Learning Rate 0.006075982\n",
      " 8128/25000 [========>.....................] - ETA: 2s - loss: 0.4336 - accuracy: 0.8600Learning Rate 0.0060750865\n",
      "Learning Rate 0.0060741915\n",
      "Learning Rate 0.0060732965\n",
      "Learning Rate 0.0060724015\n",
      "Learning Rate 0.0060715065\n",
      "Learning Rate 0.006070612\n",
      "Learning Rate 0.0060697175\n",
      "Learning Rate 0.006068823\n",
      " 8384/25000 [=========>....................] - ETA: 2s - loss: 0.4330 - accuracy: 0.8590Learning Rate 0.006067929\n",
      "Learning Rate 0.006067035\n",
      "Learning Rate 0.0060661407\n",
      "Learning Rate 0.0060652466\n",
      "Learning Rate 0.006064353\n",
      "Learning Rate 0.0060634594\n",
      "Learning Rate 0.006062566\n",
      "Learning Rate 0.0060616727\n",
      " 8640/25000 [=========>....................] - ETA: 2s - loss: 0.4333 - accuracy: 0.8588Learning Rate 0.0060607796\n",
      "Learning Rate 0.0060598864\n",
      "Learning Rate 0.0060589933\n",
      "Learning Rate 0.0060581006\n",
      "Learning Rate 0.006057208\n",
      "Learning Rate 0.0060563153\n",
      "Learning Rate 0.006055423\n",
      " 8864/25000 [=========>....................] - ETA: 2s - loss: 0.4321 - accuracy: 0.8595Learning Rate 0.006054531\n",
      "Learning Rate 0.0060536386\n",
      "Learning Rate 0.0060527464\n",
      "Learning Rate 0.0060518547\n",
      "Learning Rate 0.006050963\n",
      "Learning Rate 0.006050071\n",
      "Learning Rate 0.00604918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0060482887\n",
      " 9120/25000 [=========>....................] - ETA: 2s - loss: 0.4337 - accuracy: 0.8599Learning Rate 0.0060473974\n",
      "Learning Rate 0.006046506\n",
      "Learning Rate 0.0060456153\n",
      "Learning Rate 0.0060447245\n",
      "Learning Rate 0.0060438337\n",
      "Learning Rate 0.006042943\n",
      "Learning Rate 0.0060420525\n",
      "Learning Rate 0.006041162\n",
      " 9376/25000 [==========>...................] - ETA: 2s - loss: 0.4350 - accuracy: 0.8594Learning Rate 0.006040272\n",
      "Learning Rate 0.006039382\n",
      "Learning Rate 0.006038492\n",
      "Learning Rate 0.006037602\n",
      "Learning Rate 0.0060367123\n",
      "Learning Rate 0.006035823\n",
      "Learning Rate 0.0060349335\n",
      "Learning Rate 0.006034044\n",
      "Learning Rate 0.006033155\n",
      " 9664/25000 [==========>...................] - ETA: 2s - loss: 0.4328 - accuracy: 0.8598Learning Rate 0.006032266\n",
      "Learning Rate 0.0060313772\n",
      "Learning Rate 0.0060304883\n",
      "Learning Rate 0.0060296\n",
      "Learning Rate 0.0060287113\n",
      "Learning Rate 0.006027823\n",
      "Learning Rate 0.006026935\n",
      "Learning Rate 0.006026047\n",
      " 9920/25000 [==========>...................] - ETA: 2s - loss: 0.4340 - accuracy: 0.8597Learning Rate 0.006025159\n",
      "Learning Rate 0.006024271\n",
      "Learning Rate 0.0060233832\n",
      "Learning Rate 0.0060224957\n",
      "Learning Rate 0.006021608\n",
      "Learning Rate 0.006020721\n",
      "Learning Rate 0.006019834\n",
      "Learning Rate 0.006018947\n",
      "10176/25000 [===========>..................] - ETA: 2s - loss: 0.4343 - accuracy: 0.8589Learning Rate 0.00601806\n",
      "Learning Rate 0.006017173\n",
      "Learning Rate 0.0060162866\n",
      "Learning Rate 0.0060154\n",
      "Learning Rate 0.0060145133\n",
      "Learning Rate 0.006013627\n",
      "Learning Rate 0.006012741\n",
      "Learning Rate 0.006011855\n",
      "10432/25000 [===========>..................] - ETA: 2s - loss: 0.4342 - accuracy: 0.8588Learning Rate 0.006010969\n",
      "Learning Rate 0.0060100835\n",
      "Learning Rate 0.006009198\n",
      "Learning Rate 0.006008312\n",
      "Learning Rate 0.006007427\n",
      "Learning Rate 0.0060065417\n",
      "Learning Rate 0.0060056564\n",
      "Learning Rate 0.0060047717\n",
      "10688/25000 [===========>..................] - ETA: 2s - loss: 0.4355 - accuracy: 0.8583Learning Rate 0.006003887\n",
      "Learning Rate 0.006003002\n",
      "Learning Rate 0.0060021174\n",
      "Learning Rate 0.006001233\n",
      "Learning Rate 0.006000349\n",
      "Learning Rate 0.0059994645\n",
      "Learning Rate 0.0059985807\n",
      "10912/25000 [============>.................] - ETA: 2s - loss: 0.4338 - accuracy: 0.8580Learning Rate 0.005997697\n",
      "Learning Rate 0.005996813\n",
      "Learning Rate 0.0059959292\n",
      "Learning Rate 0.005995046\n",
      "Learning Rate 0.0059941625\n",
      "Learning Rate 0.005993279\n",
      "Learning Rate 0.005992396\n",
      "Learning Rate 0.005991513\n",
      "11168/25000 [============>.................] - ETA: 2s - loss: 0.4344 - accuracy: 0.8575Learning Rate 0.00599063\n",
      "Learning Rate 0.005989747\n",
      "Learning Rate 0.0059888647\n",
      "Learning Rate 0.0059879823\n",
      "Learning Rate 0.0059871\n",
      "Learning Rate 0.0059862174\n",
      "Learning Rate 0.0059853354\n",
      "Learning Rate 0.0059844535\n",
      "Learning Rate 0.0059835715\n",
      "11456/25000 [============>.................] - ETA: 2s - loss: 0.4339 - accuracy: 0.8577Learning Rate 0.00598269\n",
      "Learning Rate 0.0059818085\n",
      "Learning Rate 0.005980927\n",
      "Learning Rate 0.0059800455\n",
      "Learning Rate 0.0059791645\n",
      "Learning Rate 0.0059782835\n",
      "Learning Rate 0.0059774024\n",
      "Learning Rate 0.005976522\n",
      "11712/25000 [=============>................] - ETA: 2s - loss: 0.4333 - accuracy: 0.8578Learning Rate 0.0059756413\n",
      "Learning Rate 0.0059747607\n",
      "Learning Rate 0.00597388\n",
      "Learning Rate 0.005973\n",
      "Learning Rate 0.00597212\n",
      "Learning Rate 0.00597124\n",
      "Learning Rate 0.00597036\n",
      "Learning Rate 0.00596948\n",
      "Learning Rate 0.0059686005\n",
      "12000/25000 [=============>................] - ETA: 2s - loss: 0.4318 - accuracy: 0.8584Learning Rate 0.005967721\n",
      "Learning Rate 0.0059668417\n",
      "Learning Rate 0.0059659625\n",
      "Learning Rate 0.0059650834\n",
      "Learning Rate 0.005964204\n",
      "Learning Rate 0.0059633255\n",
      "Learning Rate 0.005962447\n",
      "Learning Rate 0.005961568\n",
      "Learning Rate 0.00596069\n",
      "12288/25000 [=============>................] - ETA: 2s - loss: 0.4297 - accuracy: 0.8590Learning Rate 0.0059598116\n",
      "Learning Rate 0.0059589334\n",
      "Learning Rate 0.005958055\n",
      "Learning Rate 0.0059571774\n",
      "Learning Rate 0.0059562996\n",
      "Learning Rate 0.005955422\n",
      "Learning Rate 0.005954544\n",
      "12512/25000 [==============>...............] - ETA: 2s - loss: 0.4289 - accuracy: 0.8593Learning Rate 0.0059536668\n",
      "Learning Rate 0.0059527894\n",
      "Learning Rate 0.005951912\n",
      "Learning Rate 0.0059510353\n",
      "Learning Rate 0.0059501585\n",
      "Learning Rate 0.0059492816\n",
      "Learning Rate 0.005948405\n",
      "Learning Rate 0.0059475284\n",
      "12768/25000 [==============>...............] - ETA: 2s - loss: 0.4278 - accuracy: 0.8594Learning Rate 0.005946652\n",
      "Learning Rate 0.0059457757\n",
      "Learning Rate 0.0059448997\n",
      "Learning Rate 0.005944024\n",
      "Learning Rate 0.005943148\n",
      "Learning Rate 0.005942272\n",
      "Learning Rate 0.0059413966\n",
      "Learning Rate 0.005940521\n",
      "Learning Rate 0.0059396457\n",
      "13056/25000 [==============>...............] - ETA: 2s - loss: 0.4264 - accuracy: 0.8599Learning Rate 0.0059387702\n",
      "Learning Rate 0.0059378953\n",
      "Learning Rate 0.0059370203\n",
      "Learning Rate 0.0059361453\n",
      "Learning Rate 0.005935271\n",
      "Learning Rate 0.0059343963\n",
      "Learning Rate 0.005933522\n",
      "13280/25000 [==============>...............] - ETA: 2s - loss: 0.4236 - accuracy: 0.8607Learning Rate 0.0059326473\n",
      "Learning Rate 0.005931773\n",
      "Learning Rate 0.005930899\n",
      "Learning Rate 0.005930025\n",
      "Learning Rate 0.005929151\n",
      "Learning Rate 0.0059282775\n",
      "Learning Rate 0.005927404\n",
      "13504/25000 [===============>..............] - ETA: 2s - loss: 0.4248 - accuracy: 0.8602Learning Rate 0.0059265303\n",
      "Learning Rate 0.005925657\n",
      "Learning Rate 0.005924784\n",
      "Learning Rate 0.005923911\n",
      "Learning Rate 0.005923038\n",
      "Learning Rate 0.0059221652\n",
      "Learning Rate 0.0059212926\n",
      "13728/25000 [===============>..............] - ETA: 2s - loss: 0.4247 - accuracy: 0.8601Learning Rate 0.00592042\n",
      "Learning Rate 0.0059195478\n",
      "Learning Rate 0.0059186756\n",
      "Learning Rate 0.0059178034\n",
      "Learning Rate 0.005916931\n",
      "13888/25000 [===============>..............] - ETA: 2s - loss: 0.4255 - accuracy: 0.8595Learning Rate 0.0059160595\n",
      "Learning Rate 0.0059151878\n",
      "Learning Rate 0.005914316\n",
      "Learning Rate 0.0059134443\n",
      "Learning Rate 0.005912573\n",
      "Learning Rate 0.005911702\n",
      "Learning Rate 0.0059108306\n",
      "14112/25000 [===============>..............] - ETA: 2s - loss: 0.4284 - accuracy: 0.8585Learning Rate 0.00590996\n",
      "Learning Rate 0.005909089\n",
      "Learning Rate 0.005908218\n",
      "Learning Rate 0.0059073474\n",
      "Learning Rate 0.005906477\n",
      "Learning Rate 0.005905607\n",
      "Learning Rate 0.0059047365\n",
      "Learning Rate 0.005903866\n",
      "Learning Rate 0.0059029963\n",
      "14400/25000 [================>.............] - ETA: 2s - loss: 0.4289 - accuracy: 0.8584Learning Rate 0.0059021264\n",
      "Learning Rate 0.0059012566\n",
      "Learning Rate 0.005900387\n",
      "Learning Rate 0.005899518\n",
      "Learning Rate 0.0058986484\n",
      "Learning Rate 0.005897779\n",
      "Learning Rate 0.00589691\n",
      "Learning Rate 0.005896041\n",
      "14656/25000 [================>.............] - ETA: 1s - loss: 0.4296 - accuracy: 0.8581Learning Rate 0.0058951722\n",
      "Learning Rate 0.005894304\n",
      "Learning Rate 0.0058934353\n",
      "Learning Rate 0.005892567\n",
      "Learning Rate 0.0058916984\n",
      "Learning Rate 0.0058908304\n",
      "Learning Rate 0.0058899624\n",
      "Learning Rate 0.0058890944\n",
      "Learning Rate 0.0058882264\n",
      "14944/25000 [================>.............] - ETA: 1s - loss: 0.4275 - accuracy: 0.8586Learning Rate 0.005887359\n",
      "Learning Rate 0.0058864914\n",
      "Learning Rate 0.005885624\n",
      "Learning Rate 0.005884757\n",
      "Learning Rate 0.0058838897\n",
      "Learning Rate 0.0058830227\n",
      "Learning Rate 0.0058821556\n",
      "Learning Rate 0.005881289\n",
      "15200/25000 [=================>............] - ETA: 1s - loss: 0.4266 - accuracy: 0.8593Learning Rate 0.0058804224\n",
      "Learning Rate 0.005879556\n",
      "Learning Rate 0.0058786892\n",
      "Learning Rate 0.005877823\n",
      "Learning Rate 0.005876957\n",
      "Learning Rate 0.005876091\n",
      "Learning Rate 0.005875225\n",
      "Learning Rate 0.0058743595\n",
      "Learning Rate 0.005873494\n",
      "15488/25000 [=================>............] - ETA: 1s - loss: 0.4253 - accuracy: 0.8592Learning Rate 0.005872628\n",
      "Learning Rate 0.005871763\n",
      "Learning Rate 0.005870898\n",
      "Learning Rate 0.0058700326\n",
      "Learning Rate 0.0058691674\n",
      "Learning Rate 0.0058683027\n",
      "Learning Rate 0.005867438\n",
      "Learning Rate 0.005866573\n",
      "15744/25000 [=================>............] - ETA: 1s - loss: 0.4258 - accuracy: 0.8592Learning Rate 0.005865709\n",
      "Learning Rate 0.0058648447\n",
      "Learning Rate 0.0058639804\n",
      "Learning Rate 0.005863116\n",
      "Learning Rate 0.0058622523\n",
      "Learning Rate 0.0058613885\n",
      "Learning Rate 0.0058605247\n",
      "Learning Rate 0.005859661\n",
      "Learning Rate 0.0058587976\n",
      "16032/25000 [==================>...........] - ETA: 1s - loss: 0.4254 - accuracy: 0.8592Learning Rate 0.0058579342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.005857071\n",
      "Learning Rate 0.005856208\n",
      "Learning Rate 0.005855345\n",
      "Learning Rate 0.0058544823\n",
      "Learning Rate 0.0058536194\n",
      "Learning Rate 0.005852757\n",
      "Learning Rate 0.0058518946\n",
      "Learning Rate 0.005851032\n",
      "16320/25000 [==================>...........] - ETA: 1s - loss: 0.4265 - accuracy: 0.8587Learning Rate 0.00585017\n",
      "Learning Rate 0.005849308\n",
      "Learning Rate 0.005848446\n",
      "Learning Rate 0.005847584\n",
      "Learning Rate 0.0058467225\n",
      "Learning Rate 0.005845861\n",
      "Learning Rate 0.0058449996\n",
      "Learning Rate 0.005844138\n",
      "Learning Rate 0.005843277\n",
      "Learning Rate 0.005842416\n",
      "16640/25000 [==================>...........] - ETA: 1s - loss: 0.4261 - accuracy: 0.8585Learning Rate 0.005841555\n",
      "Learning Rate 0.005840694\n",
      "Learning Rate 0.0058398335\n",
      "Learning Rate 0.005838973\n",
      "Learning Rate 0.0058381124\n",
      "Learning Rate 0.0058372524\n",
      "Learning Rate 0.0058363923\n",
      "Learning Rate 0.005835532\n",
      "Learning Rate 0.005834672\n",
      "16928/25000 [===================>..........] - ETA: 1s - loss: 0.4247 - accuracy: 0.8585Learning Rate 0.0058338125\n",
      "Learning Rate 0.005832953\n",
      "Learning Rate 0.0058320933\n",
      "Learning Rate 0.0058312337\n",
      "Learning Rate 0.0058303745\n",
      "Learning Rate 0.0058295154\n",
      "Learning Rate 0.0058286563\n",
      "Learning Rate 0.0058277976\n",
      "Learning Rate 0.005826939\n",
      "17216/25000 [===================>..........] - ETA: 1s - loss: 0.4240 - accuracy: 0.8586Learning Rate 0.00582608\n",
      "Learning Rate 0.0058252215\n",
      "Learning Rate 0.0058243633\n",
      "Learning Rate 0.005823505\n",
      "Learning Rate 0.005822647\n",
      "Learning Rate 0.0058217887\n",
      "Learning Rate 0.005820931\n",
      "Learning Rate 0.005820073\n",
      "17472/25000 [===================>..........] - ETA: 1s - loss: 0.4243 - accuracy: 0.8582Learning Rate 0.0058192154\n",
      "Learning Rate 0.005818358\n",
      "Learning Rate 0.005817501\n",
      "Learning Rate 0.0058166436\n",
      "Learning Rate 0.0058157863\n",
      "Learning Rate 0.0058149295\n",
      "Learning Rate 0.0058140727\n",
      "Learning Rate 0.005813216\n",
      "Learning Rate 0.005812359\n",
      "17760/25000 [====================>.........] - ETA: 1s - loss: 0.4239 - accuracy: 0.8581Learning Rate 0.0058115027\n",
      "Learning Rate 0.0058106463\n",
      "Learning Rate 0.00580979\n",
      "Learning Rate 0.005808934\n",
      "Learning Rate 0.005808078\n",
      "Learning Rate 0.0058072223\n",
      "Learning Rate 0.0058063664\n",
      "Learning Rate 0.005805511\n",
      "18016/25000 [====================>.........] - ETA: 1s - loss: 0.4246 - accuracy: 0.8581Learning Rate 0.0058046556\n",
      "Learning Rate 0.0058038\n",
      "Learning Rate 0.005802945\n",
      "Learning Rate 0.00580209\n",
      "Learning Rate 0.005801235\n",
      "Learning Rate 0.00580038\n",
      "Learning Rate 0.0057995254\n",
      "18240/25000 [====================>.........] - ETA: 1s - loss: 0.4247 - accuracy: 0.8578Learning Rate 0.005798671\n",
      "Learning Rate 0.0057978164\n",
      "Learning Rate 0.005796962\n",
      "Learning Rate 0.005796108\n",
      "Learning Rate 0.005795254\n",
      "Learning Rate 0.0057944\n",
      "Learning Rate 0.005793546\n",
      "Learning Rate 0.0057926923\n",
      "18496/25000 [=====================>........] - ETA: 1s - loss: 0.4260 - accuracy: 0.8574Learning Rate 0.0057918387\n",
      "Learning Rate 0.005790985\n",
      "Learning Rate 0.0057901316\n",
      "Learning Rate 0.0057892785\n",
      "Learning Rate 0.0057884254\n",
      "Learning Rate 0.0057875724\n",
      "Learning Rate 0.0057867197\n",
      "Learning Rate 0.005785867\n",
      "18752/25000 [=====================>........] - ETA: 1s - loss: 0.4263 - accuracy: 0.8573Learning Rate 0.0057850145\n",
      "Learning Rate 0.005784162\n",
      "Learning Rate 0.0057833097\n",
      "Learning Rate 0.0057824575\n",
      "Learning Rate 0.0057816054\n",
      "Learning Rate 0.005780753\n",
      "Learning Rate 0.0057799015\n",
      "Learning Rate 0.00577905\n",
      "Learning Rate 0.005778198\n",
      "19040/25000 [=====================>........] - ETA: 1s - loss: 0.4291 - accuracy: 0.8576Learning Rate 0.005777347\n",
      "Learning Rate 0.0057764957\n",
      "Learning Rate 0.0057756444\n",
      "Learning Rate 0.005774793\n",
      "Learning Rate 0.0057739425\n",
      "Learning Rate 0.0057730917\n",
      "19232/25000 [======================>.......] - ETA: 1s - loss: 0.4295 - accuracy: 0.8577Learning Rate 0.005772241\n",
      "Learning Rate 0.00577139\n",
      "Learning Rate 0.00577054\n",
      "Learning Rate 0.0057696896\n",
      "Learning Rate 0.0057688393\n",
      "Learning Rate 0.005767989\n",
      "19424/25000 [======================>.......] - ETA: 1s - loss: 0.4295 - accuracy: 0.8578Learning Rate 0.005767139\n",
      "Learning Rate 0.0057662893\n",
      "Learning Rate 0.0057654395\n",
      "Learning Rate 0.00576459\n",
      "Learning Rate 0.0057637407\n",
      "Learning Rate 0.0057628914\n",
      "19616/25000 [======================>.......] - ETA: 1s - loss: 0.4310 - accuracy: 0.8571Learning Rate 0.005762042\n",
      "Learning Rate 0.005761193\n",
      "Learning Rate 0.005760344\n",
      "Learning Rate 0.0057594953\n",
      "Learning Rate 0.0057586464\n",
      "Learning Rate 0.005757798\n",
      "19808/25000 [======================>.......] - ETA: 1s - loss: 0.4300 - accuracy: 0.8573Learning Rate 0.0057569495\n",
      "Learning Rate 0.005756101\n",
      "Learning Rate 0.005755253\n",
      "Learning Rate 0.005754405\n",
      "Learning Rate 0.005753557\n",
      "Learning Rate 0.005752709\n",
      "20000/25000 [=======================>......] - ETA: 0s - loss: 0.4296 - accuracy: 0.8570Learning Rate 0.0057518617\n",
      "Learning Rate 0.005751014\n",
      "Learning Rate 0.0057501667\n",
      "Learning Rate 0.005749319\n",
      "Learning Rate 0.005748472\n",
      "Learning Rate 0.005747625\n",
      "Learning Rate 0.005746778\n",
      "Learning Rate 0.005745931\n",
      "20256/25000 [=======================>......] - ETA: 0s - loss: 0.4281 - accuracy: 0.8572Learning Rate 0.0057450845\n",
      "Learning Rate 0.005744238\n",
      "Learning Rate 0.0057433913\n",
      "Learning Rate 0.0057425452\n",
      "Learning Rate 0.005741699\n",
      "Learning Rate 0.005740853\n",
      "Learning Rate 0.005740007\n",
      "Learning Rate 0.0057391613\n",
      "20512/25000 [=======================>......] - ETA: 0s - loss: 0.4282 - accuracy: 0.8570Learning Rate 0.0057383156\n",
      "Learning Rate 0.00573747\n",
      "Learning Rate 0.0057366244\n",
      "Learning Rate 0.005735779\n",
      "Learning Rate 0.005734934\n",
      "Learning Rate 0.005734089\n",
      "Learning Rate 0.0057332437\n",
      "Learning Rate 0.005732399\n",
      "20768/25000 [=======================>......] - ETA: 0s - loss: 0.4283 - accuracy: 0.8570Learning Rate 0.0057315542\n",
      "Learning Rate 0.0057307095\n",
      "Learning Rate 0.0057298653\n",
      "Learning Rate 0.005729021\n",
      "Learning Rate 0.005728177\n",
      "Learning Rate 0.0057273326\n",
      "Learning Rate 0.005726489\n",
      "Learning Rate 0.005725645\n",
      "21024/25000 [========================>.....] - ETA: 0s - loss: 0.4284 - accuracy: 0.8571Learning Rate 0.005724801\n",
      "Learning Rate 0.0057239574\n",
      "Learning Rate 0.005723114\n",
      "Learning Rate 0.005722271\n",
      "Learning Rate 0.0057214275\n",
      "Learning Rate 0.0057205847\n",
      "Learning Rate 0.005719742\n",
      "21248/25000 [========================>.....] - ETA: 0s - loss: 0.4285 - accuracy: 0.8572Learning Rate 0.005718899\n",
      "Learning Rate 0.005718056\n",
      "Learning Rate 0.0057172137\n",
      "Learning Rate 0.0057163713\n",
      "Learning Rate 0.005715529\n",
      "Learning Rate 0.0057146866\n",
      "Learning Rate 0.0057138447\n",
      "Learning Rate 0.0057130028\n",
      "21504/25000 [========================>.....] - ETA: 0s - loss: 0.4305 - accuracy: 0.8563Learning Rate 0.005712161\n",
      "Learning Rate 0.005711319\n",
      "Learning Rate 0.0057104775\n",
      "Learning Rate 0.005709636\n",
      "Learning Rate 0.0057087946\n",
      "Learning Rate 0.0057079536\n",
      "Learning Rate 0.0057071126\n",
      "Learning Rate 0.0057062716\n",
      "Learning Rate 0.0057054306\n",
      "21792/25000 [=========================>....] - ETA: 0s - loss: 0.4301 - accuracy: 0.8563Learning Rate 0.00570459\n",
      "Learning Rate 0.0057037496\n",
      "Learning Rate 0.005702909\n",
      "Learning Rate 0.0057020686\n",
      "Learning Rate 0.0057012285\n",
      "Learning Rate 0.0057003885\n",
      "Learning Rate 0.0056995484\n",
      "Learning Rate 0.0056987084\n",
      "Learning Rate 0.0056978688\n",
      "22080/25000 [=========================>....] - ETA: 0s - loss: 0.4290 - accuracy: 0.8566Learning Rate 0.005697029\n",
      "Learning Rate 0.0056961896\n",
      "Learning Rate 0.00569535\n",
      "Learning Rate 0.005694511\n",
      "Learning Rate 0.0056936718\n",
      "Learning Rate 0.0056928326\n",
      "Learning Rate 0.005691994\n",
      "Learning Rate 0.0056911553\n",
      "Learning Rate 0.0056903167\n",
      "22368/25000 [=========================>....] - ETA: 0s - loss: 0.4295 - accuracy: 0.8566Learning Rate 0.005689478\n",
      "Learning Rate 0.00568864\n",
      "Learning Rate 0.0056878016\n",
      "Learning Rate 0.0056869634\n",
      "Learning Rate 0.0056861253\n",
      "Learning Rate 0.0056852875\n",
      "Learning Rate 0.00568445\n",
      "Learning Rate 0.005683612\n",
      "Learning Rate 0.0056827744\n",
      "22656/25000 [==========================>...] - ETA: 0s - loss: 0.4298 - accuracy: 0.8565Learning Rate 0.005681937\n",
      "Learning Rate 0.0056811\n",
      "Learning Rate 0.0056802626\n",
      "Learning Rate 0.005679426\n",
      "Learning Rate 0.005678589\n",
      "Learning Rate 0.005677752\n",
      "Learning Rate 0.0056769154\n",
      "22880/25000 [==========================>...] - ETA: 0s - loss: 0.4295 - accuracy: 0.8566Learning Rate 0.005676079\n",
      "Learning Rate 0.0056752427\n",
      "Learning Rate 0.0056744064\n",
      "Learning Rate 0.00567357\n",
      "Learning Rate 0.005672734\n",
      "Learning Rate 0.0056718984\n",
      "Learning Rate 0.0056710625\n",
      "Learning Rate 0.0056702266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23136/25000 [==========================>...] - ETA: 0s - loss: 0.4281 - accuracy: 0.8569Learning Rate 0.0056693912\n",
      "Learning Rate 0.005668556\n",
      "Learning Rate 0.0056677205\n",
      "Learning Rate 0.005666885\n",
      "Learning Rate 0.00566605\n",
      "Learning Rate 0.005665215\n",
      "Learning Rate 0.0056643803\n",
      "Learning Rate 0.005663546\n",
      "23392/25000 [===========================>..] - ETA: 0s - loss: 0.4273 - accuracy: 0.8574Learning Rate 0.0056627113\n",
      "Learning Rate 0.005661877\n",
      "Learning Rate 0.0056610424\n",
      "Learning Rate 0.0056602084\n",
      "Learning Rate 0.0056593744\n",
      "Learning Rate 0.0056585404\n",
      "23584/25000 [===========================>..] - ETA: 0s - loss: 0.4275 - accuracy: 0.8575Learning Rate 0.0056577064\n",
      "Learning Rate 0.005656873\n",
      "Learning Rate 0.0056560393\n",
      "Learning Rate 0.005655206\n",
      "Learning Rate 0.0056543723\n",
      "Learning Rate 0.005653539\n",
      "Learning Rate 0.005652706\n",
      "Learning Rate 0.005651873\n",
      "23840/25000 [===========================>..] - ETA: 0s - loss: 0.4264 - accuracy: 0.8578Learning Rate 0.0056510405\n",
      "Learning Rate 0.005650208\n",
      "Learning Rate 0.0056493753\n",
      "Learning Rate 0.0056485427\n",
      "Learning Rate 0.0056477105\n",
      "Learning Rate 0.0056468784\n",
      "Learning Rate 0.0056460463\n",
      "Learning Rate 0.005645214\n",
      "24096/25000 [===========================>..] - ETA: 0s - loss: 0.4253 - accuracy: 0.8583Learning Rate 0.0056443824\n",
      "Learning Rate 0.005643551\n",
      "Learning Rate 0.005642719\n",
      "Learning Rate 0.0056418874\n",
      "Learning Rate 0.005641056\n",
      "Learning Rate 0.005640225\n",
      "Learning Rate 0.005639394\n",
      "Learning Rate 0.0056385626\n",
      "24352/25000 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8580Learning Rate 0.005637732\n",
      "Learning Rate 0.005636901\n",
      "Learning Rate 0.0056360704\n",
      "Learning Rate 0.00563524\n",
      "Learning Rate 0.00563441\n",
      "Learning Rate 0.0056335796\n",
      "Learning Rate 0.0056327493\n",
      "Learning Rate 0.0056319195\n",
      "24608/25000 [============================>.] - ETA: 0s - loss: 0.4278 - accuracy: 0.8574Learning Rate 0.0056310897\n",
      "Learning Rate 0.00563026\n",
      "Learning Rate 0.00562943\n",
      "Learning Rate 0.0056286007\n",
      "Learning Rate 0.0056277714\n",
      "Learning Rate 0.005626942\n",
      "Learning Rate 0.0056261127\n",
      "Learning Rate 0.005625284\n",
      "24864/25000 [============================>.] - ETA: 0s - loss: 0.4282 - accuracy: 0.8570Learning Rate 0.005624455\n",
      "Learning Rate 0.005623626\n",
      "Learning Rate 0.005622797\n",
      "Learning Rate 0.005621969\n",
      "Learning Rate 0.0056211404\n",
      "25000/25000 [==============================] - 5s 213us/sample - loss: 0.4282 - accuracy: 0.8567 - val_loss: 0.6166 - val_accuracy: 0.8118\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "        #print (\"Learning Rate\", lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "# defining the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(lr=lr0) # using Nadam optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]) # compiling the model\n",
    "n_epochs = 5\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the learning rate by epoch for exponential scheduling\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise Constant Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the epoch for piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining piecewise constant scheduling\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "# calling the function\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 173us/sample - loss: 0.9096 - accuracy: 0.7474 - val_loss: 1.4445 - val_accuracy: 0.6518\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 5s 188us/sample - loss: 0.8224 - accuracy: 0.7619 - val_loss: 1.8735 - val_accuracy: 0.6500\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 4s 141us/sample - loss: 0.8986 - accuracy: 0.7555 - val_loss: 1.7711 - val_accuracy: 0.6578\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 4s 141us/sample - loss: 0.9058 - accuracy: 0.7348 - val_loss: 0.9819 - val_accuracy: 0.7138\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 0.8147 - accuracy: 0.7630 - val_loss: 1.2003 - val_accuracy: 0.5984\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 138us/sample - loss: 0.6695 - accuracy: 0.7494 - val_loss: 0.7413 - val_accuracy: 0.7258\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 4s 145us/sample - loss: 0.5868 - accuracy: 0.7905 - val_loss: 0.7162 - val_accuracy: 0.7892\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 4s 140us/sample - loss: 0.5224 - accuracy: 0.8425 - val_loss: 0.6552 - val_accuracy: 0.8362\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 140us/sample - loss: 0.4771 - accuracy: 0.8581 - val_loss: 0.6140 - val_accuracy: 0.8428\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 4s 140us/sample - loss: 0.4490 - accuracy: 0.8608 - val_loss: 0.8387 - val_accuracy: 0.7308\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 4s 143us/sample - loss: 0.4320 - accuracy: 0.8710 - val_loss: 0.5414 - val_accuracy: 0.8598\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 4s 147us/sample - loss: 0.4140 - accuracy: 0.8729 - val_loss: 0.8509 - val_accuracy: 0.7864\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.4176 - accuracy: 0.8750 - val_loss: 0.7106 - val_accuracy: 0.8314\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 0.3989 - accuracy: 0.8794 - val_loss: 0.6810 - val_accuracy: 0.8518\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 0.3866 - accuracy: 0.8823 - val_loss: 1.1197 - val_accuracy: 0.8328\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 138us/sample - loss: 0.2835 - accuracy: 0.9102 - val_loss: 0.5432 - val_accuracy: 0.8682\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 4s 140us/sample - loss: 0.2350 - accuracy: 0.9206 - val_loss: 0.5464 - val_accuracy: 0.8748\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 0.2209 - accuracy: 0.9250 - val_loss: 0.5401 - val_accuracy: 0.8794\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.2118 - accuracy: 0.9276 - val_loss: 0.5420 - val_accuracy: 0.8712\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 4s 140us/sample - loss: 0.2019 - accuracy: 0.9329 - val_loss: 0.5738 - val_accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the LearningRateScheduler\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "# defining the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3hc1Xnv8e9PtnyVhIwxljHY3CVqgi/kTggKhJCm5eCEnJ5TaIDSBE4oJ21JoJCGhpCmxBAaSEMBNxcg0JyQYG4hgYYaBciFAHaxcYwNAQwYbIONL/Ld1nv+2FtmPMxIW7b2jGT9Ps8zj2b2XrPm3VuXV3uttddSRGBmZtbbaqodgJmZ7ZmcYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY1YBks6S1N7D97RJ+nZeMaWf8aKkL+RQ7ycl9egeiOJztCvnzPoWJxjLlaSbJEWJx2+rHVte0uP7ZNHmHwEH5/BZn5Y0V1K7pDWS5kn6p97+nCrJ5ZxZ5QyudgA2IDwIfKpo25ZqBFItEbER2NibdUo6G/gW8HfAfwG1wJHA+3rzc6olj3NmleUrGKuEzRGxrOixCkDScZK2SmrtLCzpXElrJR2cvm6TdIOkayW9mT6uklRT8J5Rkm5O922U9KCkSQX7z0r/yz9B0tOS1kt6SNJBhYFKOlnSk5I2SXpB0tckDSnY/6KkL0m6MY3xFUkXFu5Pn/44vZJ5sfDzC8odIuluScvSWOZI+tMentf/AcyKiBsj4rmIWBgRP46IC4qO6WOSHkvPy0pJ90oaVlBkWLnjSd+/l6SZklZIWifpl5LeWVTmDElLJG2Q9FNgbNH+yyQ9XbStyyawEufssvR7978l/SGN5S5J+xSUGSzpmwU/J9+UdL2ktu5Pp/U2Jxirqoj4JXAV8IM0SbQA/wL834h4vqDo6SQ/r+8DzgXOAf62YP9NwHuAU4B3AxuA+yUNLygzFLgEODutpxG4oXOnpJOA24BvA5PScp8E/rko7L8D5gPTgBnAlZI6rxrelX79DDCu4HWxOuDnwInAZOAOYFZ6/FktA97dmYhLkfRR4B7gF8DRwIeAX7Lz737Z45Ek4D5gPPCnwFTgYWC2pHFpmfeQnP+ZwBTgXuDyHhxHTxwI/C/g48BH0ni+VrD/C8BZwKeB95Ic52k5xWLdiQg//MjtQfKHZxvQXvSYUVCmFngcmAXMAX5UVEcbsBhQwbYvAa+kzw8DAvhgwf69gDXAp9PXZ6VlmgvKnA5s7qyX5A/npUWfPT2Nt7PMi8APi8o8C3yp4HUAnywqcxbQ3s25+m1RPW3At7soPw74Tfp5zwK3AmcAtQVlfgX8vy7q6PJ4gOPT4x9eVOa/gYvS5/8B/KJo/3eSPy87Xl8GPN3VOcnw+jJgE7BXwbZ/AJ4reP0acHHBawGLgLZq/y4MxIevYKwSHib5z7bwcVXnzojYSvJf5p8C+5JcoRT7baR/MVK/AcZLagCOADrSbZ11riH5r/yPCt6zOSIWFbx+FRgCjEpfHw38Q9qU1p42z/wHMBJoKnjfvKLYXk3jzkzSSElXSvp92pTTDrwTmJC1joh4LSLeB7wDuIbkj+mNwO8kjUiLTSXpn+lKV8dzNDACeL3ovBwJHJKWOYKCc58qft1blqTf27fFKmkvku/T7zp3pj8zv8Oqwp38VgkbIuK5bsp0Nmc0AmOA1b302YVJaVuZfTUFX78C/LhEPa8XPN9aop6e/rP2DeCjJE06z5I06d1CkvB6JCKeBp4GrpP0AeAR4M9Irh6z6Op4aoDlwLEl3re2B2F2kCTAQrU9eH+n3jj3ViH+xljVpR3t3wb+mqSv4FZJxf/8vCftD+j0XuDViFgLLOSt/pnOOhtI/rP/fQ9CmQO0RNJhXvwoTk5d2QoM6qbMB4BbIuKOiJgHvMJbVwS7o/N469Kvc4ETdqO+OSQd9h0lzsmKtMxCku9HoeLXrwNji76HU3YjrrdJr2yWUdDvlX5euX4wy5mvYKwShkpqKtq2PSJelzQI+AHwy4i4UdJPSJq2vgxcWlB+P+AaSf9GkjguBP4JICKelXQ3cKOkc0iufr5G8h/2f/QgzsuBn0paAtxOcsVzJPDuiLioB/W8CJwg6ZckzXJvliizGPh4GvdWkuMdVqJcWZKuJ2kimk2SoMaR9E1tAP4zLfY14F5Jz5GcC5F0jt8YERsyfMyDJP04d0u6CHiGpBnqo8CDEfEIyVDpX0u6BPgJ0ErSCV+oDdgb+KKk/5eWKb5XqDdcC1wkaTFJsj2X5Ly8lsNnWTd8BWOV8GGSX/DCx9x03xeBQ4G/AoiIlcCZwMVpc0+n20iuCh4D/h34LvDNgv1/SdLWfk/6dQTw0UjupcgkIh4A/oRkpNXv0sfFwEvZDxWAz6d1vMxbx1nsAmAFSXPWz0k6+B/p4ef8gmTk3O0kCevOdPuJEbEYICJ+RvLH/o/TWH6ZxtaR5QPSPoyPkSSxfyfpML8daCZJbkTEb0m+f58l6c/5BEmHfGE9C9P956RlTuTto/N6wzdI/mH5Psk5heS8bMrhs6wbnSNjzPqs9B6GpyPi/GrHYv2PpLnAoxHxf6sdy0DjJjIz22NImgicRHKlVktyP9JR6VerMCcYM9uTdJDcC3QVSRfA74E/jognqhrVAOUmMjMzy4U7+c3MLBduIks1NjbGoYceWu0wMlu/fj0jR46sdhiZOd58Od789beYKxXvk08++UZEjCm1zwkmNXbsWJ54ov8007a1tdHa2lrtMDJzvPlyvPnrbzFXKt70vrGS3ERmZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5aKiCUbS3pLulLRe0hJJp5UpJ0kzJK1MHzMkqWD/TEmLJHVIOqvE+/9O0jJJayV9T9LQ7mJ7cW0Hx3x9NnfNXbpLx3bX3KUc8/XZHHTxfbtcT2/U0RfrMbOBqdJXMNcBW4CxwOnA9ZImlSh3DjAdmAwcBZwMnFuw/yngPGBO8RslnQRcDJwATAQOBr6SJbilqzdyyaz5Pf5DetfcpVwyaz5LV28kdrGe3qijL9ZjZgNXxVa0lDQSOBU4MiLagUcl3QN8iiQhFDoTuDoiXknfezXwGeAGgIi4Lt2+qcRHnQl8NyIWpGW+CtxW4jNK2rh1O1+8cz6PPvdG5mP72fzX2Lh1+27V09M6li3bzE9ffyqXWLqq56oHFjF96vjM9ZjZwKWIqMwHSVOBX0XEiIJtXwCOi4iTi8quAT4SEY+lr98JPBQR9UXlHgW+ExE3FWx7CvjniPhR+nof4HVgn4hYWfT+c0iulhjSdOjR4868Zse+0cNEVis3lT+HWevpaR0d0UGN3n4B2huxdFfPTR/t+Trf7e3t1NXV9fh91eJ489Xf4oX+F3Ol4v3Qhz70ZES8s9S+il3BAHXA2qJta4D6MmXXFJWrk6ToPiOWei/p5+yUYCJiJjATYOi4w3bUO75xOL+6+PhuPuYtx3x9NktXb3zb9p7U09M6yq233RuxdFfPrqzz7fXM8+V489ffYu4L8VayD6YdaCja1gCsy1C2AWjPkFzKvZcyn/M2w2sHceFJzVmK7nDhSc0Mrx20W/X0Rh19sR4zG7gqmWAWA4MlHVawbTKwoETZBem+7sqVUuq9y4ubx0oZ3zicKz7xjh73MUyfOp4rPvEOxjcOR7tYT2/UkUc9DcOSi9xxew3bpXrMbOCqWBNZRKyXNAu4XNKngSnAKcD7SxS/BbhA0s+AAD4P/GvnTklDSJKjgFpJw4AtEdGRvvcmSbcBrwJfAm7qLr4DG2p61IRUbPrU8bv9x7c36ujteuqGDubTtzzBt0+bxtETR+12nWY2cFR6mPJ5wHBgBfBD4LMRsUDSsZLaC8rdCNwLzAeeBu5Lt3X6T2AjSXKamT7/IEBE3A9cCTwEvAQsAb6c4zHt0Zqbki6yZ5YVd5+ZmXWtkp38RMQqkvtbirc/QtI53/k6gIvSR6l6Wrv5nH8B/mV3YrXE/qOGUzd0MM+8lqkLy8xsB08VY12SREtTPYuWOcGYWc84wVi3mpvqWbhsLZW6Z8rM9gxOMNatlnENrNu0jdfWlJo4wcysNCcY61aLO/rNbBc4wVi3OkeSLXRHv5n1gBOMdathWC3jG4e7o9/MesQJxjJpaap3E5mZ9YgTjGXSMq6eP7y+ns3btndf2MwMJxjLqKWpge0dwR9WrK92KGbWTzjBWCYeSWZmPeUEY5kctM9Ihgyq4Rl39JtZRk4wlsngQTUcNrbOCcbMMnOCscyam+p55jU3kZlZNk4wltkRTQ2sWLeZVeu3VDsUM+sHnGAss5Zx7ug3s+ycYCyzHYuPecoYM8vACcYyG1M3lNEjh3jKGDPLxAnGMpNEyzhPGWNm2TjBWI80j21g0fJ1bO/w4mNm1jUnGOuRlnH1bNrawUurNlQ7FDPr45xgrEd2TBnj+2HMrBtOMNYjh+1bT41goTv6zawbTjDWI8OHDOLAfUayyB39ZtYNJxjrsWTxMV/BmFnXnGCsx1qaGliycgPrN2+rdihm1oc5wViPdXb0L17uqxgzK88JxnqspakBwM1kZtYlJxjrsf1HDWfkkEEeqmxmXXKCsR6rqVGyNoyvYMysC04wtkuamxp4Ztk6IjxljJmVVtEEI2lvSXdKWi9piaTTypSTpBmSVqaPGZJUsH+KpCclbUi/TinYN1TSDZKWS1ol6V5J4ytxfAPJEePqWbNxK8vXbq52KGbWR1X6CuY6YAswFjgduF7SpBLlzgGmA5OBo4CTgXMBJA0B7gZuBUYBNwN3p9sB/gZ4X/q+/YA3gX/N6XgGrM6O/oW+4dLMyqhYgpE0EjgVuDQi2iPiUeAe4FMlip8JXB0Rr0TEUuBq4Kx0XyswGLgmIjZHxLcAAcen+w8CHoiI5RGxCfgRUCqJ2W5oHuvFx8ysa4Mr+FmHA9siYnHBtqeA40qUnZTuKyw3qWDfvNi58X9euv1+4LvAtZL2A1aTXCn9vFRAks4huVpizJgxtLW19fCQqqe9vb3q8e49TDz81LMcwcvdlu0L8faE481Xf4sX+l/MfSHeSiaYOqC4PWUNUF+m7JqicnVpP0zxvuJ6ngVeBpYC24H5wPmlAoqImcBMgObm5mhtbc14KNXX1tZGteOd/MLveG3NJlpbP9ht2b4Qb0843nz1t3ih/8XcF+LN3EQmaaykL0i6XtI+6bZjJB2UsYp2oKFoWwNQqo2luGwD0J5etXRXz3XAUGA0MBKYRZkrGNs9LeMaeG5FO1u2dVQ7FDPrgzIlGElHA4tImpv+irf+wJ8IfC3jZy0GBks6rGDbZGBBibIL0n2lyi0AjiocVUbSod+5fwpwU0SsiojNJB387+5MitZ7Wprq2dYRPP9Ge7VDMbM+KOsVzDeAayNiKlA4LvUB4JgsFUTEepKricsljZR0DHAK8IMSxW8BLpA0Pu1L+TxwU7qvjaTp63PpkOTO5q/Z6dfHgTMk7SWpFjgPeDUi3sh2qJbVjilj3NFvZiVkTTBHkwwHLvYayZDjrM4DhgMrgB8Cn42IBZKOlVT4b/CNwL0k/SdPA/el24iILSRDmM8g6cQ/G5iebgf4ArCJpC/mdeBjwMd7EKNldPCYkdQOkocqm1lJWTv5N5Lcc1KshSRZZBIRq0iSQ/H2R0g67ztfB3BR+ihVz1ySpFdq30qSpjzLWe2gGg7dt55FnjLGzErIegVzN/BlSUPT1yHpQGAGcEcOcVk/0dJU7yYyMyspa4L5ArA3SZPTCOBR4DmSJqov5ROa9QctTfUsW7uJ1Ru2dF/YzAaUTE1kEbEW+ICk44FpJIlpTkQ8mGdw1ve1jHtrbZj3Hjy6ytGYWV+SdZjyGZKGRsTsiPhGRFwZEQ9KGiLpjLyDtL6rc3VLrw1jZsWyNpF9H9irxPb6dJ8NUPvWD2XUiFoWeflkMyuSNcEIKLXwxwTePm2LDSCSaGlqYKE7+s2sSJd9MJLmkySWAH4paVvB7kHAROBn+YVn/UFzUz23P/EyHR1BTY26f4OZDQjddfL/JP16JMnNjoU3Q24BXsTDlAe8I8bVs2HLdl5+cwMTR4+sdjhm1kd0mWAi4isAkl4EfpSur2K2k+bOxcdeW+cEY2Y7ZOqDiYibnVysnMPH1iHBM54yxswKZB2mPETSVyQtlrRJ0vbCR95BWt82YshgDhw90lPGmNlOso4i+yrpMsZAB3AhyborK0kmsLQBrnlsPc84wZhZgawJ5s+A/xMRN5JMlX93RHwO+DLJmjA2wLWMq+fFlevZsGVb94XNbEDImmDGAr9Pn7cDjenz+4GP9HZQ1v+0NDUQAc8u9+JjZpbImmBeAvZLnz8HnJQ+fx/JVP42wO2YMsYd/WaWyppg7gROSJ9fC3xF0gskq0x+J4e4rJ+ZsPcIhtcO8h39ZrZD1tmULyl4/hNJL5Mslbw4In6aV3DWf9TUiOYmLz5mZm/JuqLlTiLiMeAxAEkjI2J9r0Zl/VJLUz0PLFhGRCB5yhizgS5rE9nbSBom6ULghV6Mx/qxlqZ63tywldfXba52KGbWB3SZYNIbLL8m6XFJv5Y0Pd1+BvA88LfANysQp/UDnYuPLXQzmZnR/RXMZcD5wBLgIODHkv4N+AfgEuDAiLgi1wit3/DiY2ZWqLs+mD8DzoqIOyVNBuYCo4BJEeE76mwnjSOG0NQwzB39ZgZ0fwVzAPA4QEQ8RTJF/wwnFyunZVy9m8jMDOg+wdQChT22W/EKltaF5qZ6nluxjq3bO6odiplVWZZhyldI2pA+HwJcJmmnJJPOS2bGEU0NbN0evPDGeg4fW1/tcMysirpLMA8DhxS8/jUwoahM9GpE1q81px39C19b6wRjNsB1t6Jla4XisD3EIWPqGFwjnlm2jlOqHYyZVdUu32hpVsqQwTUcum+dR5KZmROM9b7mpnrfC2NmlU0wkvaWdKek9ZKWSDqtTDlJmiFpZfqYoYLJrSRNkfSkpA3p1ylF758m6WFJ7ZKWS/qbvI/N3tLS1MCrazaxZsPWaodiZlVU6SuY60jupRkLnA5cL2lSiXLnANOBycBRwMnAuZBMXwPcDdxKctPnzcDd6XYk7UOyENqNwGjgUOA/8zskK9YyLuncX7TczWRmA1nFEoykkcCpwKUR0R4RjwL3AJ8qUfxM4OqIeCUilgJXA2el+1pJBidcExGbI+JbgIDj0/0XAA9ExG3p/nURsTC3A7O38eJjZgYZp+uXVDw0uVMAmyLi9QzVHA5si4jFBdueAo4rUXZSuq+w3KSCffMionB49Lx0+/3Ae4H5kn5NcvXyGPDXEfFS8YdIOofkaokxY8bQ1taW4TD6hvb29j4bb0QwshZmz1nEhM0vAn073lIcb776W7zQ/2LuC/FmXQ/mRbq430XSWuD7wEVdTCNTBxT/S7sGKHWzRB07zxiwBqhL+2GK9xXXsz8wDTgRmA9cCfyQZIG0nUTETGAmQHNzc7S2tpYJve9pa2ujL8d75OLfsHZ7B62tyWnv6/EWc7z56m/xQv+LuS/Em7WJ7M+BV4AvkfzhPjF9/hJwNsmsy58CLu2ijnagoWhbA1Cqob64bAPQnl61dFfPRuDOiHg8IjYBXwHeL2mvLmKzXtaSrm7Z0eH7cM0GqqwJ5rPA30XEFRExO31cAXweODsirgU+R5KIylkMDJZ0WMG2ycCCEmUXpPtKlVsAHFU4qoxkIEDn/nnsfLXlv3BV0DKugfVbtrN09cZqh2JmVZI1wbyHpLmp2NPAu9LnvyFpniopXVZ5FnC5pJGSjgFOAX5QovgtwAWSxkvajySR3ZTuawO2A5+TNFTS+en22enX7wMfT4cy15JcVT0aEZ6ks4JaCqaMMbOBKWuCWULaGV7kMyTNZABjgFXd1HMeMBxYQdIv8tmIWCDpWEntBeVuBO4lSWpPA/el24iILSRDmM8AVpM00U1PtxMRs4Evpu9ZQdLRX/J+G8tP5zxkz/iOfrMBK2sn/+eBOyR9jHR9GOCdJBNhnpq+fhdwe1eVRMQqkuRQvP0Rks77ztcBXJQ+StUzFzi6i8+5Hri+q1gsXyOHDmbi6BGeMsZsAMuUYCLivrTv5DygOd18D3BD5/DfiPi3fEK0/qp5bD0LfS+M2YCV9QqGiHgZuCTHWGwP0zKugQcXLmfT1u3VDsXMqiBzgpE0ApgC7EtR301EzOrluGwPcERTPR0Bzy5v776wme1xst7J/2GSTvnRJXYHMKg3g7I9w47Fx5atZd8qx2JmlZd1FNm1JKOy9o+ImqKHk4uVNHH0SIbV1vDMa+7oNxuIsiaYA4GvRsSrOcZie5hBNaJ5bD2Llruj32wgyppgfsVbo8fMMksWH/MVjNlAlLWT/wbgG+ld9fOBnVaSiog5vR2Y7Rlamhq4/YlXWLM583gSM9tDZP2t/0n6dWaJfe7kt7I6Fx97eV1HlSMxs0rLmmAOyjUK22O1NCUTXzvBmA08We/kX5J3ILZnenjx69QIfrRoC49+fTYXntTM9Knje1zPXXOXctUDi3h19Ub2axxe1Xr6YixLV29k/G99fvOox3Zd2QQj6RPAvRGxNX1elm+0tFLumruUS2bNp3NJmKWrN3LJrGRS7p78onfWszGdEaCa9TiWfOvpS7HY7tPOKw8X7JA6gKaIWJE+Lyf2hHthmpubY9GiRdUOI7O+sFpdd475+uyS68HUDhJ/tF/29d9+/+oatm5/+89pnvWsW7uW+obide2qE0uedVSrnmqd3/GNw/nVxcdnrqdQf/idK1SpeCU9GRHvLLWv7BVMRNSUem6W1atlFhvbuj1oHF6buZ5Sfyjyrqdjo0rWXY1Y8qyjWvVU6/yW+5m0fHjsqOVmv8bhJa9gxjcO5+az3525nnJXQnnWk/z39/a6qxFLnnVUq55qnd/9GodnrsN2X+YrE0n7SzpN0t9KuqDwkWeA1n9deFIzw2t3bj0dXjuIC0/q2T27fakex5JvPX0pFtt9WSe7PB34HrANeJ23r3n/L70fmvV3nZ2pO0Y57eJInsJ6dmdEUG/U01dj8fktXc/Xf/4My9ZuomHYYC4/5Uh38FdaRHT7AP4A/DMwKEv5/vg4/PDDoz956KGHqh1CjzjefDne8j4w47/i//zgid2ux+e4NOCJKPN3NWsT2VjgOxHhlaPMrF+ZNmEUc156s/OfZaugrAnmZ8B78gzEzCwP0yaMYvnazby6ZlO1Qxlwso4i+wUwQ9IkSk926RstzaxPmjZhFABzlrzJeI8iq6isCebG9OsXS+zzZJdm1me1jKtnWG0Nc156k5Mn71ftcAaUrHOR+UZLM+uXagfVcNT+jcx5aXW1Qxlwuk0ckmolPSbJA8jNrF+aNmEUv391DZu2epxSJXWbYCJiK8l0/R6CYWb90rQJjWzdHsxfuqbaoQwoWZu+bgY+k2cgZmZ5mTbxrY5+q5ysnfwjgdMlnQg8Cawv3BkRn+vtwMzMess+dUOZOHoEc15ygqmkrAnmCGBO+vzgon1uOjOzPm/ahFE8+twbRASSqh3OgJB1FNmH8g7EzCxP0yY0cufcpbzy5kYO2HtEtcMZEDz82MwGhKmdN1y6maxiejJd/4ckzZR0v6TZhY8e1LG3pDslrZe0RNJpZcpJ0gxJK9PHDBVc00qaIulJSRvSr1NK1DFE0kJJr2SNz8z2XC1N9YwYMoi5vh+mYjIlGElnAT8H6oFWkin7RwHTgN/34POuA7aQTJ55OnB9Ov1MsXOA6cBk4CjgZODcNJYhwN3ArWkMNwN3p9sLXZjGaWbG4EE1HLX/Xr6CqaCsVzBfAM6PiD8nmYfskoiYSvJHvj1LBZJGAqcCl0ZEe0Q8CtwDfKpE8TOBqyPilYhYClwNnJXuayXpO7omIjZHxLcAATsW2pZ0EPAXwBUZj8/MBoDkhsu1vuGyQrKOIjsYeDB9vhmoS59/G2gDLs5Qx+HAtohYXLDtKeC4EmUnpfsKy00q2Dcvdp57e166/f709b+SzJvW5QLcks4huVpizJgxtLW1ZTiMvqG9vd3x5sjx5qta8dau3ca2juDme9to3rtnUyj6HPdc1gSzkqR5DGApcCTJH/XRQNbpSeuAtUXb1hTUW1x2TVG5urQfpnjfTvVI+jjJwmh3SmrtKqCImAnMBGhubo7W1i6L9ynJmuat1Q4jM8ebL8ebzTvaN3PtnAeJ0QfSetwhPXqvz3HPZU0wjwAfIZmq/3bgW+lNlyeQTOWfRTvQULStAViXoWwD0B4RIalsPWkz3JXAxzLGZGYDyOi6oRw4eoTv6K+QrH0w5wM/TJ9fAVxFcvVyO/DpjHUsBgZLOqxg22RgQYmyC9J9pcotAI4qHFVGMhBgAXAYcCDwiKRlwCxgnKRlkg7MGKeZ7cGSFS5Xe4XLCsh6o+WqgucdwIyeflBErJc0C7hc0qeBKcApwPtLFL8FuEDSz0hmCvg8Sb8KJH0+24HPSbqBt+ZImw10AAcU1PN+kn6iaXhEmZkBUyeOYpZvuKyIntwHM1bSFyRdL2mfdNsx6YitrM4j6bNZQXJF9NmIWCDp2LTpq9ONwL0kTXJPA/el24iILSRDmM8AVgNnA9MjYktEbIuIZZ0PYBXQkb72sBEzY9qERsA3XFZCpisYSUcD/wW8QDJa6yrgDeBEktFhJW+YLJZeCU0vsf0R3hqZRjpC7KL0UaqeucDRGT6vDdg/S2xmNjA0j01uuJyz5E1OmTK+2uHs0bJewXwDuDa992VzwfYHgGN6PSozs5wMHlTDZK9wWRFZE8zRJHfMF3uN5K58M7N+Y9rERn7/2lo2bNlW7VD2aFkTzEaSaVmKtZD0p5iZ9RvTJoxie0cw7xWvcJmnrAnmbuDLkoamryMd9jsDuCOHuMzMcuOZlSujJ3OR7U0y1HcE8CjwHMkd9F/KJzQzs3zsPXIIB+8zkjlL3A+Tp6z3wawFPiDpeJJ7SmqAORHxYNfvNDPrm6ZOGEXbohVe4TJHPVpwLCJmR8Q3IuLKiHhQ0kRJt+cVnJlZXqZNbGTl+i28tIk760MAAA+WSURBVGpDtUPZY+3uipaNJFPwm5n1K9PcD5M7L5lsZgPS4WPrqRs62P0wOXKCMbMBaVCNmHyAV7jMkxOMmQ1Y0yaM4pll63zDZU66HEUm6Z5u3l+8LouZWb/RecPlUy+v4X2HjK52OHuc7oYpr8yw/4VeisXMrKKmFsys7ATT+7pMMBHxl5UKxMys0hpHDOHgMSOZ636YXLgPxswGNK9wmR8nGDMb0KZNGMWq9VtYstI3XPY2JxgzG9CmTfQKl3lxgjGzAe2wfdMbLp1gep0TjJkNaINqxJQDGnnSd/T3OicYMxvwpk1oZNGytbRv9g2XvckJxswGvGkTR9ERMO9lX8X0JicYMxvwph7gmZXz4ARjZgPeXiNqOXTfOua85CuY3uQEY2ZG0g8z96U3fcNlL3KCMTMjueHyzQ1beeGN9dUOZY/hBGNmRtLRD7iZrBc5wZiZAYeOqaN+mG+47E1OMGZmQE16w+WcJU4wvcUJxswsNW3CKBYvX+cbLntJRROMpL0l3SlpvaQlkk4rU06SZkhamT5mSFLB/imSnpS0If06pWDfhZKelrRO0guSLqzEsZlZ/9d5w+VTvuGyV1T6CuY6YAswFjgduF7SpBLlzgGmA5OBo4CTgXMBJA0B7gZuBUYBNwN3p9sBBJyR7vsocL6k/53XAZnZnmPKAenMym4m6xUVSzCSRgKnApdGRHtEPArcA3yqRPEzgasj4pWIWApcDZyV7mslWYnzmojYHBHfIkkqxwNExJURMScitkXEIpJkdEyOh2Zme4i9htdy2L517ujvJV0umdzLDge2RcTigm1PAceVKDsp3VdYblLBvnmx891Q89Lt9xdWkjarHQvcWCogSeeQXC0xZswY2trash5L1bW3tzveHDnefPXleMcN2czvnm/noYceoqBlvk/HXEpfiLeSCaYOWFu0bQ1QX6bsmqJydWnCKN7XVT2XkVylfb9UQBExE5gJ0NzcHK2trV0eQF/S1taG482P481XX453+ciXePiO+Uw48l0cMqZux/a+HHMpfSHeSvbBtAMNRdsagHUZyjYA7elVS6Z6JJ1P0hfzJxGxeTfiNrMBZNqE9IZL98PstkommMXAYEmHFWybDCwoUXZBuq9UuQXAUYWjykgGAuyoR9LZwMXACRHxSi/EbmYDxCFj6mjwDZe9omIJJiLWA7OAyyWNlHQMcArwgxLFbwEukDRe0n7A54Gb0n1twHbgc5KGplcqALMBJJ0O/DNwYkQ8n9fxmNmeqaZGTJkwijle4XK3VXqY8nnAcGAF8EPgsxGxQNKxktoLyt0I3AvMB54G7ku3ERFbSIYwnwGsBs4GpqfbAf4JGA08Lqk9fdyQ/6GZ2Z7i6AmjWLxiHWs3ba12KP1aJTv5iYhVJMmhePsjJJ33na8DuCh9lKpnLnB0mX0H9UqwZjZgTZvYSKQ3XB572Jhqh9NveaoYM7MiUw5oRMLNZLvJCcbMrEj9sFoO37feHf27yQnGzKyEaROTFS47OrzC5a5ygjEzK2HqhFGs3bSN599o776wleQEY2ZWwls3XLofZlc5wZiZlXDwPiPZa3it+2F2gxOMmVkJNTVi6oRGJ5jd4ARjZlbGtAmjeHZFu2+43EVOMGZmZUybMIoI+O+X3A+zK5xgzMzKmHzAXskNl24m2yUVnSrGzKw/qR9WS1P9UK5v+wObt3Uw/rezufCkZqZPHd+jeu6au5SrHljEq6s3sl/j8F2qo7fq6e1YhjQdWnLaLnCCMTMr6665S1nRvoXt6c2WS1dv5JJZ8wEy/1G+a+5SLpk1n41bt+9yHb1VT16xlOMEY2ZWxlUPLNqRXDpt3Lqdv79jHrPmLs1Ux2PPr2Tzto7dqmNX6lm1ahPfe/53FYulFCcYM7MyXl29seT2zds6WLsx28iycn+Ie1LHrtSzYWswuGh73rEUc4IxMytjv8bhLC2RZMY3Dueuvz4mUx3HfH32btexK/W0tbXR2rrz9rxjKeZRZGZmZVx4UjPDawfttG147SAuPKm5onX0h1hK8RWMmVkZnR3fVz2wiKWrNzJ+F0ZdFdaxOyO3eqOePGJ5rYtyTjBmZl2YPnU806eOT5ucWnerjt6Kpdp1FNajS557slwZN5GZmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlgsnGDMzy0VFE4ykvSXdKWm9pCWSTitTTpJmSFqZPmZIUsH+KZKelLQh/Tol63vNzKwyKn0Fcx2wBRgLnA5cL2lSiXLnANOBycBRwMnAuQCShgB3A7cCo4CbgbvT7V2+18zMKqdiCUbSSOBU4NKIaI+IR4F7gE+VKH4mcHVEvBIRS4GrgbPSfa0kywxcExGbI+JbgIDjM7zXzMwqpJLrwRwObIuIxQXbngKOK1F2UrqvsNykgn3zIiIK9s9Lt9/fzXt3IukckisegM2Sns52KH3CPsAb1Q6iBxxvvhxv/vpbzJWKd2K5HZVMMHXA2qJta4D6MmXXFJWrS/tSivcV11P2vUVJiYiYCcwEkPRERLwz++FUl+PNl+PNV3+LF/pfzH0h3kr2wbQDDUXbGoB1Gco2AO1pguiunq7ea2ZmFVLJBLMYGCzpsIJtk4EFJcouSPeVKrcAOKpoZNhRRfvLvdfMzCqkYgkmItYDs4DLJY2UdAxwCvCDEsVvAS6QNF7SfsDngZvSfW3AduBzkoZKOj/dPjvDe7sys+dHVVWON1+ON1/9LV7ofzFXPV5VsuVI0t7A94ATgZXAxRHxH5KOBX4eEXVpOQEzgE+nb/0O8PedzVySpqbb/ghYCPxVRMzN8l4zM6uMiiYYMzMbODxVjJmZ5cIJxszMcjGgEkxvzYVWoViHSvpuGuc6Sf8t6Y/LlD1L0nZJ7QWP1krGm8bRJmlTQQyLypTrC+e3veixXdK/lilblfMr6XxJT0jaLOmmon0nSHomnY/vIUllb3aTdGBaZkP6ng9XMl5J75X0C0mrJL0u6ceSxnVRT6afoxzjPVBSFH2/L+2inmqf39OLYt2Qxn90mXoqcn5hgCUYemEutAoaDLxMMtPBXsCXgNslHVim/G8ioq7g0VaRKN/u/IIYmsuUqfr5LTxXQBOwEfhxF2+pxvl9FfgnkoExO0jah2RE5qXA3sATwI+6qOeHwFxgNPAPwE8kjalUvCRzBs4EDiS563sd8P1u6sryc7S7ysXbqbEghq92UU9Vz29E3Fb083we8Dwwp4u6KnF+B06CUe/NhVYREbE+Ii6LiBcjoiMifgq8AJT8r6Sfqfr5LXIqsAJ4pIoxvE1EzIqIu0hGXBb6BLAgIn4cEZuAy4DJklqK65B0ODAN+HJEbIyIO4D5JMdckXgj4udprGsjYgPwbeCY3v78nuri/GbWF85vCWcCt/SFkbMDJsFQfi60UlcwmeczqxRJY0mOodxNo1MlvSFpsaRLJVVyGqBCV6Rx/KqLZqS+dn6z/EL2lfMLRecvvcfsD5T/WX4+IgpnzKj2+f4g3d/8nOXnKG9LJL0i6fvpVWMpfer8pk2lHyS5H7ArFTm/AynB9NZcaBUnqRa4Dbg5Ip4pUeRh4EhgX5L/nP4cuLByEe7w98DBwHiSJpF7JR1SolyfOb/pL+RxJMs+lNNXzm+n7ubj29WyuZN0FPCPdH3+sv4c5eUN4F0kzXlHk5yr28qU7VPnFzgDeCQiXuiiTMXO70BKML01F1pFSaohme1gC3B+qTIR8XxEvJA2pc0HLgc+WcEwO+N4LCLWpcso3Az8CvhYiaJ95vySNJE+2tUvZF85vwV252e5q7K5knQo8HPgbyKibHNkD36OcpE2oT8REdsiYjnJ791HJJVKGn3m/KbOoOt/lip6fgdSgumtudAqJv2P/rskgxJOjYitGd8aJGvkVFu5OPrE+U11+wtZQrXP707nL+1fPITyP8sHF/1xrPj5Tq8UHwS+GhGlpofqSrXPd+c/PqX+XvaJ8wugZPqt/YCf9PCtuZ3fAZNgenEutEq6HjgCODkiNpYrJOmP0z4a0o7eS0lW/awYSY2STpI0TNJgSaeTtAXfX6J4nzi/kt5P0kzQ1eixqp3f9DwOAwYBgzrPLXAncKSkU9P9/0iyRtLbmk/TPsf/Br6cvv/jJCP37qhUvJLGk8wV+O2IuKGbOnryc5RXvO+R1CypRtJo4FtAW0QUN4X1ifNbUORM4I6i/qDiOip2fgGIiAHzIBnSeRewHngJOC3dfixJE01nOQFXAqvSx5Wk0+pUMNaJJP9ZbCK5DO98nA5MSJ9PSMt+A1ieHtfzJE04tRWOdwzwOEnTwGrgt8CJffX8pnHcCPygxPY+cX5JRodF0eOydN+HgWdIhle3AQcWvO8G4IaC1wemZTYCi4APVzJe4Mvp88Kf48Kfhy+SzEXY5c9RBeP9c5IRm+uB10j+IWrqq+c33TcsPV8nlHhfVc5vRHguMjMzy8eAaSIzM7PKcoIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4zZHipdE6SaU9rYAOcEY5YDSTelf+CLH7+tdmxmlVLNKcfN9nQP8vb1hrZUIxCzavAVjFl+NkfEsqLHKtjRfHW+pPvSJW6XSPqLwjdLeoekByVtVLLc8E2S9ioqc6ak+UqW0V0uqXjizr2VLFG8XtLzxZ9hlicnGLPq+QrJqqpTSNbluEXSO2HHDMkPkMzb9W7g48D7KVguV9K5JPOpfZ9kgsWPAU8XfcY/kkzMOZlkWeXvSZqQ3yGZvcVzkZnlQNJNwF+QTFZa6LqI+HtJAXwnIj5T8J4HgWUR8ReSPkMyyeb+kc6Om648+BBwWEQ8J+kV4NaIuLhMDAF8PSIuSV8PJll075yIuLUXD9esJPfBmOXnYeCcom2rC57/pmjfb4A/SZ8fQTIFf+HU678GOoA/krSWZKmB/+omhnmdTyJim6TXSVbmNMudE4xZfjZExHM51NuTZofiReoCN41bhfgHzax63lvi9cL0+ULgHUUrJb6f5Hd2YUSsAJYCJ+Qepdku8hWMWX6GSmoq2rY9Il5Pn39C0uMki1V9kiRZvCfddxvJIIBbJP0jMIqkQ39WwVXR14BvSloO3AeMIFlw6uq8DsisJ5xgzPLzYZIVEQstBfZPn18GnEqyJO/rwF9GxOMAEbFB0knANcDvSAYL3A38TWdFEXG9pC0kS07PIFkd9Gd5HYxZT3kUmVkVpCO8/mdE/KTasZjlxX0wZmaWCycYMzPLhZvIzMwsF76CMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLxf8H5tQvpz7UsvgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the learning rate by epoch for exponential scheduling\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 121us/sample - loss: 0.6757 - accuracy: 0.7817 - val_loss: 0.5789 - val_accuracy: 0.8188\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 106us/sample - loss: 0.5250 - accuracy: 0.8263 - val_loss: 0.5487 - val_accuracy: 0.8262\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 108us/sample - loss: 0.4851 - accuracy: 0.8427 - val_loss: 0.8095 - val_accuracy: 0.8310\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 109us/sample - loss: 0.4719 - accuracy: 0.8460 - val_loss: 0.9283 - val_accuracy: 0.7864\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 118us/sample - loss: 0.3010 - accuracy: 0.8897 - val_loss: 0.4652 - val_accuracy: 0.8586\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 108us/sample - loss: 0.2511 - accuracy: 0.9059 - val_loss: 0.4036 - val_accuracy: 0.8826\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 4s 143us/sample - loss: 0.2333 - accuracy: 0.9120 - val_loss: 0.4145 - val_accuracy: 0.8736\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 0.2113 - accuracy: 0.9208 - val_loss: 0.4538 - val_accuracy: 0.8796\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 136us/sample - loss: 0.1463 - accuracy: 0.9432 - val_loss: 0.4021 - val_accuracy: 0.8842\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 106us/sample - loss: 0.1292 - accuracy: 0.9510 - val_loss: 0.4459 - val_accuracy: 0.8774\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 108us/sample - loss: 0.1159 - accuracy: 0.9566 - val_loss: 0.4438 - val_accuracy: 0.8816\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s 108us/sample - loss: 0.0888 - accuracy: 0.9681 - val_loss: 0.4397 - val_accuracy: 0.8838\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 115us/sample - loss: 0.0801 - accuracy: 0.9722 - val_loss: 0.4483 - val_accuracy: 0.8838\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 0.0683 - accuracy: 0.9778 - val_loss: 0.4522 - val_accuracy: 0.8858\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 110us/sample - loss: 0.0644 - accuracy: 0.9792 - val_loss: 0.4594 - val_accuracy: 0.8844\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 114us/sample - loss: 0.0593 - accuracy: 0.9821 - val_loss: 0.4582 - val_accuracy: 0.8870\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 115us/sample - loss: 0.0578 - accuracy: 0.9830 - val_loss: 0.4598 - val_accuracy: 0.8866\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 111us/sample - loss: 0.0552 - accuracy: 0.9840 - val_loss: 0.4632 - val_accuracy: 0.8864\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 106us/sample - loss: 0.0544 - accuracy: 0.9840 - val_loss: 0.4640 - val_accuracy: 0.8870\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 105us/sample - loss: 0.0531 - accuracy: 0.9852 - val_loss: 0.4654 - val_accuracy: 0.8872\n"
     ]
    }
   ],
   "source": [
    "# using ReduceLROnPlateau callback for performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "# defining the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9) # using momentum optimizer\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the learning rate by epoch for ReduceLROnPleatue\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1) # get current axes\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx() # create twin axes\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
